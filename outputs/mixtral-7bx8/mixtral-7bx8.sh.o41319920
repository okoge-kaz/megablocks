MASTER_ADDR=10.0.51.1
using world size: 128, data-parallel-size: 8, tensor-model-parallel size: 4, pipeline-model-parallel size: 4 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 8
  data_path ....................................... None
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  encoder_num_layers .............................. 32
  encoder_seq_length .............................. 1024
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_signal_handler ............................. False
  ffn_hidden_size ................................. 14336
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_e4m3 ........................................ False
  fp8_hybrid ...................................... False
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 1024
  gradient_accumulation_fusion .................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 4096
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. 63312
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-06
  mmap_warmup ..................................... False
  moe_capacity_factor ............................. 0
  moe_expert_model_parallelism .................... True
  moe_jitter_eps .................................. None
  moe_lbl_in_fp32 ................................. False
  moe_loss_weight ................................. 0.1
  moe_num_experts ................................. 8
  moe_top_k ....................................... 2
  moe_use_megatron_switch ......................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_layers ...................................... 32
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 4
  pipeline_model_parallel_split_rank .............. None
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ 1
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  save_interval ................................... 20
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 969, 30, 1
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 4
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. /bb/llm/gaf51275/llm-jp/llm-ja-tokenizer/models/ver2/code10K_en20K_ja30K.ver2.2.model
  tokenizer_type .................................. SentencePieceTokenizer
  train_data_path ................................. ['1489457253', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_wiki/ja_wiki_merge_1_text_document', '4983898399', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_wiki/en_wiki_merge_1_text_document', '8967214774', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/code_stack/code_stack_merge_1_text_document', '17716652494', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_1_text_document', '17728398911', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_2_text_document', '17862741217', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_3_text_document', '17854181202', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_4_text_document', '17779824310', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_5_text_document', '17847796716', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_6_text_document', '8938950206', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_7_text_document', '19540410239', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_1_text_document', '19559059958', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_2_text_document', '19547251566', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_3_text_document', '19550089401', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_4_text_document', '19553509796', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_5_text_document', '19566479585', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_6_text_document', '17060823775', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_7_text_document']
  train_iters ..................................... 63312
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 4
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_contiguous_buffers_in_local_ddp ............. True
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. True
  use_mpi ......................................... True
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  valid_data_path ................................. ['77810430', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/code_stack_validation_0_text_document', '37133061', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/en_pile_validation_0_text_document', '1011609', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/en_wiki_validation_0_text_document', '147265562', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/ja_cc_validation_0_text_document', '1097003', '/bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/ja_wiki_validation_0_text_document']
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  wandb_entity .................................... okoge
  wandb_id ........................................ None
  wandb_name ...................................... Mixtral-7Bx8_expert=8_cap_fac=0_top_k=2_gb_1
  wandb_project ................................... megablock
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 128
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 128
> building SentencePieceTokenizer tokenizer ...
 > padded vocab (size: 48587) with 53 dummy tokens (new size: 48640)
> initializing torch distributed ...
wandb: Currently logged in as: okoge. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /bb/1/llm/gaf51275/llama/mistral/megablocks/wandb/run-20240114_140034-lwbvlr8s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mixtral-7Bx8_expert=8_cap_fac=0_top_k=2_gb_1-2024-01-14-14-00-33
wandb: ⭐️ View project at https://wandb.ai/okoge/megablock
wandb: 🚀 View run at https://wandb.ai/okoge/megablock/runs/lwbvlr8s
> wandb ...
> initialized tensor model parallel with size 4
> initialized pipeline model parallel with size 4
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/data'
>>> done with dataset index builder. Compilation time: 0.055 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_dense_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_dense_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 12.717 seconds
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/initialize.py:278: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at ../torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
time to initialize megatron (seconds): 18.842
[after megatron is initialized] datetime: 2024-01-14 14:00:49 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1074225152
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1074225152
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1074225152
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1074225152
 > number of parameters on (tensor, pipeline) model parallel rank (2, 2): 1074225152
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1074225152
 > number of parameters on (tensor, pipeline) model parallel rank (3, 2): 1074225152
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1074225152
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1124040704
 > number of parameters on (tensor, pipeline) model parallel rank (3, 3): 1124040704
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1128226816
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1128226816
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
 > number of parameters on (tensor, pipeline) model parallel rank (2, 3): 1124040704
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1128226816
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1124040704
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1128226816
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/optimizer/optimizer.py:371: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._scale_one = torch.cuda.FloatTensor([1.0])
> learning rate decay style: cosine
WARNING: could not find the metadata file /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
(min, max) time across ranks (ms):
    load-checkpoint ................................: (0.32, 0.41)
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-01-14 14:00:50 
> building train, validation, and test datasets ...
train_samples=64831488, eval_iters=6340, test_iters=10
 > datasets target sizes (minimum size):
    train:      64831488
    validation: 6492160
    test:       10240
> building train, validation, and test datasets for GPT ...
Separate data paths provided for train, valid & test. Split string will be ignored.
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002942 seconds
    number of documents: 1346724
    train:
     document indices in [0, 1346724) total of 1346724 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_wiki/ja_wiki_merge_1_text_document_train_indexmap_365460ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_wiki/ja_wiki_merge_1_text_document_train_indexmap_365460ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_wiki/ja_wiki_merge_1_text_document_train_indexmap_365460ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.010 seconds
    total number of samples: 1454549
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.006009 seconds
    number of documents: 6581920
    train:
     document indices in [0, 6581920) total of 6581920 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_wiki/en_wiki_merge_1_text_document_train_indexmap_1222870ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_wiki/en_wiki_merge_1_text_document_train_indexmap_1222870ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_wiki/en_wiki_merge_1_text_document_train_indexmap_1222870ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.009 seconds
    total number of samples: 4867089
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002420 seconds
    number of documents: 2313597
    train:
     document indices in [0, 2313597) total of 2313597 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/code_stack/code_stack_merge_1_text_document_train_indexmap_2200233ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/code_stack/code_stack_merge_1_text_document_train_indexmap_2200233ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/code_stack/code_stack_merge_1_text_document_train_indexmap_2200233ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.013 seconds
    total number of samples: 8757046
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002110 seconds
    number of documents: 9997800
    train:
     document indices in [0, 9997800) total of 9997800 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_1_text_document_train_indexmap_4347032ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_1_text_document_train_indexmap_4347032ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_1_text_document_train_indexmap_4347032ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.014 seconds
    total number of samples: 17301419
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.005985 seconds
    number of documents: 9997800
    train:
     document indices in [0, 9997800) total of 9997800 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_2_text_document_train_indexmap_4349914ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_2_text_document_train_indexmap_4349914ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_2_text_document_train_indexmap_4349914ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.010 seconds
    total number of samples: 17312890
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.007364 seconds
    number of documents: 9997800
    train:
     document indices in [0, 9997800) total of 9997800 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_3_text_document_train_indexmap_4382877ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_3_text_document_train_indexmap_4382877ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_3_text_document_train_indexmap_4382877ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.010 seconds
    total number of samples: 17444084
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.006544 seconds
    number of documents: 9997800
    train:
     document indices in [0, 9997800) total of 9997800 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_4_text_document_train_indexmap_4380776ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_4_text_document_train_indexmap_4380776ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_4_text_document_train_indexmap_4380776ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.009 seconds
    total number of samples: 17435724
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002365 seconds
    number of documents: 9997800
    train:
     document indices in [0, 9997800) total of 9997800 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_5_text_document_train_indexmap_4362532ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_5_text_document_train_indexmap_4362532ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_5_text_document_train_indexmap_4362532ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.006 seconds
    total number of samples: 17363110
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.007515 seconds
    number of documents: 9997800
    train:
     document indices in [0, 9997800) total of 9997800 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_6_text_document_train_indexmap_4379210ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_6_text_document_train_indexmap_4379210ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_6_text_document_train_indexmap_4379210ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.010 seconds
    total number of samples: 17429489
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002588 seconds
    number of documents: 4998900
    train:
     document indices in [0, 4998900) total of 4998900 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_7_text_document_train_indexmap_2193298ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_7_text_document_train_indexmap_2193298ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/en_pile/en_pile_merge_7_text_document_train_indexmap_2193298ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.011 seconds
    total number of samples: 8729444
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002208 seconds
    number of documents: 10000000
    train:
     document indices in [0, 10000000) total of 10000000 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_1_text_document_train_indexmap_4794516ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_1_text_document_train_indexmap_4794516ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_1_text_document_train_indexmap_4794516ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.013 seconds
    total number of samples: 19082432
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002177 seconds
    number of documents: 10000000
    train:
     document indices in [0, 10000000) total of 10000000 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_2_text_document_train_indexmap_4799092ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_2_text_document_train_indexmap_4799092ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_2_text_document_train_indexmap_4799092ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.018 seconds
    total number of samples: 19100645
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.005749 seconds
    number of documents: 10000000
    train:
     document indices in [0, 10000000) total of 10000000 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_3_text_document_train_indexmap_4796195ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_3_text_document_train_indexmap_4796195ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_3_text_document_train_indexmap_4796195ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.010 seconds
    total number of samples: 19089113
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.006770 seconds
    number of documents: 10000000
    train:
     document indices in [0, 10000000) total of 10000000 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_4_text_document_train_indexmap_4796891ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_4_text_document_train_indexmap_4796891ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_4_text_document_train_indexmap_4796891ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.010 seconds
    total number of samples: 19091885
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.001961 seconds
    number of documents: 10000000
    train:
     document indices in [0, 10000000) total of 10000000 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_5_text_document_train_indexmap_4797730ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_5_text_document_train_indexmap_4797730ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_5_text_document_train_indexmap_4797730ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.010 seconds
    total number of samples: 19095225
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002362 seconds
    number of documents: 10000000
    train:
     document indices in [0, 10000000) total of 10000000 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_6_text_document_train_indexmap_4800913ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_6_text_document_train_indexmap_4800913ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_6_text_document_train_indexmap_4800913ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.006 seconds
    total number of samples: 19107891
    total number of epochs: 1
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.006323 seconds
    number of documents: 8723697
    train:
     document indices in [0, 8723697) total of 8723697 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_7_text_document_train_indexmap_4186115ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_7_text_document_train_indexmap_4186115ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/train/ja_cc/ja_cc_merge_7_text_document_train_indexmap_4186115ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.010 seconds
    total number of samples: 16660961
    total number of epochs: 1
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 0.00560902, achieved: 0.00560902
   dataset 1, input: 0.0187684, achieved: 0.0187684
   dataset 2, input: 0.0337689, achieved: 0.0337689
   dataset 3, input: 0.0667176, achieved: 0.0667176
   dataset 4, input: 0.0667619, achieved: 0.0667619
   dataset 5, input: 0.0672678, achieved: 0.0672678
   dataset 6, input: 0.0672356, achieved: 0.0672355
   dataset 7, input: 0.0669555, achieved: 0.0669555
   dataset 8, input: 0.0672115, achieved: 0.0672115
   dataset 9, input: 0.0336624, achieved: 0.0336624
   dataset 10, input: 0.0735856, achieved: 0.0735856
   dataset 11, input: 0.0736558, achieved: 0.0736558
   dataset 12, input: 0.0736113, achieved: 0.0736113
   dataset 13, input: 0.073622, achieved: 0.073622
   dataset 14, input: 0.0736349, achieved: 0.0736349
   dataset 15, input: 0.0736837, achieved: 0.0736838
   dataset 16, input: 0.0642479, achieved: 0.0642479
> elapsed time for building blendable dataset indices: 7.45 (sec)
 > building dataset index ...
    warming up index mmap file...
    reading sizes...
    reading pointers...
    reading document index...
    warming up data mmap file...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.205921 seconds
    number of documents: 20191
    valid:
     document indices in [0, 20191) total of 20191 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/code_stack_validation_0_text_document_valid_indexmap_1920733ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/code_stack_validation_0_text_document_valid_indexmap_1920733ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/code_stack_validation_0_text_document_valid_indexmap_1920733ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.014 seconds
    total number of samples: 1975656
    total number of epochs: 26
 > building dataset index ...
    warming up index mmap file...
    reading sizes...
    reading pointers...
    reading document index...
    warming up data mmap file...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.133352 seconds
    number of documents: 19800
    valid:
     document indices in [0, 19800) total of 19800 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/en_pile_validation_0_text_document_valid_indexmap_916622ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/en_pile_validation_0_text_document_valid_indexmap_916622ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/en_pile_validation_0_text_document_valid_indexmap_916622ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.011 seconds
    total number of samples: 942832
    total number of epochs: 26
 > building dataset index ...
    warming up index mmap file...
    reading sizes...
    reading pointers...
    reading document index...
    warming up data mmap file...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.005544 seconds
    number of documents: 1479
    valid:
     document indices in [0, 1479) total of 1479 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/en_wiki_validation_0_text_document_valid_indexmap_24972ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/en_wiki_validation_0_text_document_valid_indexmap_24972ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/en_wiki_validation_0_text_document_valid_indexmap_24972ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 25686
    total number of epochs: 26
 > building dataset index ...
    warming up index mmap file...
    reading sizes...
    reading pointers...
    reading document index...
    warming up data mmap file...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.099607 seconds
    number of documents: 74937
    valid:
     document indices in [0, 74937) total of 74937 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/ja_cc_validation_0_text_document_valid_indexmap_3635217ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/ja_cc_validation_0_text_document_valid_indexmap_3635217ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/ja_cc_validation_0_text_document_valid_indexmap_3635217ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.015 seconds
    total number of samples: 3739165
    total number of epochs: 26
 > building dataset index ...
    warming up index mmap file...
    reading sizes...
    reading pointers...
    reading document index...
    warming up data mmap file...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.007489 seconds
    number of documents: 1134
    valid:
     document indices in [0, 1134) total of 1134 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/ja_wiki_validation_0_text_document_valid_indexmap_27080ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/ja_wiki_validation_0_text_document_valid_indexmap_27080ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llm-jp/binarize/gpt-7b/ver2.2/code10K_en20K_ja30K/val/ja_wiki_validation_0_text_document_valid_indexmap_27080ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 27854
    total number of epochs: 26
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 0.294382, achieved: 0.294382
   dataset 1, input: 0.140486, achieved: 0.140486
   dataset 2, input: 0.00382725, achieved: 0.00382734
   dataset 3, input: 0.557154, achieved: 0.557154
   dataset 4, input: 0.00415032, achieved: 0.00415038
> elapsed time for building blendable dataset indices: 0.09 (sec)
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2024-01-14 14:01:03 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (449.23, 472.07)
    train/valid/test-data-iterators-setup ..........: (12752.22, 13362.63)training ...

[before the start of training step] datetime: 2024-01-14 14:01:03 
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[Rank 2] (after 1 iterations) memory (MB) | allocated: 19384.5810546875 | max allocated: 19384.6435546875 | reserved: 29394.0 | max reserved: 29394.0
[Rank 3] (after 1 iterations) memory (MB) | allocated: 19384.5810546875 | max allocated: 19384.6435546875 | reserved: 29510.0 | max reserved: 29510.0
[Rank 1] (after 1 iterations) memory (MB) | allocated: 19384.5810546875 | max allocated: 19384.6435546875 | reserved: 29394.0 | max reserved: 29394.0
[Rank 0] (after 1 iterations) memory (MB) | allocated: 19384.5810546875 | max allocated: 19384.6435546875 | reserved: 29606.0 | max reserved: 29606.0
 iteration        1/   63312 | consumed samples:         1024 | elapsed time per iteration (ms): 45257.5 | learning rate: 1.579E-07 | global batch size:  1024 |tokens per sec: 23169.10683091588 |TFLOPS: 9.934201254134884 | load balancing loss: 1.445312E-01 | lm loss: 1.159761E+01 | loss scale: 1.0 | grad norm: 52.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 67] (after 1 iterations) memory (MB) | allocated: 18472.5498046875 | max allocated: 18472.6123046875 | reserved: 24232.0 | max reserved: 24232.0
[Rank 66] (after 1 iterations) memory (MB) | allocated: 18472.5498046875 | max allocated: 18472.6123046875 | reserved: 24232.0 | max reserved: 24232.0
[Rank 65] (after 1 iterations) memory (MB) | allocated: 18472.5498046875 | max allocated: 18472.6123046875 | reserved: 24232.0 | max reserved: 24232.0
[Rank 64] (after 1 iterations) memory (MB) | allocated: 18472.5498046875 | max allocated: 18472.6123046875 | reserved: 24232.0 | max reserved: 24232.0
[Rank 35] (after 1 iterations) memory (MB) | allocated: 18472.5498046875 | max allocated: 18472.6123046875 | reserved: 26358.0 | max reserved: 26358.0
[Rank 34] (after 1 iterations) memory (MB) | allocated: 18472.5498046875 | max allocated: 18472.6123046875 | reserved: 26470.0 | max reserved: 26470.0
[Rank 33] (after 1 iterations) memory (MB) | allocated: 18472.5498046875 | max allocated: 18472.6123046875 | reserved: 26358.0 | max reserved: 26358.0
[Rank 32] (after 1 iterations) memory (MB) | allocated: 18472.5498046875 | max allocated: 18472.6123046875 | reserved: 26358.0 | max reserved: 26358.0
[Rank 99] (after 1 iterations) memory (MB) | allocated: 19328.69140625 | max allocated: 19328.81591796875 | reserved: 23790.0 | max reserved: 23790.0
[Rank 97] (after 1 iterations) memory (MB) | allocated: 19328.69140625 | max allocated: 19328.81591796875 | reserved: 23790.0 | max reserved: 23790.0
[Rank 96] (after 1 iterations) memory (MB) | allocated: 19328.69140625 | max allocated: 19328.81591796875 | reserved: 23788.0 | max reserved: 23788.0
[Rank 98] (after 1 iterations) memory (MB) | allocated: 19328.69140625 | max allocated: 19328.81591796875 | reserved: 23790.0 | max reserved: 23790.0
 iteration        2/   63312 | consumed samples:         2048 | elapsed time per iteration (ms): 30530.2 | learning rate: 3.159E-07 | global batch size:  1024 |tokens per sec: 34345.55982326077 |TFLOPS: 14.72632095661645 | load balancing loss: 1.455078E-01 | lm loss: 1.159358E+01 | loss scale: 1.0 | grad norm: 52.580 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/   63312 | consumed samples:         3072 | elapsed time per iteration (ms): 30228.7 | learning rate: 4.738E-07 | global batch size:  1024 |tokens per sec: 34688.10026108396 |TFLOPS: 14.87319177351278 | load balancing loss: 1.445312E-01 | lm loss: 1.156950E+01 | loss scale: 1.0 | grad norm: 52.312 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/   63312 | consumed samples:         4096 | elapsed time per iteration (ms): 30102.8 | learning rate: 6.318E-07 | global batch size:  1024 |tokens per sec: 34833.12461350431 |TFLOPS: 14.935373760682523 | load balancing loss: 1.455078E-01 | lm loss: 1.154745E+01 | loss scale: 1.0 | grad norm: 52.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/   63312 | consumed samples:         5120 | elapsed time per iteration (ms): 29890.6 | learning rate: 7.897E-07 | global batch size:  1024 |tokens per sec: 35080.47186982306 |TFLOPS: 15.041428665684286 | load balancing loss: 1.445312E-01 | lm loss: 1.146054E+01 | loss scale: 1.0 | grad norm: 52.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/   63312 | consumed samples:         6144 | elapsed time per iteration (ms): 29881.2 | learning rate: 9.477E-07 | global batch size:  1024 |tokens per sec: 35091.54400637341 |TFLOPS: 15.046176057701043 | load balancing loss: 1.425781E-01 | lm loss: 1.116097E+01 | loss scale: 1.0 | grad norm: 50.500 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/   63312 | consumed samples:         7168 | elapsed time per iteration (ms): 32280.5 | learning rate: 1.106E-06 | global batch size:  1024 |tokens per sec: 32483.265394718757 |TFLOPS: 13.927826315342477 | load balancing loss: 1.484375E-01 | lm loss: 1.043050E+01 | loss scale: 1.0 | grad norm: 40.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/   63312 | consumed samples:         8192 | elapsed time per iteration (ms): 33436.7 | learning rate: 1.264E-06 | global batch size:  1024 |tokens per sec: 31360.03467610215 |TFLOPS: 13.446219488847351 | load balancing loss: 1.503906E-01 | lm loss: 1.024047E+01 | loss scale: 1.0 | grad norm: 42.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/   63312 | consumed samples:         9216 | elapsed time per iteration (ms): 41125.3 | learning rate: 1.422E-06 | global batch size:  1024 |tokens per sec: 25497.11920734318 |TFLOPS: 10.932381444606706 | load balancing loss: 1.835938E-01 | lm loss: 1.080178E+01 | loss scale: 1.0 | grad norm: 221.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/   63312 | consumed samples:        10240 | elapsed time per iteration (ms): 42358.8 | learning rate: 1.579E-06 | global batch size:  1024 |tokens per sec: 24754.62023301371 |TFLOPS: 10.614020693982722 | load balancing loss: 1.943359E-01 | lm loss: 1.011669E+01 | loss scale: 1.0 | grad norm: 123.516 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/   63312 | consumed samples:        11264 | elapsed time per iteration (ms): 43750.4 | learning rate: 1.737E-06 | global batch size:  1024 |tokens per sec: 23967.258385228328 |TFLOPS: 10.276424121408313 | load balancing loss: 2.119141E-01 | lm loss: 9.710816E+00 | loss scale: 1.0 | grad norm: 65.413 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/   63312 | consumed samples:        12288 | elapsed time per iteration (ms): 46787.4 | learning rate: 1.895E-06 | global batch size:  1024 |tokens per sec: 22411.523363182554 |TFLOPS: 9.609372736134963 | load balancing loss: 2.480469E-01 | lm loss: 9.786421E+00 | loss scale: 1.0 | grad norm: 48.304 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/   63312 | consumed samples:        13312 | elapsed time per iteration (ms): 44761.3 | learning rate: 2.053E-06 | global batch size:  1024 |tokens per sec: 23425.92903495056 |TFLOPS: 10.044318725646848 | load balancing loss: 2.441406E-01 | lm loss: 9.622053E+00 | loss scale: 1.0 | grad norm: 34.332 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/   63312 | consumed samples:        14336 | elapsed time per iteration (ms): 41722.1 | learning rate: 2.211E-06 | global batch size:  1024 |tokens per sec: 25132.3644498087 |TFLOPS: 10.775985809842073 | load balancing loss: 2.304688E-01 | lm loss: 9.386806E+00 | loss scale: 1.0 | grad norm: 26.222 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/   63312 | consumed samples:        15360 | elapsed time per iteration (ms): 42508.1 | learning rate: 2.369E-06 | global batch size:  1024 |tokens per sec: 24667.684075998794 |TFLOPS: 10.576745140533468 | load balancing loss: 2.138672E-01 | lm loss: 9.171471E+00 | loss scale: 1.0 | grad norm: 36.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/   63312 | consumed samples:        16384 | elapsed time per iteration (ms): 43612.9 | learning rate: 2.527E-06 | global batch size:  1024 |tokens per sec: 24042.789410029698 |TFLOPS: 10.308809504530059 | load balancing loss: 2.187500E-01 | lm loss: 9.219455E+00 | loss scale: 1.0 | grad norm: 55.351 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/   63312 | consumed samples:        17408 | elapsed time per iteration (ms): 44083.5 | learning rate: 2.685E-06 | global batch size:  1024 |tokens per sec: 23786.139358001255 |TFLOPS: 10.198765846509861 | load balancing loss: 2.089844E-01 | lm loss: 9.147180E+00 | loss scale: 1.0 | grad norm: 29.279 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/   63312 | consumed samples:        18432 | elapsed time per iteration (ms): 40870.7 | learning rate: 2.843E-06 | global batch size:  1024 |tokens per sec: 25655.9499360291 |TFLOPS: 11.000483181787242 | load balancing loss: 1.953125E-01 | lm loss: 9.073648E+00 | loss scale: 1.0 | grad norm: 22.288 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/   63312 | consumed samples:        19456 | elapsed time per iteration (ms): 39852.4 | learning rate: 3.001E-06 | global batch size:  1024 |tokens per sec: 26311.458501073837 |TFLOPS: 11.281545117255307 | load balancing loss: 1.835938E-01 | lm loss: 8.966060E+00 | loss scale: 1.0 | grad norm: 23.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/   63312 | consumed samples:        20480 | elapsed time per iteration (ms): 37937.8 | learning rate: 3.159E-06 | global batch size:  1024 |tokens per sec: 27639.357502868403 |TFLOPS: 11.850907416167484 | load balancing loss: 1.826172E-01 | lm loss: 8.823307E+00 | loss scale: 1.0 | grad norm: 10.733 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration      20 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration      20 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (50654.79, 50654.99)
 iteration       21/   63312 | consumed samples:        21504 | elapsed time per iteration (ms): 86421.2 | learning rate: 3.317E-06 | global batch size:  1024 |tokens per sec: 12133.315150760613 |TFLOPS: 5.20239280120472 | load balancing loss: 1.757812E-01 | lm loss: 8.810468E+00 | loss scale: 1.0 | grad norm: 19.019 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/   63312 | consumed samples:        22528 | elapsed time per iteration (ms): 35185.4 | learning rate: 3.475E-06 | global batch size:  1024 |tokens per sec: 29801.48383870358 |TFLOPS: 12.777960768452571 | load balancing loss: 1.748047E-01 | lm loss: 8.746486E+00 | loss scale: 1.0 | grad norm: 9.716 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/   63312 | consumed samples:        23552 | elapsed time per iteration (ms): 34953.4 | learning rate: 3.633E-06 | global batch size:  1024 |tokens per sec: 29999.282433374723 |TFLOPS: 12.862770729474729 | load balancing loss: 1.728516E-01 | lm loss: 8.649531E+00 | loss scale: 1.0 | grad norm: 12.782 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/   63312 | consumed samples:        24576 | elapsed time per iteration (ms): 34760.1 | learning rate: 3.791E-06 | global batch size:  1024 |tokens per sec: 30166.038492678945 |TFLOPS: 12.934270604958236 | load balancing loss: 1.728516E-01 | lm loss: 8.626020E+00 | loss scale: 1.0 | grad norm: 15.965 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/   63312 | consumed samples:        25600 | elapsed time per iteration (ms): 35201.6 | learning rate: 3.949E-06 | global batch size:  1024 |tokens per sec: 29787.715472213808 |TFLOPS: 12.772057315865974 | load balancing loss: 1.728516E-01 | lm loss: 8.529907E+00 | loss scale: 1.0 | grad norm: 14.633 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/   63312 | consumed samples:        26624 | elapsed time per iteration (ms): 34622.3 | learning rate: 4.107E-06 | global batch size:  1024 |tokens per sec: 30286.14305029786 |TFLOPS: 12.985767749653217 | load balancing loss: 1.699219E-01 | lm loss: 8.482941E+00 | loss scale: 1.0 | grad norm: 16.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/   63312 | consumed samples:        27648 | elapsed time per iteration (ms): 33758.7 | learning rate: 4.265E-06 | global batch size:  1024 |tokens per sec: 31060.86814959815 |TFLOPS: 13.317946072678133 | load balancing loss: 1.708984E-01 | lm loss: 8.457218E+00 | loss scale: 1.0 | grad norm: 18.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/   63312 | consumed samples:        28672 | elapsed time per iteration (ms): 33645.6 | learning rate: 4.423E-06 | global batch size:  1024 |tokens per sec: 31165.35939549933 |TFLOPS: 13.362748708949482 | load balancing loss: 1.708984E-01 | lm loss: 8.452458E+00 | loss scale: 1.0 | grad norm: 14.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/   63312 | consumed samples:        29696 | elapsed time per iteration (ms): 32840.3 | learning rate: 4.580E-06 | global batch size:  1024 |tokens per sec: 31929.515353662 |TFLOPS: 13.6903953089386 | load balancing loss: 1.650391E-01 | lm loss: 8.350563E+00 | loss scale: 1.0 | grad norm: 15.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/   63312 | consumed samples:        30720 | elapsed time per iteration (ms): 32243.2 | learning rate: 4.738E-06 | global batch size:  1024 |tokens per sec: 32520.821580429256 |TFLOPS: 13.943929254049115 | load balancing loss: 1.650391E-01 | lm loss: 8.308755E+00 | loss scale: 1.0 | grad norm: 10.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/   63312 | consumed samples:        31744 | elapsed time per iteration (ms): 32139.8 | learning rate: 4.896E-06 | global batch size:  1024 |tokens per sec: 32625.43764750976 |TFLOPS: 13.988785409807653 | load balancing loss: 1.621094E-01 | lm loss: 8.350038E+00 | loss scale: 1.0 | grad norm: 27.200 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/   63312 | consumed samples:        32768 | elapsed time per iteration (ms): 32409.0 | learning rate: 5.054E-06 | global batch size:  1024 |tokens per sec: 32354.509028416287 |TFLOPS: 13.87261954086755 | load balancing loss: 1.630859E-01 | lm loss: 8.309374E+00 | loss scale: 1.0 | grad norm: 11.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/   63312 | consumed samples:        33792 | elapsed time per iteration (ms): 31275.7 | learning rate: 5.212E-06 | global batch size:  1024 |tokens per sec: 33526.85557808672 |TFLOPS: 14.375285726880225 | load balancing loss: 1.591797E-01 | lm loss: 8.263464E+00 | loss scale: 1.0 | grad norm: 12.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/   63312 | consumed samples:        34816 | elapsed time per iteration (ms): 32749.9 | learning rate: 5.370E-06 | global batch size:  1024 |tokens per sec: 32017.722277821507 |TFLOPS: 13.728215728301487 | load balancing loss: 1.591797E-01 | lm loss: 8.206661E+00 | loss scale: 1.0 | grad norm: 10.042 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/   63312 | consumed samples:        35840 | elapsed time per iteration (ms): 33023.9 | learning rate: 5.528E-06 | global batch size:  1024 |tokens per sec: 31752.018743224926 |TFLOPS: 13.614290215079057 | load balancing loss: 1.591797E-01 | lm loss: 8.192047E+00 | loss scale: 1.0 | grad norm: 20.885 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/   63312 | consumed samples:        36864 | elapsed time per iteration (ms): 32281.0 | learning rate: 5.686E-06 | global batch size:  1024 |tokens per sec: 32482.729430809777 |TFLOPS: 13.927596510483118 | load balancing loss: 1.562500E-01 | lm loss: 8.124590E+00 | loss scale: 1.0 | grad norm: 9.926 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/   63312 | consumed samples:        37888 | elapsed time per iteration (ms): 33679.7 | learning rate: 5.844E-06 | global batch size:  1024 |tokens per sec: 31133.728327566965 |TFLOPS: 13.349186278726492 | load balancing loss: 1.552734E-01 | lm loss: 8.159591E+00 | loss scale: 1.0 | grad norm: 11.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/   63312 | consumed samples:        38912 | elapsed time per iteration (ms): 34695.6 | learning rate: 6.002E-06 | global batch size:  1024 |tokens per sec: 30222.1295547834 |TFLOPS: 12.958320729271199 | load balancing loss: 1.552734E-01 | lm loss: 8.034604E+00 | loss scale: 1.0 | grad norm: 10.357 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/   63312 | consumed samples:        39936 | elapsed time per iteration (ms): 34347.4 | learning rate: 6.160E-06 | global batch size:  1024 |tokens per sec: 30528.547983168217 |TFLOPS: 13.089703538188488 | load balancing loss: 1.562500E-01 | lm loss: 8.086981E+00 | loss scale: 1.0 | grad norm: 15.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/   63312 | consumed samples:        40960 | elapsed time per iteration (ms): 32295.2 | learning rate: 6.318E-06 | global batch size:  1024 |tokens per sec: 32468.430717630737 |TFLOPS: 13.921465661528414 | load balancing loss: 1.542969E-01 | lm loss: 7.981927E+00 | loss scale: 1.0 | grad norm: 7.250 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration      40 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration      40 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (51819.95, 51819.98)
 iteration       41/   63312 | consumed samples:        41984 | elapsed time per iteration (ms): 82986.0 | learning rate: 6.476E-06 | global batch size:  1024 |tokens per sec: 12635.571594401188 |TFLOPS: 5.41774493491985 | load balancing loss: 1.523438E-01 | lm loss: 7.991017E+00 | loss scale: 1.0 | grad norm: 10.532 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/   63312 | consumed samples:        43008 | elapsed time per iteration (ms): 31050.9 | learning rate: 6.634E-06 | global batch size:  1024 |tokens per sec: 33769.56327627855 |TFLOPS: 14.479351331884397 | load balancing loss: 1.503906E-01 | lm loss: 7.966203E+00 | loss scale: 1.0 | grad norm: 11.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/   63312 | consumed samples:        44032 | elapsed time per iteration (ms): 31548.7 | learning rate: 6.792E-06 | global batch size:  1024 |tokens per sec: 33236.69862460091 |TFLOPS: 14.250875338847122 | load balancing loss: 1.503906E-01 | lm loss: 7.939274E+00 | loss scale: 1.0 | grad norm: 10.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/   63312 | consumed samples:        45056 | elapsed time per iteration (ms): 31114.4 | learning rate: 6.950E-06 | global batch size:  1024 |tokens per sec: 33700.61980289238 |TFLOPS: 14.449790488439909 | load balancing loss: 1.523438E-01 | lm loss: 7.894177E+00 | loss scale: 1.0 | grad norm: 14.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/   63312 | consumed samples:        46080 | elapsed time per iteration (ms): 31407.3 | learning rate: 7.108E-06 | global batch size:  1024 |tokens per sec: 33386.34158185622 |TFLOPS: 14.315037641886555 | load balancing loss: 1.533203E-01 | lm loss: 7.863571E+00 | loss scale: 1.0 | grad norm: 9.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/   63312 | consumed samples:        47104 | elapsed time per iteration (ms): 31448.3 | learning rate: 7.266E-06 | global batch size:  1024 |tokens per sec: 33342.85500918826 |TFLOPS: 14.296391935434041 | load balancing loss: 1.484375E-01 | lm loss: 7.893085E+00 | loss scale: 1.0 | grad norm: 15.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/   63312 | consumed samples:        48128 | elapsed time per iteration (ms): 32056.4 | learning rate: 7.424E-06 | global batch size:  1024 |tokens per sec: 32710.34381393994 |TFLOPS: 14.025190565655482 | load balancing loss: 1.503906E-01 | lm loss: 7.854693E+00 | loss scale: 1.0 | grad norm: 10.747 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/   63312 | consumed samples:        49152 | elapsed time per iteration (ms): 29981.9 | learning rate: 7.582E-06 | global batch size:  1024 |tokens per sec: 34973.61882349005 |TFLOPS: 14.995613361942274 | load balancing loss: 1.503906E-01 | lm loss: 7.800607E+00 | loss scale: 1.0 | grad norm: 9.251 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/   63312 | consumed samples:        50176 | elapsed time per iteration (ms): 31142.7 | learning rate: 7.739E-06 | global batch size:  1024 |tokens per sec: 33670.03583006666 |TFLOPS: 14.436677020431885 | load balancing loss: 1.503906E-01 | lm loss: 7.788132E+00 | loss scale: 1.0 | grad norm: 8.717 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/   63312 | consumed samples:        51200 | elapsed time per iteration (ms): 30446.8 | learning rate: 7.897E-06 | global batch size:  1024 |tokens per sec: 34439.61675410906 |TFLOPS: 14.766649679135279 | load balancing loss: 1.494141E-01 | lm loss: 7.700587E+00 | loss scale: 1.0 | grad norm: 8.698 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/   63312 | consumed samples:        52224 | elapsed time per iteration (ms): 30726.4 | learning rate: 8.055E-06 | global batch size:  1024 |tokens per sec: 34126.20528016046 |TFLOPS: 14.632268467106597 | load balancing loss: 1.503906E-01 | lm loss: 7.758260E+00 | loss scale: 1.0 | grad norm: 17.063 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/   63312 | consumed samples:        53248 | elapsed time per iteration (ms): 30866.1 | learning rate: 8.213E-06 | global batch size:  1024 |tokens per sec: 33971.71791781154 |TFLOPS: 14.566029031983154 | load balancing loss: 1.484375E-01 | lm loss: 7.682914E+00 | loss scale: 1.0 | grad norm: 9.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/   63312 | consumed samples:        54272 | elapsed time per iteration (ms): 31065.0 | learning rate: 8.371E-06 | global batch size:  1024 |tokens per sec: 33754.27216584614 |TFLOPS: 14.47279497347389 | load balancing loss: 1.484375E-01 | lm loss: 7.660531E+00 | loss scale: 1.0 | grad norm: 11.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/   63312 | consumed samples:        55296 | elapsed time per iteration (ms): 30900.1 | learning rate: 8.529E-06 | global batch size:  1024 |tokens per sec: 33934.3338473222 |TFLOPS: 14.54999989099597 | load balancing loss: 1.494141E-01 | lm loss: 7.679572E+00 | loss scale: 1.0 | grad norm: 8.706 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/   63312 | consumed samples:        56320 | elapsed time per iteration (ms): 30969.2 | learning rate: 8.687E-06 | global batch size:  1024 |tokens per sec: 33858.69788572913 |TFLOPS: 14.517569514201602 | load balancing loss: 1.464844E-01 | lm loss: 7.620728E+00 | loss scale: 1.0 | grad norm: 8.025 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/   63312 | consumed samples:        57344 | elapsed time per iteration (ms): 32461.3 | learning rate: 8.845E-06 | global batch size:  1024 |tokens per sec: 32302.346643010456 |TFLOPS: 13.85025391243401 | load balancing loss: 1.494141E-01 | lm loss: 7.626801E+00 | loss scale: 1.0 | grad norm: 10.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/   63312 | consumed samples:        58368 | elapsed time per iteration (ms): 31519.5 | learning rate: 9.003E-06 | global batch size:  1024 |tokens per sec: 33267.49000012746 |TFLOPS: 14.264077734761848 | load balancing loss: 1.533203E-01 | lm loss: 7.598485E+00 | loss scale: 1.0 | grad norm: 9.147 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/   63312 | consumed samples:        59392 | elapsed time per iteration (ms): 32055.5 | learning rate: 9.161E-06 | global batch size:  1024 |tokens per sec: 32711.305536896663 |TFLOPS: 14.025602922915196 | load balancing loss: 1.503906E-01 | lm loss: 7.603839E+00 | loss scale: 1.0 | grad norm: 10.128 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/   63312 | consumed samples:        60416 | elapsed time per iteration (ms): 32290.7 | learning rate: 9.319E-06 | global batch size:  1024 |tokens per sec: 32472.953712797025 |TFLOPS: 13.923404982909307 | load balancing loss: 1.542969E-01 | lm loss: 7.509286E+00 | loss scale: 1.0 | grad norm: 10.133 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/   63312 | consumed samples:        61440 | elapsed time per iteration (ms): 34497.5 | learning rate: 9.477E-06 | global batch size:  1024 |tokens per sec: 30395.704422634655 |TFLOPS: 13.032744300385877 | load balancing loss: 1.591797E-01 | lm loss: 7.544779E+00 | loss scale: 1.0 | grad norm: 9.802 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration      60 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration      60 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (49336.55, 49336.57)
 iteration       61/   63312 | consumed samples:        62464 | elapsed time per iteration (ms): 82277.4 | learning rate: 9.635E-06 | global batch size:  1024 |tokens per sec: 12744.402221366963 |TFLOPS: 5.4644081644859135 | load balancing loss: 1.582031E-01 | lm loss: 7.475405E+00 | loss scale: 1.0 | grad norm: 7.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/   63312 | consumed samples:        63488 | elapsed time per iteration (ms): 33127.9 | learning rate: 9.793E-06 | global batch size:  1024 |tokens per sec: 31652.32600036773 |TFLOPS: 13.571545029502953 | load balancing loss: 1.601562E-01 | lm loss: 7.564914E+00 | loss scale: 1.0 | grad norm: 19.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/   63312 | consumed samples:        64512 | elapsed time per iteration (ms): 33610.9 | learning rate: 9.951E-06 | global batch size:  1024 |tokens per sec: 31197.519744700017 |TFLOPS: 13.37653807872081 | load balancing loss: 1.601562E-01 | lm loss: 7.472504E+00 | loss scale: 1.0 | grad norm: 8.039 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/   63312 | consumed samples:        65536 | elapsed time per iteration (ms): 34149.3 | learning rate: 1.011E-05 | global batch size:  1024 |tokens per sec: 30705.64734907643 |TFLOPS: 13.165638305797408 | load balancing loss: 1.591797E-01 | lm loss: 7.463962E+00 | loss scale: 1.0 | grad norm: 9.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/   63312 | consumed samples:        66560 | elapsed time per iteration (ms): 32394.8 | learning rate: 1.027E-05 | global batch size:  1024 |tokens per sec: 32368.660387286556 |TFLOPS: 13.878687208821377 | load balancing loss: 1.640625E-01 | lm loss: 7.407728E+00 | loss scale: 1.0 | grad norm: 9.832 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/   63312 | consumed samples:        67584 | elapsed time per iteration (ms): 32257.9 | learning rate: 1.042E-05 | global batch size:  1024 |tokens per sec: 32506.00328166732 |TFLOPS: 13.937575622758134 | load balancing loss: 1.650391E-01 | lm loss: 7.438332E+00 | loss scale: 1.0 | grad norm: 10.610 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/   63312 | consumed samples:        68608 | elapsed time per iteration (ms): 32810.5 | learning rate: 1.058E-05 | global batch size:  1024 |tokens per sec: 31958.583386346985 |TFLOPS: 13.702858788383958 | load balancing loss: 1.669922E-01 | lm loss: 7.444047E+00 | loss scale: 1.0 | grad norm: 8.233 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/   63312 | consumed samples:        69632 | elapsed time per iteration (ms): 32768.5 | learning rate: 1.074E-05 | global batch size:  1024 |tokens per sec: 31999.503257013675 |TFLOPS: 13.720403971867409 | load balancing loss: 1.660156E-01 | lm loss: 7.399650E+00 | loss scale: 1.0 | grad norm: 14.068 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/   63312 | consumed samples:        70656 | elapsed time per iteration (ms): 33578.4 | learning rate: 1.090E-05 | global batch size:  1024 |tokens per sec: 31227.68499906467 |TFLOPS: 13.38947201311576 | load balancing loss: 1.640625E-01 | lm loss: 7.374393E+00 | loss scale: 1.0 | grad norm: 6.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/   63312 | consumed samples:        71680 | elapsed time per iteration (ms): 32637.8 | learning rate: 1.106E-05 | global batch size:  1024 |tokens per sec: 32127.696755619072 |TFLOPS: 13.775369405965126 | load balancing loss: 1.679688E-01 | lm loss: 7.359175E+00 | loss scale: 1.0 | grad norm: 9.926 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/   63312 | consumed samples:        72704 | elapsed time per iteration (ms): 33141.9 | learning rate: 1.121E-05 | global batch size:  1024 |tokens per sec: 31639.006744130435 |TFLOPS: 13.565834141595952 | load balancing loss: 1.660156E-01 | lm loss: 7.321002E+00 | loss scale: 1.0 | grad norm: 9.387 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/   63312 | consumed samples:        73728 | elapsed time per iteration (ms): 31834.6 | learning rate: 1.137E-05 | global batch size:  1024 |tokens per sec: 32938.26514781879 |TFLOPS: 14.122916231879355 | load balancing loss: 1.669922E-01 | lm loss: 7.309194E+00 | loss scale: 1.0 | grad norm: 7.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/   63312 | consumed samples:        74752 | elapsed time per iteration (ms): 33017.1 | learning rate: 1.153E-05 | global batch size:  1024 |tokens per sec: 31758.593217977203 |TFLOPS: 13.61710914788497 | load balancing loss: 1.660156E-01 | lm loss: 7.349411E+00 | loss scale: 1.0 | grad norm: 6.667 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/   63312 | consumed samples:        75776 | elapsed time per iteration (ms): 32778.9 | learning rate: 1.169E-05 | global batch size:  1024 |tokens per sec: 31989.354444590488 |TFLOPS: 13.716052472871864 | load balancing loss: 1.669922E-01 | lm loss: 7.229241E+00 | loss scale: 1.0 | grad norm: 11.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/   63312 | consumed samples:        76800 | elapsed time per iteration (ms): 32579.9 | learning rate: 1.185E-05 | global batch size:  1024 |tokens per sec: 32184.732373564224 |TFLOPS: 13.799824526805823 | load balancing loss: 1.669922E-01 | lm loss: 7.262443E+00 | loss scale: 1.0 | grad norm: 6.161 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/   63312 | consumed samples:        77824 | elapsed time per iteration (ms): 32097.7 | learning rate: 1.200E-05 | global batch size:  1024 |tokens per sec: 32668.215434960952 |TFLOPS: 14.007127210933094 | load balancing loss: 1.679688E-01 | lm loss: 7.255510E+00 | loss scale: 1.0 | grad norm: 10.048 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/   63312 | consumed samples:        78848 | elapsed time per iteration (ms): 33950.8 | learning rate: 1.216E-05 | global batch size:  1024 |tokens per sec: 30885.145503971635 |TFLOPS: 13.242601600433156 | load balancing loss: 1.718750E-01 | lm loss: 7.279901E+00 | loss scale: 1.0 | grad norm: 8.920 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/   63312 | consumed samples:        79872 | elapsed time per iteration (ms): 34349.6 | learning rate: 1.232E-05 | global batch size:  1024 |tokens per sec: 30526.600437998637 |TFLOPS: 13.08886849064836 | load balancing loss: 1.699219E-01 | lm loss: 7.220896E+00 | loss scale: 1.0 | grad norm: 6.924 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/   63312 | consumed samples:        80896 | elapsed time per iteration (ms): 33011.6 | learning rate: 1.248E-05 | global batch size:  1024 |tokens per sec: 31763.824660197657 |TFLOPS: 13.619352229599194 | load balancing loss: 1.738281E-01 | lm loss: 7.223961E+00 | loss scale: 1.0 | grad norm: 7.813 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/   63312 | consumed samples:        81920 | elapsed time per iteration (ms): 34281.0 | learning rate: 1.264E-05 | global batch size:  1024 |tokens per sec: 30587.641502973456 |TFLOPS: 13.115041024128047 | load balancing loss: 1.806641E-01 | lm loss: 7.240505E+00 | loss scale: 1.0 | grad norm: 13.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration      80 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration      80 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (50520.91, 50520.94)
 iteration       81/   63312 | consumed samples:        82944 | elapsed time per iteration (ms): 84753.2 | learning rate: 1.279E-05 | global batch size:  1024 |tokens per sec: 12372.104611716519 |TFLOPS: 5.304778386450371 | load balancing loss: 1.767578E-01 | lm loss: 7.223489E+00 | loss scale: 1.0 | grad norm: 6.793 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/   63312 | consumed samples:        83968 | elapsed time per iteration (ms): 36033.9 | learning rate: 1.295E-05 | global batch size:  1024 |tokens per sec: 29099.692116233076 |TFLOPS: 12.477054036898934 | load balancing loss: 1.806641E-01 | lm loss: 7.126004E+00 | loss scale: 1.0 | grad norm: 11.820 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/   63312 | consumed samples:        84992 | elapsed time per iteration (ms): 33149.6 | learning rate: 1.311E-05 | global batch size:  1024 |tokens per sec: 31631.660381689555 |TFLOPS: 13.562684247061556 | load balancing loss: 1.777344E-01 | lm loss: 7.140805E+00 | loss scale: 1.0 | grad norm: 6.689 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/   63312 | consumed samples:        86016 | elapsed time per iteration (ms): 33516.8 | learning rate: 1.327E-05 | global batch size:  1024 |tokens per sec: 31285.091401038564 |TFLOPS: 13.414086114757495 | load balancing loss: 1.806641E-01 | lm loss: 7.093565E+00 | loss scale: 1.0 | grad norm: 7.886 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/   63312 | consumed samples:        87040 | elapsed time per iteration (ms): 36714.6 | learning rate: 1.343E-05 | global batch size:  1024 |tokens per sec: 28560.193842986053 |TFLOPS: 12.245733750717564 | load balancing loss: 1.855469E-01 | lm loss: 7.256129E+00 | loss scale: 1.0 | grad norm: 22.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/   63312 | consumed samples:        88064 | elapsed time per iteration (ms): 34299.2 | learning rate: 1.358E-05 | global batch size:  1024 |tokens per sec: 30571.479674025002 |TFLOPS: 13.108111328366334 | load balancing loss: 1.894531E-01 | lm loss: 7.241369E+00 | loss scale: 1.0 | grad norm: 7.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/   63312 | consumed samples:        89088 | elapsed time per iteration (ms): 35722.9 | learning rate: 1.374E-05 | global batch size:  1024 |tokens per sec: 29353.017162617067 |TFLOPS: 12.585672034642961 | load balancing loss: 1.962891E-01 | lm loss: 7.101591E+00 | loss scale: 1.0 | grad norm: 5.977 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/   63312 | consumed samples:        90112 | elapsed time per iteration (ms): 35922.0 | learning rate: 1.390E-05 | global batch size:  1024 |tokens per sec: 29190.33369139233 |TFLOPS: 12.515918359818032 | load balancing loss: 1.972656E-01 | lm loss: 7.132151E+00 | loss scale: 1.0 | grad norm: 11.614 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/   63312 | consumed samples:        91136 | elapsed time per iteration (ms): 36773.1 | learning rate: 1.406E-05 | global batch size:  1024 |tokens per sec: 28514.735713039863 |TFLOPS: 12.22624270107039 | load balancing loss: 1.972656E-01 | lm loss: 7.145463E+00 | loss scale: 1.0 | grad norm: 9.949 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/   63312 | consumed samples:        92160 | elapsed time per iteration (ms): 38163.4 | learning rate: 1.422E-05 | global batch size:  1024 |tokens per sec: 27475.934828521495 |TFLOPS: 11.780836793752083 | load balancing loss: 1.992188E-01 | lm loss: 7.157590E+00 | loss scale: 1.0 | grad norm: 5.550 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/   63312 | consumed samples:        93184 | elapsed time per iteration (ms): 35501.1 | learning rate: 1.437E-05 | global batch size:  1024 |tokens per sec: 29536.47235312337 |TFLOPS: 12.664331984588614 | load balancing loss: 1.972656E-01 | lm loss: 7.077860E+00 | loss scale: 1.0 | grad norm: 6.063 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/   63312 | consumed samples:        94208 | elapsed time per iteration (ms): 36593.4 | learning rate: 1.453E-05 | global batch size:  1024 |tokens per sec: 28654.763831497803 |TFLOPS: 12.286282456601356 | load balancing loss: 1.992188E-01 | lm loss: 7.131628E+00 | loss scale: 1.0 | grad norm: 16.862 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/   63312 | consumed samples:        95232 | elapsed time per iteration (ms): 37853.0 | learning rate: 1.469E-05 | global batch size:  1024 |tokens per sec: 27701.258671315558 |TFLOPS: 11.87744873559373 | load balancing loss: 1.972656E-01 | lm loss: 7.140945E+00 | loss scale: 1.0 | grad norm: 10.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/   63312 | consumed samples:        96256 | elapsed time per iteration (ms): 36588.2 | learning rate: 1.485E-05 | global batch size:  1024 |tokens per sec: 28658.847443947816 |TFLOPS: 12.288033384171346 | load balancing loss: 2.011719E-01 | lm loss: 7.091949E+00 | loss scale: 1.0 | grad norm: 5.511 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/   63312 | consumed samples:        97280 | elapsed time per iteration (ms): 35793.8 | learning rate: 1.501E-05 | global batch size:  1024 |tokens per sec: 29294.88367738849 |TFLOPS: 12.560746182037615 | load balancing loss: 2.001953E-01 | lm loss: 7.044616E+00 | loss scale: 1.0 | grad norm: 9.774 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/   63312 | consumed samples:        98304 | elapsed time per iteration (ms): 35138.7 | learning rate: 1.516E-05 | global batch size:  1024 |tokens per sec: 29841.092037693772 |TFLOPS: 12.794943547415691 | load balancing loss: 2.070312E-01 | lm loss: 7.095722E+00 | loss scale: 1.0 | grad norm: 9.053 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/   63312 | consumed samples:        99328 | elapsed time per iteration (ms): 36636.8 | learning rate: 1.532E-05 | global batch size:  1024 |tokens per sec: 28620.818416753937 |TFLOPS: 12.271727705562327 | load balancing loss: 2.109375E-01 | lm loss: 7.113984E+00 | loss scale: 1.0 | grad norm: 10.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/   63312 | consumed samples:       100352 | elapsed time per iteration (ms): 36964.1 | learning rate: 1.548E-05 | global batch size:  1024 |tokens per sec: 28367.42958101311 |TFLOPS: 12.163082356901691 | load balancing loss: 2.080078E-01 | lm loss: 7.114766E+00 | loss scale: 1.0 | grad norm: 5.536 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/   63312 | consumed samples:       101376 | elapsed time per iteration (ms): 34903.1 | learning rate: 1.564E-05 | global batch size:  1024 |tokens per sec: 30042.52270627833 |TFLOPS: 12.881310830154613 | load balancing loss: 2.119141E-01 | lm loss: 6.992500E+00 | loss scale: 1.0 | grad norm: 7.277 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/   63312 | consumed samples:       102400 | elapsed time per iteration (ms): 32928.8 | learning rate: 1.579E-05 | global batch size:  1024 |tokens per sec: 31843.783198840145 |TFLOPS: 13.653635994642785 | load balancing loss: 2.109375E-01 | lm loss: 6.986042E+00 | loss scale: 1.0 | grad norm: 7.166 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 100 | lm loss value: 6.792115E+00 | lm loss PPL: 8.907958E+02 | 
-----------------------------------------------------------------------------------------------
saving checkpoint at iteration     100 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     100 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (53034.07, 53034.14)
 iteration      101/   63312 | consumed samples:       103424 | elapsed time per iteration (ms): 222995.0 | learning rate: 1.595E-05 | global batch size:  1024 |tokens per sec: 4702.239460203159 |TFLOPS: 2.016175827738897 | load balancing loss: 2.128906E-01 | lm loss: 6.975705E+00 | loss scale: 1.0 | grad norm: 3.726 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/   63312 | consumed samples:       104448 | elapsed time per iteration (ms): 33316.4 | learning rate: 1.611E-05 | global batch size:  1024 |tokens per sec: 31473.252155605147 |TFLOPS: 13.494763666017267 | load balancing loss: 2.050781E-01 | lm loss: 7.021855E+00 | loss scale: 1.0 | grad norm: 8.366 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/   63312 | consumed samples:       105472 | elapsed time per iteration (ms): 34498.1 | learning rate: 1.627E-05 | global batch size:  1024 |tokens per sec: 30395.217697597665 |TFLOPS: 13.03253560764221 | load balancing loss: 2.128906E-01 | lm loss: 6.964662E+00 | loss scale: 1.0 | grad norm: 4.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/   63312 | consumed samples:       106496 | elapsed time per iteration (ms): 32029.9 | learning rate: 1.643E-05 | global batch size:  1024 |tokens per sec: 32737.380932743625 |TFLOPS: 14.036783251618212 | load balancing loss: 1.992188E-01 | lm loss: 6.990533E+00 | loss scale: 1.0 | grad norm: 8.791 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/   63312 | consumed samples:       107520 | elapsed time per iteration (ms): 33852.2 | learning rate: 1.658E-05 | global batch size:  1024 |tokens per sec: 30975.142176751364 |TFLOPS: 13.281189409023316 | load balancing loss: 2.021484E-01 | lm loss: 6.943775E+00 | loss scale: 1.0 | grad norm: 5.992 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/   63312 | consumed samples:       108544 | elapsed time per iteration (ms): 35215.4 | learning rate: 1.674E-05 | global batch size:  1024 |tokens per sec: 29776.062083258865 |TFLOPS: 12.767060700674204 | load balancing loss: 2.089844E-01 | lm loss: 6.914688E+00 | loss scale: 1.0 | grad norm: 4.857 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/   63312 | consumed samples:       109568 | elapsed time per iteration (ms): 33952.3 | learning rate: 1.690E-05 | global batch size:  1024 |tokens per sec: 30883.811471338304 |TFLOPS: 13.242029608221465 | load balancing loss: 2.070312E-01 | lm loss: 6.899120E+00 | loss scale: 1.0 | grad norm: 4.193 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/   63312 | consumed samples:       110592 | elapsed time per iteration (ms): 33143.4 | learning rate: 1.706E-05 | global batch size:  1024 |tokens per sec: 31637.515077324922 |TFLOPS: 13.565194560693751 | load balancing loss: 1.992188E-01 | lm loss: 6.904367E+00 | loss scale: 1.0 | grad norm: 5.057 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/   63312 | consumed samples:       111616 | elapsed time per iteration (ms): 33925.7 | learning rate: 1.722E-05 | global batch size:  1024 |tokens per sec: 30908.011128424863 |TFLOPS: 13.252405677766717 | load balancing loss: 2.021484E-01 | lm loss: 6.883276E+00 | loss scale: 1.0 | grad norm: 4.963 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/   63312 | consumed samples:       112640 | elapsed time per iteration (ms): 32921.9 | learning rate: 1.737E-05 | global batch size:  1024 |tokens per sec: 31850.37520477844 |TFLOPS: 13.656462444282703 | load balancing loss: 1.972656E-01 | lm loss: 6.860090E+00 | loss scale: 1.0 | grad norm: 3.634 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/   63312 | consumed samples:       113664 | elapsed time per iteration (ms): 32196.5 | learning rate: 1.753E-05 | global batch size:  1024 |tokens per sec: 32567.96399463788 |TFLOPS: 13.964142473046808 | load balancing loss: 1.953125E-01 | lm loss: 6.856910E+00 | loss scale: 1.0 | grad norm: 6.498 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/   63312 | consumed samples:       114688 | elapsed time per iteration (ms): 31750.1 | learning rate: 1.769E-05 | global batch size:  1024 |tokens per sec: 33025.89844421353 |TFLOPS: 14.160490697278558 | load balancing loss: 1.953125E-01 | lm loss: 6.871127E+00 | loss scale: 1.0 | grad norm: 4.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/   63312 | consumed samples:       115712 | elapsed time per iteration (ms): 31672.3 | learning rate: 1.785E-05 | global batch size:  1024 |tokens per sec: 33107.062811322976 |TFLOPS: 14.195291484525727 | load balancing loss: 1.943359E-01 | lm loss: 6.823804E+00 | loss scale: 1.0 | grad norm: 2.981 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/   63312 | consumed samples:       116736 | elapsed time per iteration (ms): 30515.6 | learning rate: 1.801E-05 | global batch size:  1024 |tokens per sec: 34361.94639150836 |TFLOPS: 14.733347013685638 | load balancing loss: 1.904297E-01 | lm loss: 6.805369E+00 | loss scale: 1.0 | grad norm: 4.374 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/   63312 | consumed samples:       117760 | elapsed time per iteration (ms): 30893.7 | learning rate: 1.816E-05 | global batch size:  1024 |tokens per sec: 33941.46522910004 |TFLOPS: 14.553057608426258 | load balancing loss: 1.875000E-01 | lm loss: 6.774917E+00 | loss scale: 1.0 | grad norm: 3.842 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/   63312 | consumed samples:       118784 | elapsed time per iteration (ms): 30591.6 | learning rate: 1.832E-05 | global batch size:  1024 |tokens per sec: 34276.56681419671 |TFLOPS: 14.696738873795015 | load balancing loss: 1.806641E-01 | lm loss: 6.777075E+00 | loss scale: 1.0 | grad norm: 4.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/   63312 | consumed samples:       119808 | elapsed time per iteration (ms): 31936.1 | learning rate: 1.848E-05 | global batch size:  1024 |tokens per sec: 32833.58290178868 |TFLOPS: 14.078031700620242 | load balancing loss: 1.835938E-01 | lm loss: 6.854856E+00 | loss scale: 1.0 | grad norm: 7.083 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/   63312 | consumed samples:       120832 | elapsed time per iteration (ms): 32649.8 | learning rate: 1.864E-05 | global batch size:  1024 |tokens per sec: 32115.80177787635 |TFLOPS: 13.77026920492276 | load balancing loss: 1.875000E-01 | lm loss: 6.808050E+00 | loss scale: 1.0 | grad norm: 3.892 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/   63312 | consumed samples:       121856 | elapsed time per iteration (ms): 32557.2 | learning rate: 1.880E-05 | global batch size:  1024 |tokens per sec: 32207.170138823592 |TFLOPS: 13.809445151260892 | load balancing loss: 1.855469E-01 | lm loss: 6.805843E+00 | loss scale: 1.0 | grad norm: 3.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/   63312 | consumed samples:       122880 | elapsed time per iteration (ms): 31689.8 | learning rate: 1.895E-05 | global batch size:  1024 |tokens per sec: 33088.73093332076 |TFLOPS: 14.187431338393669 | load balancing loss: 1.826172E-01 | lm loss: 6.825474E+00 | loss scale: 1.0 | grad norm: 6.902 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     120 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     120 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (47137.21, 47137.25)
 iteration      121/   63312 | consumed samples:       123904 | elapsed time per iteration (ms): 78657.0 | learning rate: 1.911E-05 | global batch size:  1024 |tokens per sec: 13330.994859994966 |TFLOPS: 5.715921067803743 | load balancing loss: 1.826172E-01 | lm loss: 6.749528E+00 | loss scale: 1.0 | grad norm: 4.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/   63312 | consumed samples:       124928 | elapsed time per iteration (ms): 31574.8 | learning rate: 1.927E-05 | global batch size:  1024 |tokens per sec: 33209.30926004537 |TFLOPS: 14.239131620726985 | load balancing loss: 1.796875E-01 | lm loss: 6.730917E+00 | loss scale: 1.0 | grad norm: 2.469 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/   63312 | consumed samples:       125952 | elapsed time per iteration (ms): 30975.3 | learning rate: 1.943E-05 | global batch size:  1024 |tokens per sec: 33851.99441945228 |TFLOPS: 14.514695273792574 | load balancing loss: 1.806641E-01 | lm loss: 6.756193E+00 | loss scale: 1.0 | grad norm: 4.938 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/   63312 | consumed samples:       126976 | elapsed time per iteration (ms): 31334.8 | learning rate: 1.959E-05 | global batch size:  1024 |tokens per sec: 33463.58665460104 |TFLOPS: 14.348157956110894 | load balancing loss: 1.845703E-01 | lm loss: 6.761873E+00 | loss scale: 1.0 | grad norm: 8.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/   63312 | consumed samples:       128000 | elapsed time per iteration (ms): 32586.4 | learning rate: 1.974E-05 | global batch size:  1024 |tokens per sec: 32178.29766009474 |TFLOPS: 13.797065519344505 | load balancing loss: 1.865234E-01 | lm loss: 6.737174E+00 | loss scale: 1.0 | grad norm: 3.051 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/   63312 | consumed samples:       129024 | elapsed time per iteration (ms): 33558.3 | learning rate: 1.990E-05 | global batch size:  1024 |tokens per sec: 31246.358971654125 |TFLOPS: 13.39747883889768 | load balancing loss: 1.816406E-01 | lm loss: 6.753170E+00 | loss scale: 1.0 | grad norm: 3.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/   63312 | consumed samples:       130048 | elapsed time per iteration (ms): 31361.2 | learning rate: 2.006E-05 | global batch size:  1024 |tokens per sec: 33435.48949087608 |TFLOPS: 14.336110755450505 | load balancing loss: 1.826172E-01 | lm loss: 6.705260E+00 | loss scale: 1.0 | grad norm: 5.229 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/   63312 | consumed samples:       131072 | elapsed time per iteration (ms): 30530.1 | learning rate: 2.022E-05 | global batch size:  1024 |tokens per sec: 34345.64994338845 |TFLOPS: 14.726359597358709 | load balancing loss: 1.806641E-01 | lm loss: 6.663682E+00 | loss scale: 1.0 | grad norm: 2.842 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/   63312 | consumed samples:       132096 | elapsed time per iteration (ms): 31363.7 | learning rate: 2.038E-05 | global batch size:  1024 |tokens per sec: 33432.760239731186 |TFLOPS: 14.334940536402168 | load balancing loss: 1.816406E-01 | lm loss: 6.664202E+00 | loss scale: 1.0 | grad norm: 3.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/   63312 | consumed samples:       133120 | elapsed time per iteration (ms): 31400.5 | learning rate: 2.053E-05 | global batch size:  1024 |tokens per sec: 33393.636716988905 |TFLOPS: 14.318165571724897 | load balancing loss: 1.816406E-01 | lm loss: 6.735136E+00 | loss scale: 1.0 | grad norm: 3.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/   63312 | consumed samples:       134144 | elapsed time per iteration (ms): 30582.2 | learning rate: 2.069E-05 | global batch size:  1024 |tokens per sec: 34287.182671438015 |TFLOPS: 14.701290627260954 | load balancing loss: 1.835938E-01 | lm loss: 6.706905E+00 | loss scale: 1.0 | grad norm: 2.964 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/   63312 | consumed samples:       135168 | elapsed time per iteration (ms): 31409.6 | learning rate: 2.085E-05 | global batch size:  1024 |tokens per sec: 33383.976886083656 |TFLOPS: 14.314023732982733 | load balancing loss: 1.855469E-01 | lm loss: 6.658626E+00 | loss scale: 1.0 | grad norm: 2.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/   63312 | consumed samples:       136192 | elapsed time per iteration (ms): 31659.1 | learning rate: 2.101E-05 | global batch size:  1024 |tokens per sec: 33120.837402069956 |TFLOPS: 14.201197605882605 | load balancing loss: 1.806641E-01 | lm loss: 6.691973E+00 | loss scale: 1.0 | grad norm: 5.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/   63312 | consumed samples:       137216 | elapsed time per iteration (ms): 31330.2 | learning rate: 2.117E-05 | global batch size:  1024 |tokens per sec: 33468.531003205484 |TFLOPS: 14.350277940902094 | load balancing loss: 1.816406E-01 | lm loss: 6.680428E+00 | loss scale: 1.0 | grad norm: 3.926 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/   63312 | consumed samples:       138240 | elapsed time per iteration (ms): 31382.4 | learning rate: 2.132E-05 | global batch size:  1024 |tokens per sec: 33412.86222889088 |TFLOPS: 14.326408880620736 | load balancing loss: 1.845703E-01 | lm loss: 6.627075E+00 | loss scale: 1.0 | grad norm: 2.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/   63312 | consumed samples:       139264 | elapsed time per iteration (ms): 31587.9 | learning rate: 2.148E-05 | global batch size:  1024 |tokens per sec: 33195.50158690632 |TFLOPS: 14.233211314656682 | load balancing loss: 1.826172E-01 | lm loss: 6.665030E+00 | loss scale: 1.0 | grad norm: 5.612 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/   63312 | consumed samples:       140288 | elapsed time per iteration (ms): 31500.9 | learning rate: 2.164E-05 | global batch size:  1024 |tokens per sec: 33287.17488235988 |TFLOPS: 14.27251800754353 | load balancing loss: 1.816406E-01 | lm loss: 6.624550E+00 | loss scale: 1.0 | grad norm: 4.115 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/   63312 | consumed samples:       141312 | elapsed time per iteration (ms): 32150.5 | learning rate: 2.180E-05 | global batch size:  1024 |tokens per sec: 32614.63112255266 |TFLOPS: 13.984151903882497 | load balancing loss: 1.787109E-01 | lm loss: 6.661805E+00 | loss scale: 1.0 | grad norm: 2.954 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/   63312 | consumed samples:       142336 | elapsed time per iteration (ms): 31501.3 | learning rate: 2.195E-05 | global batch size:  1024 |tokens per sec: 33286.740042671816 |TFLOPS: 14.272331561643565 | load balancing loss: 1.767578E-01 | lm loss: 6.594681E+00 | loss scale: 1.0 | grad norm: 2.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/   63312 | consumed samples:       143360 | elapsed time per iteration (ms): 31567.8 | learning rate: 2.211E-05 | global batch size:  1024 |tokens per sec: 33216.64388141276 |TFLOPS: 14.242276481049753 | load balancing loss: 1.757812E-01 | lm loss: 6.651506E+00 | loss scale: 1.0 | grad norm: 5.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     140 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     140 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (45471.00, 45471.15)
 iteration      141/   63312 | consumed samples:       144384 | elapsed time per iteration (ms): 75826.7 | learning rate: 2.227E-05 | global batch size:  1024 |tokens per sec: 13828.57862412116 |TFLOPS: 5.929269700087821 | load balancing loss: 1.738281E-01 | lm loss: 6.650643E+00 | loss scale: 1.0 | grad norm: 4.131 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/   63312 | consumed samples:       145408 | elapsed time per iteration (ms): 30386.6 | learning rate: 2.243E-05 | global batch size:  1024 |tokens per sec: 34507.836296053705 |TFLOPS: 14.795900123016814 | load balancing loss: 1.748047E-01 | lm loss: 6.610751E+00 | loss scale: 1.0 | grad norm: 2.460 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/   63312 | consumed samples:       146432 | elapsed time per iteration (ms): 30315.2 | learning rate: 2.259E-05 | global batch size:  1024 |tokens per sec: 34589.172401960845 |TFLOPS: 14.830774546584623 | load balancing loss: 1.757812E-01 | lm loss: 6.550292E+00 | loss scale: 1.0 | grad norm: 2.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/   63312 | consumed samples:       147456 | elapsed time per iteration (ms): 30649.2 | learning rate: 2.274E-05 | global batch size:  1024 |tokens per sec: 34212.229666960455 |TFLOPS: 14.669153081497276 | load balancing loss: 1.757812E-01 | lm loss: 6.588144E+00 | loss scale: 1.0 | grad norm: 2.862 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/   63312 | consumed samples:       148480 | elapsed time per iteration (ms): 30548.6 | learning rate: 2.290E-05 | global batch size:  1024 |tokens per sec: 34324.84689297043 |TFLOPS: 14.717439888409169 | load balancing loss: 1.718750E-01 | lm loss: 6.583375E+00 | loss scale: 1.0 | grad norm: 2.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/   63312 | consumed samples:       149504 | elapsed time per iteration (ms): 30822.6 | learning rate: 2.306E-05 | global batch size:  1024 |tokens per sec: 34019.68188038392 |TFLOPS: 14.586594505681258 | load balancing loss: 1.660156E-01 | lm loss: 6.572185E+00 | loss scale: 1.0 | grad norm: 3.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/   63312 | consumed samples:       150528 | elapsed time per iteration (ms): 31189.1 | learning rate: 2.322E-05 | global batch size:  1024 |tokens per sec: 33619.926115405695 |TFLOPS: 14.415191514155696 | load balancing loss: 1.630859E-01 | lm loss: 6.543800E+00 | loss scale: 1.0 | grad norm: 1.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/   63312 | consumed samples:       151552 | elapsed time per iteration (ms): 31052.0 | learning rate: 2.338E-05 | global batch size:  1024 |tokens per sec: 33768.35138351381 |TFLOPS: 14.478831709496218 | load balancing loss: 1.611328E-01 | lm loss: 6.602891E+00 | loss scale: 1.0 | grad norm: 4.081 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/   63312 | consumed samples:       152576 | elapsed time per iteration (ms): 29848.7 | learning rate: 2.353E-05 | global batch size:  1024 |tokens per sec: 35129.68612799279 |TFLOPS: 15.062530227725457 | load balancing loss: 1.640625E-01 | lm loss: 6.583373E+00 | loss scale: 1.0 | grad norm: 3.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/   63312 | consumed samples:       153600 | elapsed time per iteration (ms): 30373.7 | learning rate: 2.369E-05 | global batch size:  1024 |tokens per sec: 34522.46377690692 |TFLOPS: 14.802171937450465 | load balancing loss: 1.650391E-01 | lm loss: 6.528253E+00 | loss scale: 1.0 | grad norm: 2.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/   63312 | consumed samples:       154624 | elapsed time per iteration (ms): 29745.6 | learning rate: 2.385E-05 | global batch size:  1024 |tokens per sec: 35251.52455102853 |TFLOPS: 15.114770800646827 | load balancing loss: 1.630859E-01 | lm loss: 6.488230E+00 | loss scale: 1.0 | grad norm: 2.175 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/   63312 | consumed samples:       155648 | elapsed time per iteration (ms): 29616.9 | learning rate: 2.401E-05 | global batch size:  1024 |tokens per sec: 35404.62625140104 |TFLOPS: 15.18041610648232 | load balancing loss: 1.601562E-01 | lm loss: 6.478807E+00 | loss scale: 1.0 | grad norm: 2.102 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/   63312 | consumed samples:       156672 | elapsed time per iteration (ms): 29362.7 | learning rate: 2.417E-05 | global batch size:  1024 |tokens per sec: 35711.184047276365 |TFLOPS: 15.311858671898172 | load balancing loss: 1.582031E-01 | lm loss: 6.475605E+00 | loss scale: 1.0 | grad norm: 1.995 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/   63312 | consumed samples:       157696 | elapsed time per iteration (ms): 29713.3 | learning rate: 2.432E-05 | global batch size:  1024 |tokens per sec: 35289.831940283686 |TFLOPS: 15.13119583235644 | load balancing loss: 1.562500E-01 | lm loss: 6.464696E+00 | loss scale: 1.0 | grad norm: 1.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/   63312 | consumed samples:       158720 | elapsed time per iteration (ms): 29108.2 | learning rate: 2.448E-05 | global batch size:  1024 |tokens per sec: 36023.36283761891 |TFLOPS: 15.445711347064615 | load balancing loss: 1.542969E-01 | lm loss: 6.488804E+00 | loss scale: 1.0 | grad norm: 3.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/   63312 | consumed samples:       159744 | elapsed time per iteration (ms): 29287.3 | learning rate: 2.464E-05 | global batch size:  1024 |tokens per sec: 35803.08112378708 |TFLOPS: 15.351261315227777 | load balancing loss: 1.542969E-01 | lm loss: 6.434216E+00 | loss scale: 1.0 | grad norm: 1.955 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/   63312 | consumed samples:       160768 | elapsed time per iteration (ms): 30063.2 | learning rate: 2.480E-05 | global batch size:  1024 |tokens per sec: 34879.04511339476 |TFLOPS: 14.955063060357787 | load balancing loss: 1.552734E-01 | lm loss: 6.419012E+00 | loss scale: 1.0 | grad norm: 2.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/   63312 | consumed samples:       161792 | elapsed time per iteration (ms): 29731.4 | learning rate: 2.496E-05 | global batch size:  1024 |tokens per sec: 35268.30359773389 |TFLOPS: 15.12196514042177 | load balancing loss: 1.533203E-01 | lm loss: 6.429782E+00 | loss scale: 1.0 | grad norm: 2.640 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/   63312 | consumed samples:       162816 | elapsed time per iteration (ms): 29660.3 | learning rate: 2.511E-05 | global batch size:  1024 |tokens per sec: 35352.797908872766 |TFLOPS: 15.158193705372884 | load balancing loss: 1.513672E-01 | lm loss: 6.422844E+00 | loss scale: 1.0 | grad norm: 2.834 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/   63312 | consumed samples:       163840 | elapsed time per iteration (ms): 29313.6 | learning rate: 2.527E-05 | global batch size:  1024 |tokens per sec: 35771.02374630801 |TFLOPS: 15.337516096567388 | load balancing loss: 1.542969E-01 | lm loss: 6.422082E+00 | loss scale: 1.0 | grad norm: 2.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     160 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     160 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (53899.28, 53899.29)
 iteration      161/   63312 | consumed samples:       164864 | elapsed time per iteration (ms): 84421.5 | learning rate: 2.543E-05 | global batch size:  1024 |tokens per sec: 12420.723968056822 |TFLOPS: 5.325624872862466 | load balancing loss: 1.513672E-01 | lm loss: 6.406054E+00 | loss scale: 1.0 | grad norm: 2.147 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/   63312 | consumed samples:       165888 | elapsed time per iteration (ms): 29313.8 | learning rate: 2.559E-05 | global batch size:  1024 |tokens per sec: 35770.75812037439 |TFLOPS: 15.33740220432708 | load balancing loss: 1.513672E-01 | lm loss: 6.417105E+00 | loss scale: 1.0 | grad norm: 2.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/   63312 | consumed samples:       166912 | elapsed time per iteration (ms): 28651.6 | learning rate: 2.575E-05 | global batch size:  1024 |tokens per sec: 36597.52125316576 |TFLOPS: 15.69189283750458 | load balancing loss: 1.484375E-01 | lm loss: 6.382024E+00 | loss scale: 1.0 | grad norm: 1.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/   63312 | consumed samples:       167936 | elapsed time per iteration (ms): 28696.0 | learning rate: 2.590E-05 | global batch size:  1024 |tokens per sec: 36540.85910792177 |TFLOPS: 15.667597850285059 | load balancing loss: 1.464844E-01 | lm loss: 6.387415E+00 | loss scale: 1.0 | grad norm: 1.892 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/   63312 | consumed samples:       168960 | elapsed time per iteration (ms): 28930.2 | learning rate: 2.606E-05 | global batch size:  1024 |tokens per sec: 36245.04207066214 |TFLOPS: 15.540760592207514 | load balancing loss: 1.484375E-01 | lm loss: 6.390592E+00 | loss scale: 1.0 | grad norm: 3.192 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/   63312 | consumed samples:       169984 | elapsed time per iteration (ms): 28726.1 | learning rate: 2.622E-05 | global batch size:  1024 |tokens per sec: 36502.56558618334 |TFLOPS: 15.651178764540608 | load balancing loss: 1.513672E-01 | lm loss: 6.367042E+00 | loss scale: 1.0 | grad norm: 2.187 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/   63312 | consumed samples:       171008 | elapsed time per iteration (ms): 28865.1 | learning rate: 2.638E-05 | global batch size:  1024 |tokens per sec: 36326.771457176364 |TFLOPS: 15.57580364241806 | load balancing loss: 1.484375E-01 | lm loss: 6.390377E+00 | loss scale: 1.0 | grad norm: 2.041 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/   63312 | consumed samples:       172032 | elapsed time per iteration (ms): 29323.3 | learning rate: 2.654E-05 | global batch size:  1024 |tokens per sec: 35759.106174769666 |TFLOPS: 15.332406207999545 | load balancing loss: 1.484375E-01 | lm loss: 6.360089E+00 | loss scale: 1.0 | grad norm: 2.616 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/   63312 | consumed samples:       173056 | elapsed time per iteration (ms): 28974.8 | learning rate: 2.669E-05 | global batch size:  1024 |tokens per sec: 36189.23570141431 |TFLOPS: 15.51683253544571 | load balancing loss: 1.474609E-01 | lm loss: 6.306524E+00 | loss scale: 1.0 | grad norm: 1.935 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/   63312 | consumed samples:       174080 | elapsed time per iteration (ms): 28609.8 | learning rate: 2.685E-05 | global batch size:  1024 |tokens per sec: 36650.9917230469 |TFLOPS: 15.714819332376779 | load balancing loss: 1.474609E-01 | lm loss: 6.377678E+00 | loss scale: 1.0 | grad norm: 2.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/   63312 | consumed samples:       175104 | elapsed time per iteration (ms): 29132.6 | learning rate: 2.701E-05 | global batch size:  1024 |tokens per sec: 35993.18090377512 |TFLOPS: 15.432770261021409 | load balancing loss: 1.474609E-01 | lm loss: 6.321908E+00 | loss scale: 1.0 | grad norm: 1.573 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/   63312 | consumed samples:       176128 | elapsed time per iteration (ms): 29094.6 | learning rate: 2.717E-05 | global batch size:  1024 |tokens per sec: 36040.18343803751 |TFLOPS: 15.452923503795269 | load balancing loss: 1.464844E-01 | lm loss: 6.360221E+00 | loss scale: 1.0 | grad norm: 3.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/   63312 | consumed samples:       177152 | elapsed time per iteration (ms): 29269.3 | learning rate: 2.732E-05 | global batch size:  1024 |tokens per sec: 35825.090359680915 |TFLOPS: 15.360698199455328 | load balancing loss: 1.455078E-01 | lm loss: 6.328958E+00 | loss scale: 1.0 | grad norm: 1.815 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/   63312 | consumed samples:       178176 | elapsed time per iteration (ms): 28607.5 | learning rate: 2.748E-05 | global batch size:  1024 |tokens per sec: 36653.93294419736 |TFLOPS: 15.716080437651783 | load balancing loss: 1.425781E-01 | lm loss: 6.334478E+00 | loss scale: 1.0 | grad norm: 1.892 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/   63312 | consumed samples:       179200 | elapsed time per iteration (ms): 27993.8 | learning rate: 2.764E-05 | global batch size:  1024 |tokens per sec: 37457.43997563231 |TFLOPS: 16.060599568995084 | load balancing loss: 1.425781E-01 | lm loss: 6.288323E+00 | loss scale: 1.0 | grad norm: 1.924 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/   63312 | consumed samples:       180224 | elapsed time per iteration (ms): 28482.4 | learning rate: 2.780E-05 | global batch size:  1024 |tokens per sec: 36814.84780100116 |TFLOPS: 15.785075784944851 | load balancing loss: 1.406250E-01 | lm loss: 6.257942E+00 | loss scale: 1.0 | grad norm: 1.796 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/   63312 | consumed samples:       181248 | elapsed time per iteration (ms): 28282.2 | learning rate: 2.796E-05 | global batch size:  1024 |tokens per sec: 37075.438337428815 |TFLOPS: 15.896809001623748 | load balancing loss: 1.425781E-01 | lm loss: 6.247746E+00 | loss scale: 1.0 | grad norm: 1.705 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/   63312 | consumed samples:       182272 | elapsed time per iteration (ms): 28563.2 | learning rate: 2.811E-05 | global batch size:  1024 |tokens per sec: 36710.76088619474 |TFLOPS: 15.74044651342588 | load balancing loss: 1.396484E-01 | lm loss: 6.280852E+00 | loss scale: 1.0 | grad norm: 1.237 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/   63312 | consumed samples:       183296 | elapsed time per iteration (ms): 29281.7 | learning rate: 2.827E-05 | global batch size:  1024 |tokens per sec: 35809.90145276335 |TFLOPS: 15.354185662772297 | load balancing loss: 1.416016E-01 | lm loss: 6.254364E+00 | loss scale: 1.0 | grad norm: 3.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/   63312 | consumed samples:       184320 | elapsed time per iteration (ms): 28776.1 | learning rate: 2.843E-05 | global batch size:  1024 |tokens per sec: 36439.0804018729 |TFLOPS: 15.623958267773155 | load balancing loss: 1.396484E-01 | lm loss: 6.266970E+00 | loss scale: 1.0 | grad norm: 1.769 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     180 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     180 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (54122.33, 54122.36)
 iteration      181/   63312 | consumed samples:       185344 | elapsed time per iteration (ms): 82140.3 | learning rate: 2.859E-05 | global batch size:  1024 |tokens per sec: 12765.675181666074 |TFLOPS: 5.473529356356831 | load balancing loss: 1.376953E-01 | lm loss: 6.249420E+00 | loss scale: 1.0 | grad norm: 3.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/   63312 | consumed samples:       186368 | elapsed time per iteration (ms): 28109.5 | learning rate: 2.875E-05 | global batch size:  1024 |tokens per sec: 37303.269413813905 |TFLOPS: 15.99449596820701 | load balancing loss: 1.367188E-01 | lm loss: 6.225027E+00 | loss scale: 1.0 | grad norm: 1.911 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/   63312 | consumed samples:       187392 | elapsed time per iteration (ms): 27813.6 | learning rate: 2.890E-05 | global batch size:  1024 |tokens per sec: 37700.08875717408 |TFLOPS: 16.164639912349628 | load balancing loss: 1.376953E-01 | lm loss: 6.222588E+00 | loss scale: 1.0 | grad norm: 2.219 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/   63312 | consumed samples:       188416 | elapsed time per iteration (ms): 27778.0 | learning rate: 2.906E-05 | global batch size:  1024 |tokens per sec: 37748.45459009788 |TFLOPS: 16.18537769570896 | load balancing loss: 1.386719E-01 | lm loss: 6.209358E+00 | loss scale: 1.0 | grad norm: 1.591 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/   63312 | consumed samples:       189440 | elapsed time per iteration (ms): 27658.9 | learning rate: 2.922E-05 | global batch size:  1024 |tokens per sec: 37910.94189070039 |TFLOPS: 16.255047258597447 | load balancing loss: 1.376953E-01 | lm loss: 6.218348E+00 | loss scale: 1.0 | grad norm: 2.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/   63312 | consumed samples:       190464 | elapsed time per iteration (ms): 27788.2 | learning rate: 2.938E-05 | global batch size:  1024 |tokens per sec: 37734.54574431325 |TFLOPS: 16.179414009916258 | load balancing loss: 1.386719E-01 | lm loss: 6.226949E+00 | loss scale: 1.0 | grad norm: 1.919 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/   63312 | consumed samples:       191488 | elapsed time per iteration (ms): 28293.4 | learning rate: 2.954E-05 | global batch size:  1024 |tokens per sec: 37060.83573056879 |TFLOPS: 15.890547852394254 | load balancing loss: 1.367188E-01 | lm loss: 6.215424E+00 | loss scale: 1.0 | grad norm: 1.910 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/   63312 | consumed samples:       192512 | elapsed time per iteration (ms): 27866.7 | learning rate: 2.969E-05 | global batch size:  1024 |tokens per sec: 37628.3010419447 |TFLOPS: 16.133859545377877 | load balancing loss: 1.386719E-01 | lm loss: 6.152912E+00 | loss scale: 1.0 | grad norm: 1.871 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/   63312 | consumed samples:       193536 | elapsed time per iteration (ms): 27763.2 | learning rate: 2.985E-05 | global batch size:  1024 |tokens per sec: 37768.49784811579 |TFLOPS: 16.193971629018154 | load balancing loss: 1.386719E-01 | lm loss: 6.180539E+00 | loss scale: 1.0 | grad norm: 1.424 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/   63312 | consumed samples:       194560 | elapsed time per iteration (ms): 27541.3 | learning rate: 3.001E-05 | global batch size:  1024 |tokens per sec: 38072.915450748515 |TFLOPS: 16.324496545318315 | load balancing loss: 1.367188E-01 | lm loss: 6.154099E+00 | loss scale: 1.0 | grad norm: 2.188 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/   63312 | consumed samples:       195584 | elapsed time per iteration (ms): 27393.7 | learning rate: 3.017E-05 | global batch size:  1024 |tokens per sec: 38278.05302390766 |TFLOPS: 16.412453234862713 | load balancing loss: 1.347656E-01 | lm loss: 6.154392E+00 | loss scale: 1.0 | grad norm: 1.644 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/   63312 | consumed samples:       196608 | elapsed time per iteration (ms): 27351.9 | learning rate: 3.033E-05 | global batch size:  1024 |tokens per sec: 38336.526087472965 |TFLOPS: 16.437524688227 | load balancing loss: 1.347656E-01 | lm loss: 6.139335E+00 | loss scale: 1.0 | grad norm: 1.922 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/   63312 | consumed samples:       197632 | elapsed time per iteration (ms): 27676.2 | learning rate: 3.048E-05 | global batch size:  1024 |tokens per sec: 37887.30420031459 |TFLOPS: 16.24491214310986 | load balancing loss: 1.357422E-01 | lm loss: 6.156377E+00 | loss scale: 1.0 | grad norm: 1.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/   63312 | consumed samples:       198656 | elapsed time per iteration (ms): 27421.9 | learning rate: 3.064E-05 | global batch size:  1024 |tokens per sec: 38238.611097075365 |TFLOPS: 16.395541748293013 | load balancing loss: 1.337891E-01 | lm loss: 6.180622E+00 | loss scale: 1.0 | grad norm: 1.910 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/   63312 | consumed samples:       199680 | elapsed time per iteration (ms): 27357.0 | learning rate: 3.080E-05 | global batch size:  1024 |tokens per sec: 38329.29737802054 |TFLOPS: 16.434425239679758 | load balancing loss: 1.337891E-01 | lm loss: 6.107085E+00 | loss scale: 1.0 | grad norm: 1.654 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/   63312 | consumed samples:       200704 | elapsed time per iteration (ms): 27487.4 | learning rate: 3.096E-05 | global batch size:  1024 |tokens per sec: 38147.45688091875 |TFLOPS: 16.356457620662578 | load balancing loss: 1.347656E-01 | lm loss: 6.144437E+00 | loss scale: 1.0 | grad norm: 1.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/   63312 | consumed samples:       201728 | elapsed time per iteration (ms): 27201.9 | learning rate: 3.112E-05 | global batch size:  1024 |tokens per sec: 38547.956359406824 |TFLOPS: 16.52817949369429 | load balancing loss: 1.367188E-01 | lm loss: 6.058060E+00 | loss scale: 1.0 | grad norm: 1.887 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/   63312 | consumed samples:       202752 | elapsed time per iteration (ms): 27371.6 | learning rate: 3.127E-05 | global batch size:  1024 |tokens per sec: 38308.94928969618 |TFLOPS: 16.425700604499543 | load balancing loss: 1.347656E-01 | lm loss: 6.128755E+00 | loss scale: 1.0 | grad norm: 1.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/   63312 | consumed samples:       203776 | elapsed time per iteration (ms): 26858.5 | learning rate: 3.143E-05 | global batch size:  1024 |tokens per sec: 39040.73465314121 |TFLOPS: 16.739467687898408 | load balancing loss: 1.337891E-01 | lm loss: 6.103922E+00 | loss scale: 1.0 | grad norm: 2.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/   63312 | consumed samples:       204800 | elapsed time per iteration (ms): 26556.5 | learning rate: 3.159E-05 | global batch size:  1024 |tokens per sec: 39484.66530846121 |TFLOPS: 16.929811515349893 | load balancing loss: 1.337891E-01 | lm loss: 6.098602E+00 | loss scale: 1.0 | grad norm: 1.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 200 | lm loss value: 5.832111E+00 | lm loss PPL: 3.410781E+02 | 
-----------------------------------------------------------------------------------------------
saving checkpoint at iteration     200 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     200 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (49961.91, 49961.98)
 iteration      201/   63312 | consumed samples:       205824 | elapsed time per iteration (ms): 187235.7 | learning rate: 3.175E-05 | global batch size:  1024 |tokens per sec: 5600.298071125957 |TFLOPS: 2.401235771742065 | load balancing loss: 1.328125E-01 | lm loss: 6.084144E+00 | loss scale: 1.0 | grad norm: 1.549 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      202/   63312 | consumed samples:       206848 | elapsed time per iteration (ms): 27001.3 | learning rate: 3.191E-05 | global batch size:  1024 |tokens per sec: 38834.29124105356 |TFLOPS: 16.65095109473684 | load balancing loss: 1.328125E-01 | lm loss: 6.075798E+00 | loss scale: 1.0 | grad norm: 1.998 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      203/   63312 | consumed samples:       207872 | elapsed time per iteration (ms): 26663.0 | learning rate: 3.206E-05 | global batch size:  1024 |tokens per sec: 39326.98268985454 |TFLOPS: 16.862202052501395 | load balancing loss: 1.318359E-01 | lm loss: 6.100813E+00 | loss scale: 1.0 | grad norm: 1.481 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      204/   63312 | consumed samples:       208896 | elapsed time per iteration (ms): 26677.4 | learning rate: 3.222E-05 | global batch size:  1024 |tokens per sec: 39305.80490197189 |TFLOPS: 16.853121667638955 | load balancing loss: 1.328125E-01 | lm loss: 6.100480E+00 | loss scale: 1.0 | grad norm: 2.052 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      205/   63312 | consumed samples:       209920 | elapsed time per iteration (ms): 27169.8 | learning rate: 3.238E-05 | global batch size:  1024 |tokens per sec: 38593.42672706939 |TFLOPS: 16.5476757904983 | load balancing loss: 1.328125E-01 | lm loss: 6.089294E+00 | loss scale: 1.0 | grad norm: 1.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      206/   63312 | consumed samples:       210944 | elapsed time per iteration (ms): 26822.1 | learning rate: 3.254E-05 | global batch size:  1024 |tokens per sec: 39093.66545054475 |TFLOPS: 16.76216278779095 | load balancing loss: 1.328125E-01 | lm loss: 6.029367E+00 | loss scale: 1.0 | grad norm: 1.911 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      207/   63312 | consumed samples:       211968 | elapsed time per iteration (ms): 27547.6 | learning rate: 3.270E-05 | global batch size:  1024 |tokens per sec: 38064.12207531638 |TFLOPS: 16.320726216065513 | load balancing loss: 1.337891E-01 | lm loss: 6.121744E+00 | loss scale: 1.0 | grad norm: 2.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      208/   63312 | consumed samples:       212992 | elapsed time per iteration (ms): 26812.9 | learning rate: 3.285E-05 | global batch size:  1024 |tokens per sec: 39107.18329831137 |TFLOPS: 16.767958825644993 | load balancing loss: 1.328125E-01 | lm loss: 6.065332E+00 | loss scale: 1.0 | grad norm: 1.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      209/   63312 | consumed samples:       214016 | elapsed time per iteration (ms): 26675.5 | learning rate: 3.301E-05 | global batch size:  1024 |tokens per sec: 39308.5412132915 |TFLOPS: 16.854294913873325 | load balancing loss: 1.328125E-01 | lm loss: 6.067280E+00 | loss scale: 1.0 | grad norm: 1.648 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      210/   63312 | consumed samples:       215040 | elapsed time per iteration (ms): 26546.8 | learning rate: 3.317E-05 | global batch size:  1024 |tokens per sec: 39499.07716657669 |TFLOPS: 16.935990877377524 | load balancing loss: 1.308594E-01 | lm loss: 6.025848E+00 | loss scale: 1.0 | grad norm: 1.757 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      211/   63312 | consumed samples:       216064 | elapsed time per iteration (ms): 26466.2 | learning rate: 3.333E-05 | global batch size:  1024 |tokens per sec: 39619.5110597218 |TFLOPS: 16.98762923102895 | load balancing loss: 1.308594E-01 | lm loss: 6.000831E+00 | loss scale: 1.0 | grad norm: 2.041 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      212/   63312 | consumed samples:       217088 | elapsed time per iteration (ms): 26506.9 | learning rate: 3.348E-05 | global batch size:  1024 |tokens per sec: 39558.557352843454 |TFLOPS: 16.961494154017394 | load balancing loss: 1.308594E-01 | lm loss: 5.972522E+00 | loss scale: 1.0 | grad norm: 1.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      213/   63312 | consumed samples:       218112 | elapsed time per iteration (ms): 26621.9 | learning rate: 3.364E-05 | global batch size:  1024 |tokens per sec: 39387.7371858372 |TFLOPS: 16.888251714000642 | load balancing loss: 1.328125E-01 | lm loss: 5.994262E+00 | loss scale: 1.0 | grad norm: 2.286 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      214/   63312 | consumed samples:       219136 | elapsed time per iteration (ms): 26900.0 | learning rate: 3.380E-05 | global batch size:  1024 |tokens per sec: 38980.52687221123 |TFLOPS: 16.71365244101866 | load balancing loss: 1.318359E-01 | lm loss: 5.953180E+00 | loss scale: 1.0 | grad norm: 1.265 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      215/   63312 | consumed samples:       220160 | elapsed time per iteration (ms): 26451.2 | learning rate: 3.396E-05 | global batch size:  1024 |tokens per sec: 39641.98019948196 |TFLOPS: 16.997263307906135 | load balancing loss: 1.308594E-01 | lm loss: 6.022758E+00 | loss scale: 1.0 | grad norm: 2.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      216/   63312 | consumed samples:       221184 | elapsed time per iteration (ms): 26815.6 | learning rate: 3.412E-05 | global batch size:  1024 |tokens per sec: 39103.26849814434 |TFLOPS: 16.766280279596028 | load balancing loss: 1.318359E-01 | lm loss: 5.969831E+00 | loss scale: 1.0 | grad norm: 1.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      217/   63312 | consumed samples:       222208 | elapsed time per iteration (ms): 26711.6 | learning rate: 3.427E-05 | global batch size:  1024 |tokens per sec: 39255.47163186662 |TFLOPS: 16.831540307655874 | load balancing loss: 1.298828E-01 | lm loss: 6.024270E+00 | loss scale: 1.0 | grad norm: 2.351 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      218/   63312 | consumed samples:       223232 | elapsed time per iteration (ms): 26875.5 | learning rate: 3.443E-05 | global batch size:  1024 |tokens per sec: 39016.04968381455 |TFLOPS: 16.72888353137339 | load balancing loss: 1.308594E-01 | lm loss: 5.944321E+00 | loss scale: 1.0 | grad norm: 1.424 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      219/   63312 | consumed samples:       224256 | elapsed time per iteration (ms): 26648.4 | learning rate: 3.459E-05 | global batch size:  1024 |tokens per sec: 39348.597664128036 |TFLOPS: 16.87146988945786 | load balancing loss: 1.298828E-01 | lm loss: 5.996381E+00 | loss scale: 1.0 | grad norm: 1.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     220 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
 iteration      220/   63312 | consumed samples:       225280 | elapsed time per iteration (ms): 26487.6 | learning rate: 3.475E-05 | global batch size:  1024 |tokens per sec: 39587.48148253028 |TFLOPS: 16.97389593227784 | load balancing loss: 1.298828E-01 | lm loss: 5.921948E+00 | loss scale: 1.0 | grad norm: 1.657 | number of skipped iterations:   0 | number of nan iterations:   0 |
  successfully saved checkpoint at iteration     220 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (55734.43, 55734.74)
 iteration      221/   63312 | consumed samples:       226304 | elapsed time per iteration (ms): 82097.1 | learning rate: 3.491E-05 | global batch size:  1024 |tokens per sec: 12772.393416735464 |TFLOPS: 5.476409929170405 | load balancing loss: 1.298828E-01 | lm loss: 5.930539E+00 | loss scale: 1.0 | grad norm: 2.072 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      222/   63312 | consumed samples:       227328 | elapsed time per iteration (ms): 25943.2 | learning rate: 3.506E-05 | global batch size:  1024 |tokens per sec: 40418.0768488761 |TFLOPS: 17.330029709477277 | load balancing loss: 1.298828E-01 | lm loss: 5.983642E+00 | loss scale: 1.0 | grad norm: 1.427 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      223/   63312 | consumed samples:       228352 | elapsed time per iteration (ms): 26243.3 | learning rate: 3.522E-05 | global batch size:  1024 |tokens per sec: 39955.874854690206 |TFLOPS: 17.131851693215623 | load balancing loss: 1.298828E-01 | lm loss: 5.919157E+00 | loss scale: 1.0 | grad norm: 1.265 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      224/   63312 | consumed samples:       229376 | elapsed time per iteration (ms): 26334.3 | learning rate: 3.538E-05 | global batch size:  1024 |tokens per sec: 39817.836781767786 |TFLOPS: 17.072665208076092 | load balancing loss: 1.289062E-01 | lm loss: 5.946731E+00 | loss scale: 1.0 | grad norm: 1.964 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      225/   63312 | consumed samples:       230400 | elapsed time per iteration (ms): 26034.2 | learning rate: 3.554E-05 | global batch size:  1024 |tokens per sec: 40276.86444266792 |TFLOPS: 17.26948216774032 | load balancing loss: 1.289062E-01 | lm loss: 5.966299E+00 | loss scale: 1.0 | grad norm: 1.374 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      226/   63312 | consumed samples:       231424 | elapsed time per iteration (ms): 26113.8 | learning rate: 3.570E-05 | global batch size:  1024 |tokens per sec: 40154.142720209886 |TFLOPS: 17.216862863161637 | load balancing loss: 1.289062E-01 | lm loss: 5.892405E+00 | loss scale: 1.0 | grad norm: 1.781 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      227/   63312 | consumed samples:       232448 | elapsed time per iteration (ms): 26187.9 | learning rate: 3.585E-05 | global batch size:  1024 |tokens per sec: 40040.41973991846 |TFLOPS: 17.168101942782627 | load balancing loss: 1.289062E-01 | lm loss: 5.964388E+00 | loss scale: 1.0 | grad norm: 1.318 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      228/   63312 | consumed samples:       233472 | elapsed time per iteration (ms): 26305.1 | learning rate: 3.601E-05 | global batch size:  1024 |tokens per sec: 39862.148677100035 |TFLOPS: 17.091664787533134 | load balancing loss: 1.289062E-01 | lm loss: 5.891793E+00 | loss scale: 1.0 | grad norm: 1.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      229/   63312 | consumed samples:       234496 | elapsed time per iteration (ms): 26913.8 | learning rate: 3.617E-05 | global batch size:  1024 |tokens per sec: 38960.47285770912 |TFLOPS: 16.705053895659486 | load balancing loss: 1.298828E-01 | lm loss: 5.882183E+00 | loss scale: 1.0 | grad norm: 1.481 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      230/   63312 | consumed samples:       235520 | elapsed time per iteration (ms): 26580.5 | learning rate: 3.633E-05 | global batch size:  1024 |tokens per sec: 39449.0912045719 |TFLOPS: 16.914558432438625 | load balancing loss: 1.289062E-01 | lm loss: 5.883347E+00 | loss scale: 1.0 | grad norm: 1.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      231/   63312 | consumed samples:       236544 | elapsed time per iteration (ms): 26052.2 | learning rate: 3.649E-05 | global batch size:  1024 |tokens per sec: 40249.08586309463 |TFLOPS: 17.257571566177262 | load balancing loss: 1.279297E-01 | lm loss: 5.845932E+00 | loss scale: 1.0 | grad norm: 1.792 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      232/   63312 | consumed samples:       237568 | elapsed time per iteration (ms): 26131.8 | learning rate: 3.664E-05 | global batch size:  1024 |tokens per sec: 40126.39325387394 |TFLOPS: 17.204964744460383 | load balancing loss: 1.269531E-01 | lm loss: 5.898337E+00 | loss scale: 1.0 | grad norm: 1.744 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      233/   63312 | consumed samples:       238592 | elapsed time per iteration (ms): 26190.2 | learning rate: 3.680E-05 | global batch size:  1024 |tokens per sec: 40036.93364522849 |TFLOPS: 17.166607212472393 | load balancing loss: 1.269531E-01 | lm loss: 5.876426E+00 | loss scale: 1.0 | grad norm: 1.757 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      234/   63312 | consumed samples:       239616 | elapsed time per iteration (ms): 26180.5 | learning rate: 3.696E-05 | global batch size:  1024 |tokens per sec: 40051.84820936255 |TFLOPS: 17.17300211939767 | load balancing loss: 1.279297E-01 | lm loss: 5.866109E+00 | loss scale: 1.0 | grad norm: 2.127 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      235/   63312 | consumed samples:       240640 | elapsed time per iteration (ms): 26211.0 | learning rate: 3.712E-05 | global batch size:  1024 |tokens per sec: 40005.21673337392 |TFLOPS: 17.153007975012688 | load balancing loss: 1.279297E-01 | lm loss: 5.836364E+00 | loss scale: 1.0 | grad norm: 1.680 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      236/   63312 | consumed samples:       241664 | elapsed time per iteration (ms): 26198.8 | learning rate: 3.728E-05 | global batch size:  1024 |tokens per sec: 40023.818480379574 |TFLOPS: 17.160983832683044 | load balancing loss: 1.279297E-01 | lm loss: 5.858369E+00 | loss scale: 1.0 | grad norm: 2.175 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      237/   63312 | consumed samples:       242688 | elapsed time per iteration (ms): 25617.4 | learning rate: 3.743E-05 | global batch size:  1024 |tokens per sec: 40932.12111539502 |TFLOPS: 17.55043609952072 | load balancing loss: 1.259766E-01 | lm loss: 5.824760E+00 | loss scale: 1.0 | grad norm: 1.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      238/   63312 | consumed samples:       243712 | elapsed time per iteration (ms): 26033.3 | learning rate: 3.759E-05 | global batch size:  1024 |tokens per sec: 40278.29563712735 |TFLOPS: 17.270095819958232 | load balancing loss: 1.269531E-01 | lm loss: 5.847394E+00 | loss scale: 1.0 | grad norm: 1.598 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      239/   63312 | consumed samples:       244736 | elapsed time per iteration (ms): 26121.0 | learning rate: 3.775E-05 | global batch size:  1024 |tokens per sec: 40143.01378022617 |TFLOPS: 17.212091115577653 | load balancing loss: 1.269531E-01 | lm loss: 5.848668E+00 | loss scale: 1.0 | grad norm: 1.629 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      240/   63312 | consumed samples:       245760 | elapsed time per iteration (ms): 25990.5 | learning rate: 3.791E-05 | global batch size:  1024 |tokens per sec: 40344.56195046997 |TFLOPS: 17.298508779418405 | load balancing loss: 1.269531E-01 | lm loss: 5.803932E+00 | loss scale: 1.0 | grad norm: 2.063 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     240 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     240 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (64323.78, 64323.81)
 iteration      241/   63312 | consumed samples:       246784 | elapsed time per iteration (ms): 90646.9 | learning rate: 3.807E-05 | global batch size:  1024 |tokens per sec: 11567.699136995909 |TFLOPS: 4.9598740302263575 | load balancing loss: 1.269531E-01 | lm loss: 5.811721E+00 | loss scale: 1.0 | grad norm: 1.525 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      242/   63312 | consumed samples:       247808 | elapsed time per iteration (ms): 27046.5 | learning rate: 3.822E-05 | global batch size:  1024 |tokens per sec: 38769.40023987762 |TFLOPS: 16.623127826884154 | load balancing loss: 1.289062E-01 | lm loss: 5.828704E+00 | loss scale: 1.0 | grad norm: 1.740 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      243/   63312 | consumed samples:       248832 | elapsed time per iteration (ms): 26833.5 | learning rate: 3.838E-05 | global batch size:  1024 |tokens per sec: 39077.10998972384 |TFLOPS: 16.7550643147747 | load balancing loss: 1.279297E-01 | lm loss: 5.790374E+00 | loss scale: 1.0 | grad norm: 1.332 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      244/   63312 | consumed samples:       249856 | elapsed time per iteration (ms): 26398.9 | learning rate: 3.854E-05 | global batch size:  1024 |tokens per sec: 39720.45361094753 |TFLOPS: 17.03091029603937 | load balancing loss: 1.259766E-01 | lm loss: 5.785838E+00 | loss scale: 1.0 | grad norm: 1.168 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      245/   63312 | consumed samples:       250880 | elapsed time per iteration (ms): 26270.6 | learning rate: 3.870E-05 | global batch size:  1024 |tokens per sec: 39914.4079252005 |TFLOPS: 17.114071947714514 | load balancing loss: 1.259766E-01 | lm loss: 5.766595E+00 | loss scale: 1.0 | grad norm: 1.926 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      246/   63312 | consumed samples:       251904 | elapsed time per iteration (ms): 26146.6 | learning rate: 3.886E-05 | global batch size:  1024 |tokens per sec: 40103.75794867818 |TFLOPS: 17.19525942094902 | load balancing loss: 1.259766E-01 | lm loss: 5.821773E+00 | loss scale: 1.0 | grad norm: 1.553 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      247/   63312 | consumed samples:       252928 | elapsed time per iteration (ms): 26502.7 | learning rate: 3.901E-05 | global batch size:  1024 |tokens per sec: 39564.84413356094 |TFLOPS: 16.964189732459147 | load balancing loss: 1.269531E-01 | lm loss: 5.806838E+00 | loss scale: 1.0 | grad norm: 1.616 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      248/   63312 | consumed samples:       253952 | elapsed time per iteration (ms): 26295.6 | learning rate: 3.917E-05 | global batch size:  1024 |tokens per sec: 39876.49507035443 |TFLOPS: 17.09781608023942 | load balancing loss: 1.259766E-01 | lm loss: 5.784527E+00 | loss scale: 1.0 | grad norm: 1.262 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      249/   63312 | consumed samples:       254976 | elapsed time per iteration (ms): 26460.4 | learning rate: 3.933E-05 | global batch size:  1024 |tokens per sec: 39628.11766597216 |TFLOPS: 16.991319479394168 | load balancing loss: 1.259766E-01 | lm loss: 5.797725E+00 | loss scale: 1.0 | grad norm: 2.161 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      250/   63312 | consumed samples:       256000 | elapsed time per iteration (ms): 26206.8 | learning rate: 3.949E-05 | global batch size:  1024 |tokens per sec: 40011.660128759584 |TFLOPS: 17.155770705012955 | load balancing loss: 1.269531E-01 | lm loss: 5.749951E+00 | loss scale: 1.0 | grad norm: 1.210 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      251/   63312 | consumed samples:       257024 | elapsed time per iteration (ms): 26165.1 | learning rate: 3.964E-05 | global batch size:  1024 |tokens per sec: 40075.31593128678 |TFLOPS: 17.18306435763036 | load balancing loss: 1.279297E-01 | lm loss: 5.780229E+00 | loss scale: 1.0 | grad norm: 2.746 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      252/   63312 | consumed samples:       258048 | elapsed time per iteration (ms): 26193.0 | learning rate: 3.980E-05 | global batch size:  1024 |tokens per sec: 40032.65558895246 |TFLOPS: 17.164772913363123 | load balancing loss: 1.298828E-01 | lm loss: 5.841833E+00 | loss scale: 1.0 | grad norm: 1.651 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      253/   63312 | consumed samples:       259072 | elapsed time per iteration (ms): 26022.0 | learning rate: 3.996E-05 | global batch size:  1024 |tokens per sec: 40295.789877581075 |TFLOPS: 17.277596812841722 | load balancing loss: 1.289062E-01 | lm loss: 5.765844E+00 | loss scale: 1.0 | grad norm: 1.543 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      254/   63312 | consumed samples:       260096 | elapsed time per iteration (ms): 25679.8 | learning rate: 4.012E-05 | global batch size:  1024 |tokens per sec: 40832.729980923665 |TFLOPS: 17.507820234355055 | load balancing loss: 1.259766E-01 | lm loss: 5.785078E+00 | loss scale: 1.0 | grad norm: 2.080 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      255/   63312 | consumed samples:       261120 | elapsed time per iteration (ms): 25802.8 | learning rate: 4.028E-05 | global batch size:  1024 |tokens per sec: 40638.07338524572 |TFLOPS: 17.424357465978968 | load balancing loss: 1.269531E-01 | lm loss: 5.738296E+00 | loss scale: 1.0 | grad norm: 1.375 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      256/   63312 | consumed samples:       262144 | elapsed time per iteration (ms): 26184.9 | learning rate: 4.043E-05 | global batch size:  1024 |tokens per sec: 40045.06115365214 |TFLOPS: 17.170092038407397 | load balancing loss: 1.269531E-01 | lm loss: 5.721544E+00 | loss scale: 1.0 | grad norm: 1.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      257/   63312 | consumed samples:       263168 | elapsed time per iteration (ms): 26065.5 | learning rate: 4.059E-05 | global batch size:  1024 |tokens per sec: 40228.48578867849 |TFLOPS: 17.248738887101908 | load balancing loss: 1.259766E-01 | lm loss: 5.726387E+00 | loss scale: 1.0 | grad norm: 1.422 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      258/   63312 | consumed samples:       264192 | elapsed time per iteration (ms): 26719.0 | learning rate: 4.075E-05 | global batch size:  1024 |tokens per sec: 39244.641539986995 |TFLOPS: 16.826896696958315 | load balancing loss: 1.279297E-01 | lm loss: 5.723864E+00 | loss scale: 1.0 | grad norm: 1.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      259/   63312 | consumed samples:       265216 | elapsed time per iteration (ms): 26409.5 | learning rate: 4.091E-05 | global batch size:  1024 |tokens per sec: 39704.45528572249 |TFLOPS: 17.024050705651426 | load balancing loss: 1.279297E-01 | lm loss: 5.704105E+00 | loss scale: 1.0 | grad norm: 1.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      260/   63312 | consumed samples:       266240 | elapsed time per iteration (ms): 25722.1 | learning rate: 4.107E-05 | global batch size:  1024 |tokens per sec: 40765.52764683656 |TFLOPS: 17.47900593795421 | load balancing loss: 1.269531E-01 | lm loss: 5.717189E+00 | loss scale: 1.0 | grad norm: 1.636 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     260 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     260 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (55686.77, 55686.80)
 iteration      261/   63312 | consumed samples:       267264 | elapsed time per iteration (ms): 81680.9 | learning rate: 4.122E-05 | global batch size:  1024 |tokens per sec: 12837.47538971075 |TFLOPS: 5.504315079863997 | load balancing loss: 1.289062E-01 | lm loss: 5.717744E+00 | loss scale: 1.0 | grad norm: 2.210 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      262/   63312 | consumed samples:       268288 | elapsed time per iteration (ms): 25739.1 | learning rate: 4.138E-05 | global batch size:  1024 |tokens per sec: 40738.609213077056 |TFLOPS: 17.467464140492417 | load balancing loss: 1.289062E-01 | lm loss: 5.727283E+00 | loss scale: 1.0 | grad norm: 1.575 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      263/   63312 | consumed samples:       269312 | elapsed time per iteration (ms): 26491.2 | learning rate: 4.154E-05 | global batch size:  1024 |tokens per sec: 39581.99151024761 |TFLOPS: 16.971542000814978 | load balancing loss: 1.279297E-01 | lm loss: 5.659951E+00 | loss scale: 1.0 | grad norm: 1.504 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      264/   63312 | consumed samples:       270336 | elapsed time per iteration (ms): 25744.2 | learning rate: 4.170E-05 | global batch size:  1024 |tokens per sec: 40730.622505729545 |TFLOPS: 17.464039685733454 | load balancing loss: 1.269531E-01 | lm loss: 5.670225E+00 | loss scale: 1.0 | grad norm: 1.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      265/   63312 | consumed samples:       271360 | elapsed time per iteration (ms): 25615.8 | learning rate: 4.186E-05 | global batch size:  1024 |tokens per sec: 40934.65840378579 |TFLOPS: 17.551524010837184 | load balancing loss: 1.269531E-01 | lm loss: 5.695516E+00 | loss scale: 1.0 | grad norm: 1.765 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      266/   63312 | consumed samples:       272384 | elapsed time per iteration (ms): 25876.0 | learning rate: 4.201E-05 | global batch size:  1024 |tokens per sec: 40523.16036383701 |TFLOPS: 17.37508629252693 | load balancing loss: 1.250000E-01 | lm loss: 5.711190E+00 | loss scale: 1.0 | grad norm: 1.191 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      267/   63312 | consumed samples:       273408 | elapsed time per iteration (ms): 26408.4 | learning rate: 4.217E-05 | global batch size:  1024 |tokens per sec: 39706.18627670488 |TFLOPS: 17.024792901408635 | load balancing loss: 1.269531E-01 | lm loss: 5.713465E+00 | loss scale: 1.0 | grad norm: 2.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      268/   63312 | consumed samples:       274432 | elapsed time per iteration (ms): 26396.9 | learning rate: 4.233E-05 | global batch size:  1024 |tokens per sec: 39723.52745369806 |TFLOPS: 17.032228265382347 | load balancing loss: 1.259766E-01 | lm loss: 5.665924E+00 | loss scale: 1.0 | grad norm: 1.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      269/   63312 | consumed samples:       275456 | elapsed time per iteration (ms): 26171.9 | learning rate: 4.249E-05 | global batch size:  1024 |tokens per sec: 40064.9197067964 |TFLOPS: 17.178606775940903 | load balancing loss: 1.250000E-01 | lm loss: 5.603222E+00 | loss scale: 1.0 | grad norm: 1.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      270/   63312 | consumed samples:       276480 | elapsed time per iteration (ms): 25684.4 | learning rate: 4.265E-05 | global batch size:  1024 |tokens per sec: 40825.46273616858 |TFLOPS: 17.504704263053835 | load balancing loss: 1.245117E-01 | lm loss: 5.677171E+00 | loss scale: 1.0 | grad norm: 2.497 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      271/   63312 | consumed samples:       277504 | elapsed time per iteration (ms): 25416.4 | learning rate: 4.280E-05 | global batch size:  1024 |tokens per sec: 41255.843400703285 |TFLOPS: 17.6892382707123 | load balancing loss: 1.259766E-01 | lm loss: 5.636876E+00 | loss scale: 1.0 | grad norm: 1.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      272/   63312 | consumed samples:       278528 | elapsed time per iteration (ms): 25434.0 | learning rate: 4.296E-05 | global batch size:  1024 |tokens per sec: 41227.31878018747 |TFLOPS: 17.677007789711457 | load balancing loss: 1.259766E-01 | lm loss: 5.658151E+00 | loss scale: 1.0 | grad norm: 2.039 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      273/   63312 | consumed samples:       279552 | elapsed time per iteration (ms): 25432.0 | learning rate: 4.312E-05 | global batch size:  1024 |tokens per sec: 41230.57036813495 |TFLOPS: 17.678401970734555 | load balancing loss: 1.240234E-01 | lm loss: 5.623251E+00 | loss scale: 1.0 | grad norm: 1.499 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      274/   63312 | consumed samples:       280576 | elapsed time per iteration (ms): 25579.0 | learning rate: 4.328E-05 | global batch size:  1024 |tokens per sec: 40993.68681626122 |TFLOPS: 17.576833580753814 | load balancing loss: 1.245117E-01 | lm loss: 5.666013E+00 | loss scale: 1.0 | grad norm: 1.599 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      275/   63312 | consumed samples:       281600 | elapsed time per iteration (ms): 25567.4 | learning rate: 4.344E-05 | global batch size:  1024 |tokens per sec: 41012.28926223085 |TFLOPS: 17.58480973811845 | load balancing loss: 1.259766E-01 | lm loss: 5.648529E+00 | loss scale: 1.0 | grad norm: 1.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      276/   63312 | consumed samples:       282624 | elapsed time per iteration (ms): 25734.8 | learning rate: 4.359E-05 | global batch size:  1024 |tokens per sec: 40745.45146978073 |TFLOPS: 17.470397889972826 | load balancing loss: 1.259766E-01 | lm loss: 5.622552E+00 | loss scale: 1.0 | grad norm: 1.131 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      277/   63312 | consumed samples:       283648 | elapsed time per iteration (ms): 25837.1 | learning rate: 4.375E-05 | global batch size:  1024 |tokens per sec: 40584.04596294841 |TFLOPS: 17.401192167020298 | load balancing loss: 1.259766E-01 | lm loss: 5.627166E+00 | loss scale: 1.0 | grad norm: 1.770 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      278/   63312 | consumed samples:       284672 | elapsed time per iteration (ms): 25732.3 | learning rate: 4.391E-05 | global batch size:  1024 |tokens per sec: 40749.40750778675 |TFLOPS: 17.472094117540323 | load balancing loss: 1.230469E-01 | lm loss: 5.659344E+00 | loss scale: 1.0 | grad norm: 1.724 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      279/   63312 | consumed samples:       285696 | elapsed time per iteration (ms): 25735.0 | learning rate: 4.407E-05 | global batch size:  1024 |tokens per sec: 40745.153637148826 |TFLOPS: 17.470270188489682 | load balancing loss: 1.230469E-01 | lm loss: 5.615716E+00 | loss scale: 1.0 | grad norm: 1.398 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      280/   63312 | consumed samples:       286720 | elapsed time per iteration (ms): 25457.5 | learning rate: 4.423E-05 | global batch size:  1024 |tokens per sec: 41189.29286293661 |TFLOPS: 17.660703444550467 | load balancing loss: 1.240234E-01 | lm loss: 5.592944E+00 | loss scale: 1.0 | grad norm: 1.479 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     280 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     280 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (61226.78, 61226.86)
 iteration      281/   63312 | consumed samples:       287744 | elapsed time per iteration (ms): 86828.6 | learning rate: 4.438E-05 | global batch size:  1024 |tokens per sec: 12076.384678791828 |TFLOPS: 5.177982763728604 | load balancing loss: 1.240234E-01 | lm loss: 5.618871E+00 | loss scale: 1.0 | grad norm: 1.819 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      282/   63312 | consumed samples:       288768 | elapsed time per iteration (ms): 25267.5 | learning rate: 4.454E-05 | global batch size:  1024 |tokens per sec: 41499.06347006102 |TFLOPS: 17.793523564732364 | load balancing loss: 1.220703E-01 | lm loss: 5.606394E+00 | loss scale: 1.0 | grad norm: 1.184 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      283/   63312 | consumed samples:       289792 | elapsed time per iteration (ms): 25417.2 | learning rate: 4.470E-05 | global batch size:  1024 |tokens per sec: 41254.642187245096 |TFLOPS: 17.688723227282704 | load balancing loss: 1.220703E-01 | lm loss: 5.614776E+00 | loss scale: 1.0 | grad norm: 2.585 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      284/   63312 | consumed samples:       290816 | elapsed time per iteration (ms): 25242.0 | learning rate: 4.486E-05 | global batch size:  1024 |tokens per sec: 41540.904461586695 |TFLOPS: 17.811463696543314 | load balancing loss: 1.230469E-01 | lm loss: 5.608865E+00 | loss scale: 1.0 | grad norm: 1.230 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      285/   63312 | consumed samples:       291840 | elapsed time per iteration (ms): 25045.5 | learning rate: 4.502E-05 | global batch size:  1024 |tokens per sec: 41866.77691820848 |TFLOPS: 17.95118779514087 | load balancing loss: 1.250000E-01 | lm loss: 5.628753E+00 | loss scale: 1.0 | grad norm: 2.238 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      286/   63312 | consumed samples:       292864 | elapsed time per iteration (ms): 25115.3 | learning rate: 4.517E-05 | global batch size:  1024 |tokens per sec: 41750.55581718306 |TFLOPS: 17.901355757333395 | load balancing loss: 1.235352E-01 | lm loss: 5.678323E+00 | loss scale: 1.0 | grad norm: 2.080 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      287/   63312 | consumed samples:       293888 | elapsed time per iteration (ms): 25205.6 | learning rate: 4.533E-05 | global batch size:  1024 |tokens per sec: 41600.85029710194 |TFLOPS: 17.837166629276183 | load balancing loss: 1.240234E-01 | lm loss: 5.577079E+00 | loss scale: 1.0 | grad norm: 1.558 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      288/   63312 | consumed samples:       294912 | elapsed time per iteration (ms): 25470.6 | learning rate: 4.549E-05 | global batch size:  1024 |tokens per sec: 41168.13864548596 |TFLOPS: 17.65163316596519 | load balancing loss: 1.250000E-01 | lm loss: 5.600923E+00 | loss scale: 1.0 | grad norm: 1.692 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      289/   63312 | consumed samples:       295936 | elapsed time per iteration (ms): 25737.1 | learning rate: 4.565E-05 | global batch size:  1024 |tokens per sec: 40741.84379797862 |TFLOPS: 17.46885103113176 | load balancing loss: 1.240234E-01 | lm loss: 5.575936E+00 | loss scale: 1.0 | grad norm: 1.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      290/   63312 | consumed samples:       296960 | elapsed time per iteration (ms): 25981.3 | learning rate: 4.580E-05 | global batch size:  1024 |tokens per sec: 40358.93035268441 |TFLOPS: 17.30466950889064 | load balancing loss: 1.240234E-01 | lm loss: 5.574043E+00 | loss scale: 1.0 | grad norm: 2.241 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      291/   63312 | consumed samples:       297984 | elapsed time per iteration (ms): 25539.2 | learning rate: 4.596E-05 | global batch size:  1024 |tokens per sec: 41057.44478944386 |TFLOPS: 17.604171041009597 | load balancing loss: 1.240234E-01 | lm loss: 5.556842E+00 | loss scale: 1.0 | grad norm: 1.585 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      292/   63312 | consumed samples:       299008 | elapsed time per iteration (ms): 25561.8 | learning rate: 4.612E-05 | global batch size:  1024 |tokens per sec: 41021.25954290563 |TFLOPS: 17.588655918904777 | load balancing loss: 1.245117E-01 | lm loss: 5.562276E+00 | loss scale: 1.0 | grad norm: 1.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      293/   63312 | consumed samples:       300032 | elapsed time per iteration (ms): 25341.3 | learning rate: 4.628E-05 | global batch size:  1024 |tokens per sec: 41378.21833828055 |TFLOPS: 17.74170888458735 | load balancing loss: 1.240234E-01 | lm loss: 5.549035E+00 | loss scale: 1.0 | grad norm: 1.197 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      294/   63312 | consumed samples:       301056 | elapsed time per iteration (ms): 25110.7 | learning rate: 4.644E-05 | global batch size:  1024 |tokens per sec: 41758.11493732171 |TFLOPS: 17.904596875832677 | load balancing loss: 1.240234E-01 | lm loss: 5.544523E+00 | loss scale: 1.0 | grad norm: 2.733 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      295/   63312 | consumed samples:       302080 | elapsed time per iteration (ms): 25125.8 | learning rate: 4.659E-05 | global batch size:  1024 |tokens per sec: 41732.98368494819 |TFLOPS: 17.893821366846982 | load balancing loss: 1.240234E-01 | lm loss: 5.558020E+00 | loss scale: 1.0 | grad norm: 1.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      296/   63312 | consumed samples:       303104 | elapsed time per iteration (ms): 25290.0 | learning rate: 4.675E-05 | global batch size:  1024 |tokens per sec: 41462.12645066296 |TFLOPS: 17.777686105519713 | load balancing loss: 1.250000E-01 | lm loss: 5.546094E+00 | loss scale: 1.0 | grad norm: 1.825 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      297/   63312 | consumed samples:       304128 | elapsed time per iteration (ms): 24952.8 | learning rate: 4.691E-05 | global batch size:  1024 |tokens per sec: 42022.44725103055 |TFLOPS: 18.01793445166235 | load balancing loss: 1.235352E-01 | lm loss: 5.547719E+00 | loss scale: 1.0 | grad norm: 1.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      298/   63312 | consumed samples:       305152 | elapsed time per iteration (ms): 25490.6 | learning rate: 4.707E-05 | global batch size:  1024 |tokens per sec: 41135.81529611913 |TFLOPS: 17.637773906729986 | load balancing loss: 1.240234E-01 | lm loss: 5.553268E+00 | loss scale: 1.0 | grad norm: 1.780 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      299/   63312 | consumed samples:       306176 | elapsed time per iteration (ms): 25489.0 | learning rate: 4.723E-05 | global batch size:  1024 |tokens per sec: 41138.37674680746 |TFLOPS: 17.638872178097373 | load balancing loss: 1.230469E-01 | lm loss: 5.560982E+00 | loss scale: 1.0 | grad norm: 1.174 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      300/   63312 | consumed samples:       307200 | elapsed time per iteration (ms): 25087.9 | learning rate: 4.738E-05 | global batch size:  1024 |tokens per sec: 41796.057288883036 |TFLOPS: 17.920865390593132 | load balancing loss: 1.210938E-01 | lm loss: 5.510392E+00 | loss scale: 1.0 | grad norm: 1.655 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     300 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
-----------------------------------------------------------------------------------------------
 validation loss at iteration 300 | lm loss value: 5.250086E+00 | lm loss PPL: 1.905827E+02 | 
-----------------------------------------------------------------------------------------------
  successfully saved checkpoint at iteration     300 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (72523.88, 72523.92)
 iteration      301/   63312 | consumed samples:       308224 | elapsed time per iteration (ms): 199651.8 | learning rate: 4.754E-05 | global batch size:  1024 |tokens per sec: 5252.023330267903 |TFLOPS: 2.2519062618621706 | load balancing loss: 1.230469E-01 | lm loss: 5.530576E+00 | loss scale: 1.0 | grad norm: 1.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      302/   63312 | consumed samples:       309248 | elapsed time per iteration (ms): 24674.8 | learning rate: 4.770E-05 | global batch size:  1024 |tokens per sec: 42495.83980084252 |TFLOPS: 18.220910634402593 | load balancing loss: 1.225586E-01 | lm loss: 5.495042E+00 | loss scale: 1.0 | grad norm: 1.214 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      303/   63312 | consumed samples:       310272 | elapsed time per iteration (ms): 25269.7 | learning rate: 4.786E-05 | global batch size:  1024 |tokens per sec: 41495.32933969071 |TFLOPS: 17.791922484342063 | load balancing loss: 1.230469E-01 | lm loss: 5.476249E+00 | loss scale: 1.0 | grad norm: 1.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      304/   63312 | consumed samples:       311296 | elapsed time per iteration (ms): 25071.9 | learning rate: 4.802E-05 | global batch size:  1024 |tokens per sec: 41822.83588183354 |TFLOPS: 17.932347228611928 | load balancing loss: 1.220703E-01 | lm loss: 5.479084E+00 | loss scale: 1.0 | grad norm: 1.188 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      305/   63312 | consumed samples:       312320 | elapsed time per iteration (ms): 24857.1 | learning rate: 4.817E-05 | global batch size:  1024 |tokens per sec: 42184.1246119291 |TFLOPS: 18.08725673728712 | load balancing loss: 1.220703E-01 | lm loss: 5.500782E+00 | loss scale: 1.0 | grad norm: 1.272 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      306/   63312 | consumed samples:       313344 | elapsed time per iteration (ms): 25151.7 | learning rate: 4.833E-05 | global batch size:  1024 |tokens per sec: 41690.08757559614 |TFLOPS: 17.8754288329253 | load balancing loss: 1.230469E-01 | lm loss: 5.486382E+00 | loss scale: 1.0 | grad norm: 1.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      307/   63312 | consumed samples:       314368 | elapsed time per iteration (ms): 25055.7 | learning rate: 4.849E-05 | global batch size:  1024 |tokens per sec: 41849.84632908458 |TFLOPS: 17.94392847863224 | load balancing loss: 1.215820E-01 | lm loss: 5.485782E+00 | loss scale: 1.0 | grad norm: 1.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      308/   63312 | consumed samples:       315392 | elapsed time per iteration (ms): 25040.0 | learning rate: 4.865E-05 | global batch size:  1024 |tokens per sec: 41876.09382503346 |TFLOPS: 17.95518259857204 | load balancing loss: 1.201172E-01 | lm loss: 5.509838E+00 | loss scale: 1.0 | grad norm: 1.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      309/   63312 | consumed samples:       316416 | elapsed time per iteration (ms): 25127.2 | learning rate: 4.881E-05 | global batch size:  1024 |tokens per sec: 41730.66204543533 |TFLOPS: 17.892825919144634 | load balancing loss: 1.220703E-01 | lm loss: 5.478189E+00 | loss scale: 1.0 | grad norm: 1.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      310/   63312 | consumed samples:       317440 | elapsed time per iteration (ms): 24999.0 | learning rate: 4.896E-05 | global batch size:  1024 |tokens per sec: 41944.70886639968 |TFLOPS: 17.984602620455806 | load balancing loss: 1.206055E-01 | lm loss: 5.467153E+00 | loss scale: 1.0 | grad norm: 1.324 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      311/   63312 | consumed samples:       318464 | elapsed time per iteration (ms): 24462.6 | learning rate: 4.912E-05 | global batch size:  1024 |tokens per sec: 42864.53871177089 |TFLOPS: 18.37899740097813 | load balancing loss: 1.230469E-01 | lm loss: 5.405575E+00 | loss scale: 1.0 | grad norm: 1.559 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      312/   63312 | consumed samples:       319488 | elapsed time per iteration (ms): 24804.7 | learning rate: 4.928E-05 | global batch size:  1024 |tokens per sec: 42273.21523593227 |TFLOPS: 18.12545605999571 | load balancing loss: 1.235352E-01 | lm loss: 5.417876E+00 | loss scale: 1.0 | grad norm: 1.127 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      313/   63312 | consumed samples:       320512 | elapsed time per iteration (ms): 24897.3 | learning rate: 4.944E-05 | global batch size:  1024 |tokens per sec: 42116.08258100789 |TFLOPS: 18.058082404679293 | load balancing loss: 1.220703E-01 | lm loss: 5.457897E+00 | loss scale: 1.0 | grad norm: 1.408 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      314/   63312 | consumed samples:       321536 | elapsed time per iteration (ms): 24608.5 | learning rate: 4.960E-05 | global batch size:  1024 |tokens per sec: 42610.35761099796 |TFLOPS: 18.270012353410117 | load balancing loss: 1.210938E-01 | lm loss: 5.477803E+00 | loss scale: 1.0 | grad norm: 1.275 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      315/   63312 | consumed samples:       322560 | elapsed time per iteration (ms): 24803.0 | learning rate: 4.975E-05 | global batch size:  1024 |tokens per sec: 42276.13527024426 |TFLOPS: 18.126708081005237 | load balancing loss: 1.210938E-01 | lm loss: 5.410523E+00 | loss scale: 1.0 | grad norm: 1.350 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      316/   63312 | consumed samples:       323584 | elapsed time per iteration (ms): 24699.0 | learning rate: 4.991E-05 | global batch size:  1024 |tokens per sec: 42454.269364432235 |TFLOPS: 18.203086508313667 | load balancing loss: 1.210938E-01 | lm loss: 5.457170E+00 | loss scale: 1.0 | grad norm: 1.336 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      317/   63312 | consumed samples:       324608 | elapsed time per iteration (ms): 24957.8 | learning rate: 5.007E-05 | global batch size:  1024 |tokens per sec: 42013.89429215375 |TFLOPS: 18.014267205642874 | load balancing loss: 1.201172E-01 | lm loss: 5.417903E+00 | loss scale: 1.0 | grad norm: 1.275 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      318/   63312 | consumed samples:       325632 | elapsed time per iteration (ms): 25053.9 | learning rate: 5.023E-05 | global batch size:  1024 |tokens per sec: 41852.779855973335 |TFLOPS: 17.94518628484419 | load balancing loss: 1.210938E-01 | lm loss: 5.411000E+00 | loss scale: 1.0 | grad norm: 1.728 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      319/   63312 | consumed samples:       326656 | elapsed time per iteration (ms): 24275.1 | learning rate: 5.039E-05 | global batch size:  1024 |tokens per sec: 43195.545463798226 |TFLOPS: 18.520922927720033 | load balancing loss: 1.201172E-01 | lm loss: 5.430422E+00 | loss scale: 1.0 | grad norm: 1.309 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      320/   63312 | consumed samples:       327680 | elapsed time per iteration (ms): 24795.1 | learning rate: 5.054E-05 | global batch size:  1024 |tokens per sec: 42289.61548515109 |TFLOPS: 18.13248798304508 | load balancing loss: 1.210938E-01 | lm loss: 5.438788E+00 | loss scale: 1.0 | grad norm: 1.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     320 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     320 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (52091.15, 52093.96)
 iteration      321/   63312 | consumed samples:       328704 | elapsed time per iteration (ms): 76980.6 | learning rate: 5.070E-05 | global batch size:  1024 |tokens per sec: 13621.295472602744 |TFLOPS: 5.840393052455138 | load balancing loss: 1.210938E-01 | lm loss: 5.425416E+00 | loss scale: 1.0 | grad norm: 1.187 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      322/   63312 | consumed samples:       329728 | elapsed time per iteration (ms): 24355.4 | learning rate: 5.086E-05 | global batch size:  1024 |tokens per sec: 43053.188128323454 |TFLOPS: 18.459884475485797 | load balancing loss: 1.210938E-01 | lm loss: 5.395780E+00 | loss scale: 1.0 | grad norm: 1.723 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      323/   63312 | consumed samples:       330752 | elapsed time per iteration (ms): 24593.7 | learning rate: 5.102E-05 | global batch size:  1024 |tokens per sec: 42635.96303999383 |TFLOPS: 18.280991174764768 | load balancing loss: 1.210938E-01 | lm loss: 5.393212E+00 | loss scale: 1.0 | grad norm: 1.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      324/   63312 | consumed samples:       331776 | elapsed time per iteration (ms): 24614.9 | learning rate: 5.118E-05 | global batch size:  1024 |tokens per sec: 42599.209182851104 |TFLOPS: 18.265232249900457 | load balancing loss: 1.210938E-01 | lm loss: 5.407849E+00 | loss scale: 1.0 | grad norm: 1.359 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      325/   63312 | consumed samples:       332800 | elapsed time per iteration (ms): 24680.8 | learning rate: 5.133E-05 | global batch size:  1024 |tokens per sec: 42485.47023976448 |TFLOPS: 18.21646448516524 | load balancing loss: 1.210938E-01 | lm loss: 5.365625E+00 | loss scale: 1.0 | grad norm: 1.258 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      326/   63312 | consumed samples:       333824 | elapsed time per iteration (ms): 24702.5 | learning rate: 5.149E-05 | global batch size:  1024 |tokens per sec: 42448.1644780066 |TFLOPS: 18.200468920556467 | load balancing loss: 1.201172E-01 | lm loss: 5.362902E+00 | loss scale: 1.0 | grad norm: 1.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      327/   63312 | consumed samples:       334848 | elapsed time per iteration (ms): 24414.4 | learning rate: 5.165E-05 | global batch size:  1024 |tokens per sec: 42949.10463932826 |TFLOPS: 18.415256672849438 | load balancing loss: 1.206055E-01 | lm loss: 5.372880E+00 | loss scale: 1.0 | grad norm: 1.240 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      328/   63312 | consumed samples:       335872 | elapsed time per iteration (ms): 24589.9 | learning rate: 5.181E-05 | global batch size:  1024 |tokens per sec: 42642.6301981597 |TFLOPS: 18.28384984737119 | load balancing loss: 1.206055E-01 | lm loss: 5.438426E+00 | loss scale: 1.0 | grad norm: 1.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      329/   63312 | consumed samples:       336896 | elapsed time per iteration (ms): 24504.7 | learning rate: 5.196E-05 | global batch size:  1024 |tokens per sec: 42790.84038087843 |TFLOPS: 18.34739782070417 | load balancing loss: 1.201172E-01 | lm loss: 5.378996E+00 | loss scale: 1.0 | grad norm: 1.529 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      330/   63312 | consumed samples:       337920 | elapsed time per iteration (ms): 24270.7 | learning rate: 5.212E-05 | global batch size:  1024 |tokens per sec: 43203.32542149123 |TFLOPS: 18.524258734578492 | load balancing loss: 1.201172E-01 | lm loss: 5.390388E+00 | loss scale: 1.0 | grad norm: 1.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      331/   63312 | consumed samples:       338944 | elapsed time per iteration (ms): 24258.6 | learning rate: 5.228E-05 | global batch size:  1024 |tokens per sec: 43224.96151150765 |TFLOPS: 18.533535625316848 | load balancing loss: 1.201172E-01 | lm loss: 5.347292E+00 | loss scale: 1.0 | grad norm: 1.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      332/   63312 | consumed samples:       339968 | elapsed time per iteration (ms): 24587.6 | learning rate: 5.244E-05 | global batch size:  1024 |tokens per sec: 42646.598080620846 |TFLOPS: 18.285551153477183 | load balancing loss: 1.201172E-01 | lm loss: 5.330602E+00 | loss scale: 1.0 | grad norm: 1.250 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      333/   63312 | consumed samples:       340992 | elapsed time per iteration (ms): 24494.2 | learning rate: 5.260E-05 | global batch size:  1024 |tokens per sec: 42809.230256753384 |TFLOPS: 18.35528283454236 | load balancing loss: 1.201172E-01 | lm loss: 5.378408E+00 | loss scale: 1.0 | grad norm: 1.351 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      334/   63312 | consumed samples:       342016 | elapsed time per iteration (ms): 24395.6 | learning rate: 5.275E-05 | global batch size:  1024 |tokens per sec: 42982.23368100687 |TFLOPS: 18.429461388197062 | load balancing loss: 1.196289E-01 | lm loss: 5.341385E+00 | loss scale: 1.0 | grad norm: 1.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      335/   63312 | consumed samples:       343040 | elapsed time per iteration (ms): 24691.6 | learning rate: 5.291E-05 | global batch size:  1024 |tokens per sec: 42466.917830086655 |TFLOPS: 18.208509781825416 | load balancing loss: 1.201172E-01 | lm loss: 5.338814E+00 | loss scale: 1.0 | grad norm: 2.076 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      336/   63312 | consumed samples:       344064 | elapsed time per iteration (ms): 24722.4 | learning rate: 5.307E-05 | global batch size:  1024 |tokens per sec: 42413.99701783957 |TFLOPS: 18.18581896326122 | load balancing loss: 1.201172E-01 | lm loss: 5.335958E+00 | loss scale: 1.0 | grad norm: 1.423 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      337/   63312 | consumed samples:       345088 | elapsed time per iteration (ms): 24686.1 | learning rate: 5.323E-05 | global batch size:  1024 |tokens per sec: 42476.36143632597 |TFLOPS: 18.212558910073252 | load balancing loss: 1.196289E-01 | lm loss: 5.322991E+00 | loss scale: 1.0 | grad norm: 1.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      338/   63312 | consumed samples:       346112 | elapsed time per iteration (ms): 24538.6 | learning rate: 5.339E-05 | global batch size:  1024 |tokens per sec: 42731.74855381571 |TFLOPS: 18.322061060560603 | load balancing loss: 1.201172E-01 | lm loss: 5.317752E+00 | loss scale: 1.0 | grad norm: 2.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      339/   63312 | consumed samples:       347136 | elapsed time per iteration (ms): 24396.9 | learning rate: 5.354E-05 | global batch size:  1024 |tokens per sec: 42979.86379596527 |TFLOPS: 18.428445254294093 | load balancing loss: 1.220703E-01 | lm loss: 5.306629E+00 | loss scale: 1.0 | grad norm: 1.658 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      340/   63312 | consumed samples:       348160 | elapsed time per iteration (ms): 24588.7 | learning rate: 5.370E-05 | global batch size:  1024 |tokens per sec: 42644.6615988835 |TFLOPS: 18.284720849596926 | load balancing loss: 1.220703E-01 | lm loss: 5.302071E+00 | loss scale: 1.0 | grad norm: 1.511 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     340 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     340 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (55730.65, 55730.70)
 iteration      341/   63312 | consumed samples:       349184 | elapsed time per iteration (ms): 80408.3 | learning rate: 5.386E-05 | global batch size:  1024 |tokens per sec: 13040.638567947843 |TFLOPS: 5.591425209519228 | load balancing loss: 1.210938E-01 | lm loss: 5.293747E+00 | loss scale: 1.0 | grad norm: 1.201 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      342/   63312 | consumed samples:       350208 | elapsed time per iteration (ms): 24450.6 | learning rate: 5.402E-05 | global batch size:  1024 |tokens per sec: 42885.46035917861 |TFLOPS: 18.387967960673553 | load balancing loss: 1.210938E-01 | lm loss: 5.329047E+00 | loss scale: 1.0 | grad norm: 2.156 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      343/   63312 | consumed samples:       351232 | elapsed time per iteration (ms): 24754.8 | learning rate: 5.418E-05 | global batch size:  1024 |tokens per sec: 42358.45360286765 |TFLOPS: 18.16200365321497 | load balancing loss: 1.201172E-01 | lm loss: 5.335771E+00 | loss scale: 1.0 | grad norm: 1.193 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      344/   63312 | consumed samples:       352256 | elapsed time per iteration (ms): 24456.9 | learning rate: 5.433E-05 | global batch size:  1024 |tokens per sec: 42874.42833826634 |TFLOPS: 18.383237769010055 | load balancing loss: 1.210938E-01 | lm loss: 5.308444E+00 | loss scale: 1.0 | grad norm: 1.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      345/   63312 | consumed samples:       353280 | elapsed time per iteration (ms): 24756.9 | learning rate: 5.449E-05 | global batch size:  1024 |tokens per sec: 42354.85608605784 |TFLOPS: 18.160461148522636 | load balancing loss: 1.191406E-01 | lm loss: 5.304117E+00 | loss scale: 1.0 | grad norm: 1.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      346/   63312 | consumed samples:       354304 | elapsed time per iteration (ms): 24682.9 | learning rate: 5.465E-05 | global batch size:  1024 |tokens per sec: 42481.91636104255 |TFLOPS: 18.214940691144434 | load balancing loss: 1.206055E-01 | lm loss: 5.306849E+00 | loss scale: 1.0 | grad norm: 1.687 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      347/   63312 | consumed samples:       355328 | elapsed time per iteration (ms): 24450.3 | learning rate: 5.481E-05 | global batch size:  1024 |tokens per sec: 42886.05041530527 |TFLOPS: 18.38822095861414 | load balancing loss: 1.206055E-01 | lm loss: 5.324305E+00 | loss scale: 1.0 | grad norm: 1.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      348/   63312 | consumed samples:       356352 | elapsed time per iteration (ms): 24454.2 | learning rate: 5.497E-05 | global batch size:  1024 |tokens per sec: 42879.163953515774 |TFLOPS: 18.385268255350912 | load balancing loss: 1.210938E-01 | lm loss: 5.336436E+00 | loss scale: 1.0 | grad norm: 1.262 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      349/   63312 | consumed samples:       357376 | elapsed time per iteration (ms): 24718.3 | learning rate: 5.512E-05 | global batch size:  1024 |tokens per sec: 42420.96440702668 |TFLOPS: 18.18880636570646 | load balancing loss: 1.215820E-01 | lm loss: 5.364022E+00 | loss scale: 1.0 | grad norm: 1.881 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      350/   63312 | consumed samples:       358400 | elapsed time per iteration (ms): 24477.4 | learning rate: 5.528E-05 | global batch size:  1024 |tokens per sec: 42838.47699140317 |TFLOPS: 18.367822935900506 | load balancing loss: 1.220703E-01 | lm loss: 5.324870E+00 | loss scale: 1.0 | grad norm: 1.179 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      351/   63312 | consumed samples:       359424 | elapsed time per iteration (ms): 24926.5 | learning rate: 5.544E-05 | global batch size:  1024 |tokens per sec: 42066.67094251331 |TFLOPS: 18.036896212018352 | load balancing loss: 1.220703E-01 | lm loss: 5.358603E+00 | loss scale: 1.0 | grad norm: 1.778 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      352/   63312 | consumed samples:       360448 | elapsed time per iteration (ms): 24904.2 | learning rate: 5.560E-05 | global batch size:  1024 |tokens per sec: 42104.40805422194 |TFLOPS: 18.05307672623494 | load balancing loss: 1.210938E-01 | lm loss: 5.295022E+00 | loss scale: 1.0 | grad norm: 1.226 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      353/   63312 | consumed samples:       361472 | elapsed time per iteration (ms): 24388.7 | learning rate: 5.576E-05 | global batch size:  1024 |tokens per sec: 42994.312728896715 |TFLOPS: 18.43464051286388 | load balancing loss: 1.206055E-01 | lm loss: 5.306481E+00 | loss scale: 1.0 | grad norm: 1.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      354/   63312 | consumed samples:       362496 | elapsed time per iteration (ms): 24489.5 | learning rate: 5.591E-05 | global batch size:  1024 |tokens per sec: 42817.38897128407 |TFLOPS: 18.358781040697412 | load balancing loss: 1.210938E-01 | lm loss: 5.317409E+00 | loss scale: 1.0 | grad norm: 1.123 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      355/   63312 | consumed samples:       363520 | elapsed time per iteration (ms): 24156.7 | learning rate: 5.607E-05 | global batch size:  1024 |tokens per sec: 43407.29050505566 |TFLOPS: 18.61171269660355 | load balancing loss: 1.210938E-01 | lm loss: 5.310588E+00 | loss scale: 1.0 | grad norm: 1.137 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      356/   63312 | consumed samples:       364544 | elapsed time per iteration (ms): 24045.2 | learning rate: 5.623E-05 | global batch size:  1024 |tokens per sec: 43608.51327442789 |TFLOPS: 18.697990838546886 | load balancing loss: 1.210938E-01 | lm loss: 5.283388E+00 | loss scale: 1.0 | grad norm: 1.437 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      357/   63312 | consumed samples:       365568 | elapsed time per iteration (ms): 24494.8 | learning rate: 5.639E-05 | global batch size:  1024 |tokens per sec: 42808.08938483713 |TFLOPS: 18.354793663712258 | load balancing loss: 1.210938E-01 | lm loss: 5.245266E+00 | loss scale: 1.0 | grad norm: 1.046 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      358/   63312 | consumed samples:       366592 | elapsed time per iteration (ms): 24192.0 | learning rate: 5.655E-05 | global batch size:  1024 |tokens per sec: 43343.926180259725 |TFLOPS: 18.584544020683115 | load balancing loss: 1.210938E-01 | lm loss: 5.282333E+00 | loss scale: 1.0 | grad norm: 1.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      359/   63312 | consumed samples:       367616 | elapsed time per iteration (ms): 24509.5 | learning rate: 5.670E-05 | global batch size:  1024 |tokens per sec: 42782.35800968614 |TFLOPS: 18.34376084051536 | load balancing loss: 1.215820E-01 | lm loss: 5.312826E+00 | loss scale: 1.0 | grad norm: 1.165 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      360/   63312 | consumed samples:       368640 | elapsed time per iteration (ms): 24517.3 | learning rate: 5.686E-05 | global batch size:  1024 |tokens per sec: 42768.76609139816 |TFLOPS: 18.337933043497205 | load balancing loss: 1.220703E-01 | lm loss: 5.225876E+00 | loss scale: 1.0 | grad norm: 1.457 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     360 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     360 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (50914.90, 50914.92)
 iteration      361/   63312 | consumed samples:       369664 | elapsed time per iteration (ms): 75545.7 | learning rate: 5.702E-05 | global batch size:  1024 |tokens per sec: 13880.024555529893 |TFLOPS: 5.9513281350568725 | load balancing loss: 1.215820E-01 | lm loss: 5.253590E+00 | loss scale: 1.0 | grad norm: 1.114 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      362/   63312 | consumed samples:       370688 | elapsed time per iteration (ms): 24370.7 | learning rate: 5.718E-05 | global batch size:  1024 |tokens per sec: 43026.09848761537 |TFLOPS: 18.448269269743932 | load balancing loss: 1.220703E-01 | lm loss: 5.258646E+00 | loss scale: 1.0 | grad norm: 1.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      363/   63312 | consumed samples:       371712 | elapsed time per iteration (ms): 24506.5 | learning rate: 5.734E-05 | global batch size:  1024 |tokens per sec: 42787.625275466744 |TFLOPS: 18.346019282271676 | load balancing loss: 1.210938E-01 | lm loss: 5.249879E+00 | loss scale: 1.0 | grad norm: 1.145 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      364/   63312 | consumed samples:       372736 | elapsed time per iteration (ms): 24145.6 | learning rate: 5.749E-05 | global batch size:  1024 |tokens per sec: 43427.23814172212 |TFLOPS: 18.620265630414732 | load balancing loss: 1.215820E-01 | lm loss: 5.256180E+00 | loss scale: 1.0 | grad norm: 1.585 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      365/   63312 | consumed samples:       373760 | elapsed time per iteration (ms): 24305.9 | learning rate: 5.765E-05 | global batch size:  1024 |tokens per sec: 43140.83361691249 |TFLOPS: 18.497464168523365 | load balancing loss: 1.210938E-01 | lm loss: 5.260439E+00 | loss scale: 1.0 | grad norm: 1.178 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      366/   63312 | consumed samples:       374784 | elapsed time per iteration (ms): 24576.5 | learning rate: 5.781E-05 | global batch size:  1024 |tokens per sec: 42665.809910150034 |TFLOPS: 18.293788595791895 | load balancing loss: 1.210938E-01 | lm loss: 5.273638E+00 | loss scale: 1.0 | grad norm: 1.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      367/   63312 | consumed samples:       375808 | elapsed time per iteration (ms): 24089.2 | learning rate: 5.797E-05 | global batch size:  1024 |tokens per sec: 43528.833010009635 |TFLOPS: 18.663826388942063 | load balancing loss: 1.201172E-01 | lm loss: 5.244763E+00 | loss scale: 1.0 | grad norm: 1.235 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      368/   63312 | consumed samples:       376832 | elapsed time per iteration (ms): 24104.3 | learning rate: 5.812E-05 | global batch size:  1024 |tokens per sec: 43501.5556152407 |TFLOPS: 18.652130680026712 | load balancing loss: 1.191406E-01 | lm loss: 5.249001E+00 | loss scale: 1.0 | grad norm: 1.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      369/   63312 | consumed samples:       377856 | elapsed time per iteration (ms): 24296.9 | learning rate: 5.828E-05 | global batch size:  1024 |tokens per sec: 43156.81344237899 |TFLOPS: 18.50431582678316 | load balancing loss: 1.220703E-01 | lm loss: 5.270467E+00 | loss scale: 1.0 | grad norm: 1.178 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      370/   63312 | consumed samples:       378880 | elapsed time per iteration (ms): 24084.3 | learning rate: 5.844E-05 | global batch size:  1024 |tokens per sec: 43537.666148232725 |TFLOPS: 18.667613767258118 | load balancing loss: 1.201172E-01 | lm loss: 5.183145E+00 | loss scale: 1.0 | grad norm: 1.090 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      371/   63312 | consumed samples:       379904 | elapsed time per iteration (ms): 24142.2 | learning rate: 5.860E-05 | global batch size:  1024 |tokens per sec: 43433.236302560785 |TFLOPS: 18.62283745751885 | load balancing loss: 1.196289E-01 | lm loss: 5.198869E+00 | loss scale: 1.0 | grad norm: 1.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      372/   63312 | consumed samples:       380928 | elapsed time per iteration (ms): 24673.5 | learning rate: 5.876E-05 | global batch size:  1024 |tokens per sec: 42498.00466444972 |TFLOPS: 18.22183886141275 | load balancing loss: 1.191406E-01 | lm loss: 5.219243E+00 | loss scale: 1.0 | grad norm: 1.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      373/   63312 | consumed samples:       381952 | elapsed time per iteration (ms): 24697.3 | learning rate: 5.891E-05 | global batch size:  1024 |tokens per sec: 42457.02551796637 |TFLOPS: 18.204268262280067 | load balancing loss: 1.201172E-01 | lm loss: 5.215211E+00 | loss scale: 1.0 | grad norm: 1.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      374/   63312 | consumed samples:       382976 | elapsed time per iteration (ms): 24456.7 | learning rate: 5.907E-05 | global batch size:  1024 |tokens per sec: 42874.80534336477 |TFLOPS: 18.38339941721467 | load balancing loss: 1.191406E-01 | lm loss: 5.259405E+00 | loss scale: 1.0 | grad norm: 1.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      375/   63312 | consumed samples:       384000 | elapsed time per iteration (ms): 24448.6 | learning rate: 5.923E-05 | global batch size:  1024 |tokens per sec: 42888.98671914345 |TFLOPS: 18.3894799554967 | load balancing loss: 1.186523E-01 | lm loss: 5.262540E+00 | loss scale: 1.0 | grad norm: 1.077 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      376/   63312 | consumed samples:       385024 | elapsed time per iteration (ms): 24999.9 | learning rate: 5.939E-05 | global batch size:  1024 |tokens per sec: 41943.16200035486 |TFLOPS: 17.983939371815513 | load balancing loss: 1.201172E-01 | lm loss: 5.236286E+00 | loss scale: 1.0 | grad norm: 1.541 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      377/   63312 | consumed samples:       386048 | elapsed time per iteration (ms): 25011.0 | learning rate: 5.955E-05 | global batch size:  1024 |tokens per sec: 41924.53257003656 |TFLOPS: 17.975951644391127 | load balancing loss: 1.191406E-01 | lm loss: 5.207603E+00 | loss scale: 1.0 | grad norm: 1.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      378/   63312 | consumed samples:       387072 | elapsed time per iteration (ms): 24095.4 | learning rate: 5.970E-05 | global batch size:  1024 |tokens per sec: 43517.672506182476 |TFLOPS: 18.659041107751655 | load balancing loss: 1.201172E-01 | lm loss: 5.165548E+00 | loss scale: 1.0 | grad norm: 1.200 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      379/   63312 | consumed samples:       388096 | elapsed time per iteration (ms): 24364.3 | learning rate: 5.986E-05 | global batch size:  1024 |tokens per sec: 43037.37716659173 |TFLOPS: 18.453105220807974 | load balancing loss: 1.191406E-01 | lm loss: 5.191033E+00 | loss scale: 1.0 | grad norm: 1.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      380/   63312 | consumed samples:       389120 | elapsed time per iteration (ms): 24407.3 | learning rate: 6.002E-05 | global batch size:  1024 |tokens per sec: 42961.62543799906 |TFLOPS: 18.420625206680537 | load balancing loss: 1.191406E-01 | lm loss: 5.195011E+00 | loss scale: 1.0 | grad norm: 1.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     380 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     380 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (62134.69, 62134.71)
 iteration      381/   63312 | consumed samples:       390144 | elapsed time per iteration (ms): 86222.3 | learning rate: 6.018E-05 | global batch size:  1024 |tokens per sec: 12161.306995808287 |TFLOPS: 5.214394844451681 | load balancing loss: 1.191406E-01 | lm loss: 5.199535E+00 | loss scale: 1.0 | grad norm: 1.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      382/   63312 | consumed samples:       391168 | elapsed time per iteration (ms): 24183.6 | learning rate: 6.034E-05 | global batch size:  1024 |tokens per sec: 43359.02919612155 |TFLOPS: 18.591019729920017 | load balancing loss: 1.191406E-01 | lm loss: 5.173594E+00 | loss scale: 1.0 | grad norm: 1.212 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      383/   63312 | consumed samples:       392192 | elapsed time per iteration (ms): 24037.1 | learning rate: 6.049E-05 | global batch size:  1024 |tokens per sec: 43623.24439602714 |TFLOPS: 18.704307090948596 | load balancing loss: 1.191406E-01 | lm loss: 5.230297E+00 | loss scale: 1.0 | grad norm: 1.414 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      384/   63312 | consumed samples:       393216 | elapsed time per iteration (ms): 24029.3 | learning rate: 6.065E-05 | global batch size:  1024 |tokens per sec: 43637.41004831924 |TFLOPS: 18.710380887482607 | load balancing loss: 1.206055E-01 | lm loss: 5.149761E+00 | loss scale: 1.0 | grad norm: 1.237 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      385/   63312 | consumed samples:       394240 | elapsed time per iteration (ms): 24221.4 | learning rate: 6.081E-05 | global batch size:  1024 |tokens per sec: 43291.34397357299 |TFLOPS: 18.56199838578123 | load balancing loss: 1.191406E-01 | lm loss: 5.205260E+00 | loss scale: 1.0 | grad norm: 1.597 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      386/   63312 | consumed samples:       395264 | elapsed time per iteration (ms): 24251.6 | learning rate: 6.097E-05 | global batch size:  1024 |tokens per sec: 43237.34103923792 |TFLOPS: 18.5388435865085 | load balancing loss: 1.201172E-01 | lm loss: 5.166268E+00 | loss scale: 1.0 | grad norm: 1.272 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      387/   63312 | consumed samples:       396288 | elapsed time per iteration (ms): 24688.2 | learning rate: 6.113E-05 | global batch size:  1024 |tokens per sec: 42472.73073925488 |TFLOPS: 18.21100217870418 | load balancing loss: 1.201172E-01 | lm loss: 5.189258E+00 | loss scale: 1.0 | grad norm: 1.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      388/   63312 | consumed samples:       397312 | elapsed time per iteration (ms): 24482.2 | learning rate: 6.128E-05 | global batch size:  1024 |tokens per sec: 42830.11628744415 |TFLOPS: 18.3642381228837 | load balancing loss: 1.201172E-01 | lm loss: 5.123097E+00 | loss scale: 1.0 | grad norm: 1.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      389/   63312 | consumed samples:       398336 | elapsed time per iteration (ms): 23956.6 | learning rate: 6.144E-05 | global batch size:  1024 |tokens per sec: 43769.87598212472 |TFLOPS: 18.76717821054491 | load balancing loss: 1.186523E-01 | lm loss: 5.116309E+00 | loss scale: 1.0 | grad norm: 0.978 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      390/   63312 | consumed samples:       399360 | elapsed time per iteration (ms): 23846.9 | learning rate: 6.160E-05 | global batch size:  1024 |tokens per sec: 43971.239505279395 |TFLOPS: 18.853516703386205 | load balancing loss: 1.191406E-01 | lm loss: 5.150726E+00 | loss scale: 1.0 | grad norm: 1.723 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      391/   63312 | consumed samples:       400384 | elapsed time per iteration (ms): 24314.7 | learning rate: 6.176E-05 | global batch size:  1024 |tokens per sec: 43125.26015203778 |TFLOPS: 18.49078674520193 | load balancing loss: 1.196289E-01 | lm loss: 5.168983E+00 | loss scale: 1.0 | grad norm: 1.087 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      392/   63312 | consumed samples:       401408 | elapsed time per iteration (ms): 24222.4 | learning rate: 6.192E-05 | global batch size:  1024 |tokens per sec: 43289.5841273676 |TFLOPS: 18.56124381779083 | load balancing loss: 1.201172E-01 | lm loss: 5.128608E+00 | loss scale: 1.0 | grad norm: 1.403 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      393/   63312 | consumed samples:       402432 | elapsed time per iteration (ms): 23591.4 | learning rate: 6.207E-05 | global batch size:  1024 |tokens per sec: 44447.34086846922 |TFLOPS: 19.05765434208812 | load balancing loss: 1.191406E-01 | lm loss: 5.152496E+00 | loss scale: 1.0 | grad norm: 1.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      394/   63312 | consumed samples:       403456 | elapsed time per iteration (ms): 24037.5 | learning rate: 6.223E-05 | global batch size:  1024 |tokens per sec: 43622.46643462762 |TFLOPS: 18.70397352499945 | load balancing loss: 1.191406E-01 | lm loss: 5.137786E+00 | loss scale: 1.0 | grad norm: 1.180 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      395/   63312 | consumed samples:       404480 | elapsed time per iteration (ms): 24263.0 | learning rate: 6.239E-05 | global batch size:  1024 |tokens per sec: 43217.002618354694 |TFLOPS: 18.530123096430057 | load balancing loss: 1.201172E-01 | lm loss: 5.129469E+00 | loss scale: 1.0 | grad norm: 1.173 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      396/   63312 | consumed samples:       405504 | elapsed time per iteration (ms): 23870.9 | learning rate: 6.255E-05 | global batch size:  1024 |tokens per sec: 43926.87344689732 |TFLOPS: 18.83449390047728 | load balancing loss: 1.201172E-01 | lm loss: 5.184230E+00 | loss scale: 1.0 | grad norm: 1.367 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      397/   63312 | consumed samples:       406528 | elapsed time per iteration (ms): 24216.6 | learning rate: 6.271E-05 | global batch size:  1024 |tokens per sec: 43299.85802467349 |TFLOPS: 18.565648949341472 | load balancing loss: 1.201172E-01 | lm loss: 5.144660E+00 | loss scale: 1.0 | grad norm: 1.101 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      398/   63312 | consumed samples:       407552 | elapsed time per iteration (ms): 23994.8 | learning rate: 6.286E-05 | global batch size:  1024 |tokens per sec: 43700.15179833414 |TFLOPS: 18.737282622462434 | load balancing loss: 1.196289E-01 | lm loss: 5.130723E+00 | loss scale: 1.0 | grad norm: 1.119 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      399/   63312 | consumed samples:       408576 | elapsed time per iteration (ms): 23583.7 | learning rate: 6.302E-05 | global batch size:  1024 |tokens per sec: 44461.95698178282 |TFLOPS: 19.06392128246999 | load balancing loss: 1.201172E-01 | lm loss: 5.153016E+00 | loss scale: 1.0 | grad norm: 1.133 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      400/   63312 | consumed samples:       409600 | elapsed time per iteration (ms): 23926.1 | learning rate: 6.318E-05 | global batch size:  1024 |tokens per sec: 43825.63440376762 |TFLOPS: 18.79108570884667 | load balancing loss: 1.201172E-01 | lm loss: 5.089493E+00 | loss scale: 1.0 | grad norm: 1.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     400 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
-----------------------------------------------------------------------------------------------
 validation loss at iteration 400 | lm loss value: 4.816957E+00 | lm loss PPL: 1.235884E+02 | 
-----------------------------------------------------------------------------------------------
  successfully saved checkpoint at iteration     400 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (58464.02, 58464.08)
 iteration      401/   63312 | consumed samples:       410624 | elapsed time per iteration (ms): 179318.5 | learning rate: 6.334E-05 | global batch size:  1024 |tokens per sec: 5847.562369092954 |TFLOPS: 2.5072551067510807 | load balancing loss: 1.201172E-01 | lm loss: 5.079042E+00 | loss scale: 1.0 | grad norm: 1.063 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      402/   63312 | consumed samples:       411648 | elapsed time per iteration (ms): 23938.1 | learning rate: 6.350E-05 | global batch size:  1024 |tokens per sec: 43803.61284258548 |TFLOPS: 18.781643539914125 | load balancing loss: 1.191406E-01 | lm loss: 5.116933E+00 | loss scale: 1.0 | grad norm: 1.358 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      403/   63312 | consumed samples:       412672 | elapsed time per iteration (ms): 23546.3 | learning rate: 6.365E-05 | global batch size:  1024 |tokens per sec: 44532.426727354876 |TFLOPS: 19.094136544540707 | load balancing loss: 1.191406E-01 | lm loss: 5.116577E+00 | loss scale: 1.0 | grad norm: 1.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      404/   63312 | consumed samples:       413696 | elapsed time per iteration (ms): 23979.4 | learning rate: 6.381E-05 | global batch size:  1024 |tokens per sec: 43728.127143265796 |TFLOPS: 18.749277590966535 | load balancing loss: 1.181641E-01 | lm loss: 5.118845E+00 | loss scale: 1.0 | grad norm: 1.245 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      405/   63312 | consumed samples:       414720 | elapsed time per iteration (ms): 24129.4 | learning rate: 6.397E-05 | global batch size:  1024 |tokens per sec: 43456.28196651251 |TFLOPS: 18.632718730258556 | load balancing loss: 1.181641E-01 | lm loss: 5.083455E+00 | loss scale: 1.0 | grad norm: 1.191 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      406/   63312 | consumed samples:       415744 | elapsed time per iteration (ms): 23569.8 | learning rate: 6.413E-05 | global batch size:  1024 |tokens per sec: 44488.145588354615 |TFLOPS: 19.075150152453983 | load balancing loss: 1.171875E-01 | lm loss: 5.087925E+00 | loss scale: 1.0 | grad norm: 1.135 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      407/   63312 | consumed samples:       416768 | elapsed time per iteration (ms): 23899.5 | learning rate: 6.428E-05 | global batch size:  1024 |tokens per sec: 43874.37292860507 |TFLOPS: 18.811983291049486 | load balancing loss: 1.191406E-01 | lm loss: 5.130002E+00 | loss scale: 1.0 | grad norm: 1.719 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      408/   63312 | consumed samples:       417792 | elapsed time per iteration (ms): 23926.3 | learning rate: 6.444E-05 | global batch size:  1024 |tokens per sec: 43825.17935303838 |TFLOPS: 18.790890597073133 | load balancing loss: 1.191406E-01 | lm loss: 5.105755E+00 | loss scale: 1.0 | grad norm: 1.361 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      409/   63312 | consumed samples:       418816 | elapsed time per iteration (ms): 24127.8 | learning rate: 6.460E-05 | global batch size:  1024 |tokens per sec: 43459.24319664749 |TFLOPS: 18.63398841477144 | load balancing loss: 1.201172E-01 | lm loss: 5.115371E+00 | loss scale: 1.0 | grad norm: 1.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      410/   63312 | consumed samples:       419840 | elapsed time per iteration (ms): 23709.4 | learning rate: 6.476E-05 | global batch size:  1024 |tokens per sec: 44226.24761728583 |TFLOPS: 18.96285634796536 | load balancing loss: 1.196289E-01 | lm loss: 5.111161E+00 | loss scale: 1.0 | grad norm: 1.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      411/   63312 | consumed samples:       420864 | elapsed time per iteration (ms): 23864.4 | learning rate: 6.492E-05 | global batch size:  1024 |tokens per sec: 43938.915154702256 |TFLOPS: 18.839657014862777 | load balancing loss: 1.191406E-01 | lm loss: 5.092653E+00 | loss scale: 1.0 | grad norm: 1.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      412/   63312 | consumed samples:       421888 | elapsed time per iteration (ms): 24237.3 | learning rate: 6.507E-05 | global batch size:  1024 |tokens per sec: 43262.852495723266 |TFLOPS: 18.549782115337468 | load balancing loss: 1.186523E-01 | lm loss: 5.028005E+00 | loss scale: 1.0 | grad norm: 1.164 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      413/   63312 | consumed samples:       422912 | elapsed time per iteration (ms): 23797.7 | learning rate: 6.523E-05 | global batch size:  1024 |tokens per sec: 44062.15590747089 |TFLOPS: 18.89249886369404 | load balancing loss: 1.191406E-01 | lm loss: 5.047424E+00 | loss scale: 1.0 | grad norm: 1.106 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      414/   63312 | consumed samples:       423936 | elapsed time per iteration (ms): 23610.1 | learning rate: 6.539E-05 | global batch size:  1024 |tokens per sec: 44412.23374151764 |TFLOPS: 19.042601484542224 | load balancing loss: 1.191406E-01 | lm loss: 5.060956E+00 | loss scale: 1.0 | grad norm: 1.221 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      415/   63312 | consumed samples:       424960 | elapsed time per iteration (ms): 24197.8 | learning rate: 6.555E-05 | global batch size:  1024 |tokens per sec: 43333.53316111835 |TFLOPS: 18.58008781334884 | load balancing loss: 1.210938E-01 | lm loss: 5.031993E+00 | loss scale: 1.0 | grad norm: 1.205 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      416/   63312 | consumed samples:       425984 | elapsed time per iteration (ms): 24056.4 | learning rate: 6.571E-05 | global batch size:  1024 |tokens per sec: 43588.2061264667 |TFLOPS: 18.689283757336717 | load balancing loss: 1.191406E-01 | lm loss: 5.061831E+00 | loss scale: 1.0 | grad norm: 1.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      417/   63312 | consumed samples:       427008 | elapsed time per iteration (ms): 23957.2 | learning rate: 6.586E-05 | global batch size:  1024 |tokens per sec: 43768.63629123585 |TFLOPS: 18.766646669175067 | load balancing loss: 1.191406E-01 | lm loss: 5.105156E+00 | loss scale: 1.0 | grad norm: 0.963 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      418/   63312 | consumed samples:       428032 | elapsed time per iteration (ms): 23900.9 | learning rate: 6.602E-05 | global batch size:  1024 |tokens per sec: 43871.89095530382 |TFLOPS: 18.81091909714413 | load balancing loss: 1.201172E-01 | lm loss: 5.011345E+00 | loss scale: 1.0 | grad norm: 1.258 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      419/   63312 | consumed samples:       429056 | elapsed time per iteration (ms): 24158.7 | learning rate: 6.618E-05 | global batch size:  1024 |tokens per sec: 43403.60044457528 |TFLOPS: 18.61013051202822 | load balancing loss: 1.191406E-01 | lm loss: 5.061940E+00 | loss scale: 1.0 | grad norm: 1.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      420/   63312 | consumed samples:       430080 | elapsed time per iteration (ms): 23427.3 | learning rate: 6.634E-05 | global batch size:  1024 |tokens per sec: 44758.66112510503 |TFLOPS: 19.191138904375272 | load balancing loss: 1.191406E-01 | lm loss: 5.013679E+00 | loss scale: 1.0 | grad norm: 1.022 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     420 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     420 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (52067.53, 52067.65)
 iteration      421/   63312 | consumed samples:       431104 | elapsed time per iteration (ms): 75775.0 | learning rate: 6.650E-05 | global batch size:  1024 |tokens per sec: 13838.022571338113 |TFLOPS: 5.933318974536391 | load balancing loss: 1.191406E-01 | lm loss: 5.004049E+00 | loss scale: 1.0 | grad norm: 1.019 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      422/   63312 | consumed samples:       432128 | elapsed time per iteration (ms): 23449.8 | learning rate: 6.665E-05 | global batch size:  1024 |tokens per sec: 44715.740338462245 |TFLOPS: 19.172735789589414 | load balancing loss: 1.181641E-01 | lm loss: 5.032928E+00 | loss scale: 1.0 | grad norm: 1.237 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      423/   63312 | consumed samples:       433152 | elapsed time per iteration (ms): 23608.4 | learning rate: 6.681E-05 | global batch size:  1024 |tokens per sec: 44415.45587291344 |TFLOPS: 19.043983035500865 | load balancing loss: 1.171875E-01 | lm loss: 5.041020E+00 | loss scale: 1.0 | grad norm: 0.991 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      424/   63312 | consumed samples:       434176 | elapsed time per iteration (ms): 23299.3 | learning rate: 6.697E-05 | global batch size:  1024 |tokens per sec: 45004.542116770856 |TFLOPS: 19.296565120137515 | load balancing loss: 1.181641E-01 | lm loss: 5.005614E+00 | loss scale: 1.0 | grad norm: 1.134 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      425/   63312 | consumed samples:       435200 | elapsed time per iteration (ms): 23596.9 | learning rate: 6.713E-05 | global batch size:  1024 |tokens per sec: 44437.066186632874 |TFLOPS: 19.053248874154924 | load balancing loss: 1.181641E-01 | lm loss: 5.055852E+00 | loss scale: 1.0 | grad norm: 1.117 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      426/   63312 | consumed samples:       436224 | elapsed time per iteration (ms): 24187.0 | learning rate: 6.729E-05 | global batch size:  1024 |tokens per sec: 43352.83655855772 |TFLOPS: 18.588364517170472 | load balancing loss: 1.171875E-01 | lm loss: 4.986200E+00 | loss scale: 1.0 | grad norm: 0.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      427/   63312 | consumed samples:       437248 | elapsed time per iteration (ms): 23594.7 | learning rate: 6.744E-05 | global batch size:  1024 |tokens per sec: 44441.08989725062 |TFLOPS: 19.054974117659423 | load balancing loss: 1.181641E-01 | lm loss: 4.957991E+00 | loss scale: 1.0 | grad norm: 1.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      428/   63312 | consumed samples:       438272 | elapsed time per iteration (ms): 23664.7 | learning rate: 6.760E-05 | global batch size:  1024 |tokens per sec: 44309.70706005855 |TFLOPS: 18.998641193152224 | load balancing loss: 1.176758E-01 | lm loss: 4.995576E+00 | loss scale: 1.0 | grad norm: 1.463 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      429/   63312 | consumed samples:       439296 | elapsed time per iteration (ms): 24087.8 | learning rate: 6.776E-05 | global batch size:  1024 |tokens per sec: 43531.35258176743 |TFLOPS: 18.66490670391056 | load balancing loss: 1.181641E-01 | lm loss: 4.970034E+00 | loss scale: 1.0 | grad norm: 0.988 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      430/   63312 | consumed samples:       440320 | elapsed time per iteration (ms): 23684.6 | learning rate: 6.792E-05 | global batch size:  1024 |tokens per sec: 44272.55506397679 |TFLOPS: 18.98271155854168 | load balancing loss: 1.166992E-01 | lm loss: 5.027267E+00 | loss scale: 1.0 | grad norm: 1.504 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      431/   63312 | consumed samples:       441344 | elapsed time per iteration (ms): 23603.4 | learning rate: 6.808E-05 | global batch size:  1024 |tokens per sec: 44424.79883929471 |TFLOPS: 19.047989012469227 | load balancing loss: 1.181641E-01 | lm loss: 4.991820E+00 | loss scale: 1.0 | grad norm: 1.043 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      432/   63312 | consumed samples:       442368 | elapsed time per iteration (ms): 23418.6 | learning rate: 6.823E-05 | global batch size:  1024 |tokens per sec: 44775.260917020634 |TFLOPS: 19.198256385203077 | load balancing loss: 1.181641E-01 | lm loss: 4.983980E+00 | loss scale: 1.0 | grad norm: 1.242 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      433/   63312 | consumed samples:       443392 | elapsed time per iteration (ms): 23576.7 | learning rate: 6.839E-05 | global batch size:  1024 |tokens per sec: 44475.09128071112 |TFLOPS: 19.069552866364788 | load balancing loss: 1.186523E-01 | lm loss: 4.990170E+00 | loss scale: 1.0 | grad norm: 1.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      434/   63312 | consumed samples:       444416 | elapsed time per iteration (ms): 23762.0 | learning rate: 6.855E-05 | global batch size:  1024 |tokens per sec: 44128.34183222028 |TFLOPS: 18.920877354994968 | load balancing loss: 1.181641E-01 | lm loss: 5.019136E+00 | loss scale: 1.0 | grad norm: 1.117 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      435/   63312 | consumed samples:       445440 | elapsed time per iteration (ms): 23615.3 | learning rate: 6.871E-05 | global batch size:  1024 |tokens per sec: 44402.43432109936 |TFLOPS: 19.03839979410506 | load balancing loss: 1.186523E-01 | lm loss: 5.024140E+00 | loss scale: 1.0 | grad norm: 1.594 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      436/   63312 | consumed samples:       446464 | elapsed time per iteration (ms): 23970.7 | learning rate: 6.887E-05 | global batch size:  1024 |tokens per sec: 43744.04038365724 |TFLOPS: 18.756100699591638 | load balancing loss: 1.181641E-01 | lm loss: 5.000421E+00 | loss scale: 1.0 | grad norm: 0.980 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      437/   63312 | consumed samples:       447488 | elapsed time per iteration (ms): 23760.8 | learning rate: 6.902E-05 | global batch size:  1024 |tokens per sec: 44130.49467116366 |TFLOPS: 18.92180042619868 | load balancing loss: 1.191406E-01 | lm loss: 4.989195E+00 | loss scale: 1.0 | grad norm: 1.130 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      438/   63312 | consumed samples:       448512 | elapsed time per iteration (ms): 23278.0 | learning rate: 6.918E-05 | global batch size:  1024 |tokens per sec: 45045.76179125128 |TFLOPS: 19.31423885028632 | load balancing loss: 1.191406E-01 | lm loss: 4.977622E+00 | loss scale: 1.0 | grad norm: 1.242 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      439/   63312 | consumed samples:       449536 | elapsed time per iteration (ms): 23373.5 | learning rate: 6.934E-05 | global batch size:  1024 |tokens per sec: 44861.80396141656 |TFLOPS: 19.23536338403773 | load balancing loss: 1.191406E-01 | lm loss: 4.992379E+00 | loss scale: 1.0 | grad norm: 1.102 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      440/   63312 | consumed samples:       450560 | elapsed time per iteration (ms): 23462.6 | learning rate: 6.950E-05 | global batch size:  1024 |tokens per sec: 44691.35575895022 |TFLOPS: 19.16228043098894 | load balancing loss: 1.186523E-01 | lm loss: 4.971120E+00 | loss scale: 1.0 | grad norm: 1.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     440 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     440 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (57149.80, 57149.81)
 iteration      441/   63312 | consumed samples:       451584 | elapsed time per iteration (ms): 81100.0 | learning rate: 6.966E-05 | global batch size:  1024 |tokens per sec: 12929.427477612086 |TFLOPS: 5.543741310387951 | load balancing loss: 1.181641E-01 | lm loss: 4.988819E+00 | loss scale: 1.0 | grad norm: 1.284 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      442/   63312 | consumed samples:       452608 | elapsed time per iteration (ms): 23654.5 | learning rate: 6.981E-05 | global batch size:  1024 |tokens per sec: 44328.7637436638 |TFLOPS: 19.006812113660832 | load balancing loss: 1.181641E-01 | lm loss: 4.982453E+00 | loss scale: 1.0 | grad norm: 0.926 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      443/   63312 | consumed samples:       453632 | elapsed time per iteration (ms): 23758.2 | learning rate: 6.997E-05 | global batch size:  1024 |tokens per sec: 44135.40598458135 |TFLOPS: 18.923906246516637 | load balancing loss: 1.186523E-01 | lm loss: 4.964911E+00 | loss scale: 1.0 | grad norm: 0.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      444/   63312 | consumed samples:       454656 | elapsed time per iteration (ms): 23765.9 | learning rate: 7.013E-05 | global batch size:  1024 |tokens per sec: 44121.062170172714 |TFLOPS: 18.917756059540196 | load balancing loss: 1.191406E-01 | lm loss: 4.914207E+00 | loss scale: 1.0 | grad norm: 1.088 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      445/   63312 | consumed samples:       455680 | elapsed time per iteration (ms): 23547.0 | learning rate: 7.029E-05 | global batch size:  1024 |tokens per sec: 44531.14977791564 |TFLOPS: 19.09358902784905 | load balancing loss: 1.191406E-01 | lm loss: 4.930666E+00 | loss scale: 1.0 | grad norm: 1.160 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      446/   63312 | consumed samples:       456704 | elapsed time per iteration (ms): 23250.8 | learning rate: 7.044E-05 | global batch size:  1024 |tokens per sec: 45098.424384605794 |TFLOPS: 19.33681895252187 | load balancing loss: 1.171875E-01 | lm loss: 4.923532E+00 | loss scale: 1.0 | grad norm: 0.906 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      447/   63312 | consumed samples:       457728 | elapsed time per iteration (ms): 23080.8 | learning rate: 7.060E-05 | global batch size:  1024 |tokens per sec: 45430.74487364452 |TFLOPS: 19.479307769336252 | load balancing loss: 1.186523E-01 | lm loss: 4.931996E+00 | loss scale: 1.0 | grad norm: 1.040 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      448/   63312 | consumed samples:       458752 | elapsed time per iteration (ms): 23965.0 | learning rate: 7.076E-05 | global batch size:  1024 |tokens per sec: 43754.47020959453 |TFLOPS: 18.760572688549296 | load balancing loss: 1.176758E-01 | lm loss: 4.913369E+00 | loss scale: 1.0 | grad norm: 1.070 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      449/   63312 | consumed samples:       459776 | elapsed time per iteration (ms): 23284.7 | learning rate: 7.092E-05 | global batch size:  1024 |tokens per sec: 45032.91680036462 |TFLOPS: 19.30873131279224 | load balancing loss: 1.181641E-01 | lm loss: 4.927171E+00 | loss scale: 1.0 | grad norm: 1.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      450/   63312 | consumed samples:       460800 | elapsed time per iteration (ms): 23497.8 | learning rate: 7.108E-05 | global batch size:  1024 |tokens per sec: 44624.511044155435 |TFLOPS: 19.133619470754574 | load balancing loss: 1.181641E-01 | lm loss: 4.867655E+00 | loss scale: 1.0 | grad norm: 0.876 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      451/   63312 | consumed samples:       461824 | elapsed time per iteration (ms): 24050.1 | learning rate: 7.123E-05 | global batch size:  1024 |tokens per sec: 43599.66865696686 |TFLOPS: 18.694198538286248 | load balancing loss: 1.181641E-01 | lm loss: 4.877797E+00 | loss scale: 1.0 | grad norm: 1.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      452/   63312 | consumed samples:       462848 | elapsed time per iteration (ms): 23126.8 | learning rate: 7.139E-05 | global batch size:  1024 |tokens per sec: 45340.36409209799 |TFLOPS: 19.440555266706706 | load balancing loss: 1.181641E-01 | lm loss: 4.927709E+00 | loss scale: 1.0 | grad norm: 0.980 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      453/   63312 | consumed samples:       463872 | elapsed time per iteration (ms): 23093.5 | learning rate: 7.155E-05 | global batch size:  1024 |tokens per sec: 45405.72727936095 |TFLOPS: 19.468580993447954 | load balancing loss: 1.176758E-01 | lm loss: 4.921690E+00 | loss scale: 1.0 | grad norm: 1.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      454/   63312 | consumed samples:       464896 | elapsed time per iteration (ms): 23311.7 | learning rate: 7.171E-05 | global batch size:  1024 |tokens per sec: 44980.61861180336 |TFLOPS: 19.286307456137525 | load balancing loss: 1.181641E-01 | lm loss: 4.885957E+00 | loss scale: 1.0 | grad norm: 0.861 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      455/   63312 | consumed samples:       465920 | elapsed time per iteration (ms): 23691.5 | learning rate: 7.187E-05 | global batch size:  1024 |tokens per sec: 44259.58909485851 |TFLOPS: 18.977152149298334 | load balancing loss: 1.191406E-01 | lm loss: 4.863255E+00 | loss scale: 1.0 | grad norm: 1.210 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      456/   63312 | consumed samples:       466944 | elapsed time per iteration (ms): 23886.3 | learning rate: 7.202E-05 | global batch size:  1024 |tokens per sec: 43898.623590122224 |TFLOPS: 18.822381229727725 | load balancing loss: 1.171875E-01 | lm loss: 4.885200E+00 | loss scale: 1.0 | grad norm: 0.881 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      457/   63312 | consumed samples:       467968 | elapsed time per iteration (ms): 23401.9 | learning rate: 7.218E-05 | global batch size:  1024 |tokens per sec: 44807.26078500533 |TFLOPS: 19.21197694555897 | load balancing loss: 1.171875E-01 | lm loss: 4.897076E+00 | loss scale: 1.0 | grad norm: 0.936 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      458/   63312 | consumed samples:       468992 | elapsed time per iteration (ms): 23753.6 | learning rate: 7.234E-05 | global batch size:  1024 |tokens per sec: 44143.94736426936 |TFLOPS: 18.927568527735673 | load balancing loss: 1.176758E-01 | lm loss: 4.880525E+00 | loss scale: 1.0 | grad norm: 1.103 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      459/   63312 | consumed samples:       470016 | elapsed time per iteration (ms): 23813.7 | learning rate: 7.250E-05 | global batch size:  1024 |tokens per sec: 44032.41279035355 |TFLOPS: 18.879745928782683 | load balancing loss: 1.171875E-01 | lm loss: 4.893044E+00 | loss scale: 1.0 | grad norm: 1.051 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      460/   63312 | consumed samples:       471040 | elapsed time per iteration (ms): 23200.2 | learning rate: 7.266E-05 | global batch size:  1024 |tokens per sec: 45196.84602010525 |TFLOPS: 19.379019126311395 | load balancing loss: 1.171875E-01 | lm loss: 4.857123E+00 | loss scale: 1.0 | grad norm: 1.156 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     460 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     460 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (50444.76, 50444.78)
 iteration      461/   63312 | consumed samples:       472064 | elapsed time per iteration (ms): 73785.3 | learning rate: 7.281E-05 | global batch size:  1024 |tokens per sec: 14211.18036313014 |TFLOPS: 6.093317572249449 | load balancing loss: 1.171875E-01 | lm loss: 4.884813E+00 | loss scale: 1.0 | grad norm: 1.089 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      462/   63312 | consumed samples:       473088 | elapsed time per iteration (ms): 23256.0 | learning rate: 7.297E-05 | global batch size:  1024 |tokens per sec: 45088.34527096333 |TFLOPS: 19.33249733822235 | load balancing loss: 1.162109E-01 | lm loss: 4.875301E+00 | loss scale: 1.0 | grad norm: 0.908 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      463/   63312 | consumed samples:       474112 | elapsed time per iteration (ms): 23245.0 | learning rate: 7.313E-05 | global batch size:  1024 |tokens per sec: 45109.785431636024 |TFLOPS: 19.34169022047707 | load balancing loss: 1.176758E-01 | lm loss: 4.869112E+00 | loss scale: 1.0 | grad norm: 1.445 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      464/   63312 | consumed samples:       475136 | elapsed time per iteration (ms): 23220.5 | learning rate: 7.329E-05 | global batch size:  1024 |tokens per sec: 45157.29121446365 |TFLOPS: 19.362059240775906 | load balancing loss: 1.176758E-01 | lm loss: 4.857274E+00 | loss scale: 1.0 | grad norm: 0.861 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      465/   63312 | consumed samples:       476160 | elapsed time per iteration (ms): 23353.9 | learning rate: 7.345E-05 | global batch size:  1024 |tokens per sec: 44899.43165663123 |TFLOPS: 19.25149698382298 | load balancing loss: 1.171875E-01 | lm loss: 4.875740E+00 | loss scale: 1.0 | grad norm: 1.057 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      466/   63312 | consumed samples:       477184 | elapsed time per iteration (ms): 23472.2 | learning rate: 7.360E-05 | global batch size:  1024 |tokens per sec: 44673.13275597322 |TFLOPS: 19.154466967123057 | load balancing loss: 1.162109E-01 | lm loss: 4.887553E+00 | loss scale: 1.0 | grad norm: 1.090 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      467/   63312 | consumed samples:       478208 | elapsed time per iteration (ms): 23453.7 | learning rate: 7.376E-05 | global batch size:  1024 |tokens per sec: 44708.33695542596 |TFLOPS: 19.169561446375383 | load balancing loss: 1.171875E-01 | lm loss: 4.862593E+00 | loss scale: 1.0 | grad norm: 1.293 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      468/   63312 | consumed samples:       479232 | elapsed time per iteration (ms): 23248.9 | learning rate: 7.392E-05 | global batch size:  1024 |tokens per sec: 45102.15248622166 |TFLOPS: 19.33841744796747 | load balancing loss: 1.171875E-01 | lm loss: 4.840088E+00 | loss scale: 1.0 | grad norm: 1.208 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      469/   63312 | consumed samples:       480256 | elapsed time per iteration (ms): 23460.1 | learning rate: 7.408E-05 | global batch size:  1024 |tokens per sec: 44696.223279237995 |TFLOPS: 19.164367474158112 | load balancing loss: 1.176758E-01 | lm loss: 4.868406E+00 | loss scale: 1.0 | grad norm: 1.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      470/   63312 | consumed samples:       481280 | elapsed time per iteration (ms): 23663.1 | learning rate: 7.424E-05 | global batch size:  1024 |tokens per sec: 44312.74556302672 |TFLOPS: 18.99994400988216 | load balancing loss: 1.171875E-01 | lm loss: 4.872535E+00 | loss scale: 1.0 | grad norm: 1.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      471/   63312 | consumed samples:       482304 | elapsed time per iteration (ms): 23256.5 | learning rate: 7.439E-05 | global batch size:  1024 |tokens per sec: 45087.4906029406 |TFLOPS: 19.332130882829606 | load balancing loss: 1.181641E-01 | lm loss: 4.857603E+00 | loss scale: 1.0 | grad norm: 1.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      472/   63312 | consumed samples:       483328 | elapsed time per iteration (ms): 23252.9 | learning rate: 7.455E-05 | global batch size:  1024 |tokens per sec: 45094.486983544266 |TFLOPS: 19.335130715903645 | load balancing loss: 1.171875E-01 | lm loss: 4.828247E+00 | loss scale: 1.0 | grad norm: 0.978 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      473/   63312 | consumed samples:       484352 | elapsed time per iteration (ms): 23273.0 | learning rate: 7.471E-05 | global batch size:  1024 |tokens per sec: 45055.410152527955 |TFLOPS: 19.318375771204103 | load balancing loss: 1.171875E-01 | lm loss: 4.834089E+00 | loss scale: 1.0 | grad norm: 1.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      474/   63312 | consumed samples:       485376 | elapsed time per iteration (ms): 23036.9 | learning rate: 7.487E-05 | global batch size:  1024 |tokens per sec: 45517.205157079996 |TFLOPS: 19.516379282813478 | load balancing loss: 1.171875E-01 | lm loss: 4.802209E+00 | loss scale: 1.0 | grad norm: 1.115 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      475/   63312 | consumed samples:       486400 | elapsed time per iteration (ms): 22687.0 | learning rate: 7.503E-05 | global batch size:  1024 |tokens per sec: 46219.20994134216 |TFLOPS: 19.817377368718116 | load balancing loss: 1.162109E-01 | lm loss: 4.802229E+00 | loss scale: 1.0 | grad norm: 0.783 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      476/   63312 | consumed samples:       487424 | elapsed time per iteration (ms): 23492.9 | learning rate: 7.518E-05 | global batch size:  1024 |tokens per sec: 44633.823499857375 |TFLOPS: 19.13761236568093 | load balancing loss: 1.171875E-01 | lm loss: 4.778378E+00 | loss scale: 1.0 | grad norm: 1.079 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      477/   63312 | consumed samples:       488448 | elapsed time per iteration (ms): 23215.7 | learning rate: 7.534E-05 | global batch size:  1024 |tokens per sec: 45166.60195666675 |TFLOPS: 19.366051401006594 | load balancing loss: 1.191406E-01 | lm loss: 4.753503E+00 | loss scale: 1.0 | grad norm: 1.085 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      478/   63312 | consumed samples:       489472 | elapsed time per iteration (ms): 23056.6 | learning rate: 7.550E-05 | global batch size:  1024 |tokens per sec: 45478.34629146414 |TFLOPS: 19.499717794981752 | load balancing loss: 1.171875E-01 | lm loss: 4.805625E+00 | loss scale: 1.0 | grad norm: 0.806 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      479/   63312 | consumed samples:       490496 | elapsed time per iteration (ms): 23225.7 | learning rate: 7.566E-05 | global batch size:  1024 |tokens per sec: 45147.28680614163 |TFLOPS: 19.35776965782285 | load balancing loss: 1.181641E-01 | lm loss: 4.780193E+00 | loss scale: 1.0 | grad norm: 1.184 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      480/   63312 | consumed samples:       491520 | elapsed time per iteration (ms): 23119.6 | learning rate: 7.582E-05 | global batch size:  1024 |tokens per sec: 45354.37147997133 |TFLOPS: 19.446561204319842 | load balancing loss: 1.171875E-01 | lm loss: 4.801565E+00 | loss scale: 1.0 | grad norm: 1.060 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     480 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     480 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (51712.22, 51712.26)
 iteration      481/   63312 | consumed samples:       492544 | elapsed time per iteration (ms): 75133.1 | learning rate: 7.597E-05 | global batch size:  1024 |tokens per sec: 13956.242277473704 |TFLOPS: 5.984007952817961 | load balancing loss: 1.171875E-01 | lm loss: 4.743267E+00 | loss scale: 1.0 | grad norm: 0.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      482/   63312 | consumed samples:       493568 | elapsed time per iteration (ms): 22824.1 | learning rate: 7.613E-05 | global batch size:  1024 |tokens per sec: 45941.558162436915 |TFLOPS: 19.698328815386198 | load balancing loss: 1.171875E-01 | lm loss: 4.770849E+00 | loss scale: 1.0 | grad norm: 1.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      483/   63312 | consumed samples:       494592 | elapsed time per iteration (ms): 23219.2 | learning rate: 7.629E-05 | global batch size:  1024 |tokens per sec: 45159.78535820033 |TFLOPS: 19.3631286529901 | load balancing loss: 1.162109E-01 | lm loss: 4.821207E+00 | loss scale: 1.0 | grad norm: 1.296 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      484/   63312 | consumed samples:       495616 | elapsed time per iteration (ms): 23260.0 | learning rate: 7.645E-05 | global batch size:  1024 |tokens per sec: 45080.5920438168 |TFLOPS: 19.32917299260106 | load balancing loss: 1.162109E-01 | lm loss: 4.833601E+00 | loss scale: 1.0 | grad norm: 0.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      485/   63312 | consumed samples:       496640 | elapsed time per iteration (ms): 23472.7 | learning rate: 7.660E-05 | global batch size:  1024 |tokens per sec: 44672.202552797404 |TFLOPS: 19.154068124577105 | load balancing loss: 1.176758E-01 | lm loss: 4.809971E+00 | loss scale: 1.0 | grad norm: 1.181 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      486/   63312 | consumed samples:       497664 | elapsed time per iteration (ms): 23224.6 | learning rate: 7.676E-05 | global batch size:  1024 |tokens per sec: 45149.2839033223 |TFLOPS: 19.358625951743093 | load balancing loss: 1.171875E-01 | lm loss: 4.728555E+00 | loss scale: 1.0 | grad norm: 1.095 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      487/   63312 | consumed samples:       498688 | elapsed time per iteration (ms): 23357.1 | learning rate: 7.692E-05 | global batch size:  1024 |tokens per sec: 44893.26780411884 |TFLOPS: 19.248854113219217 | load balancing loss: 1.166992E-01 | lm loss: 4.791486E+00 | loss scale: 1.0 | grad norm: 1.194 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      488/   63312 | consumed samples:       499712 | elapsed time per iteration (ms): 23241.0 | learning rate: 7.708E-05 | global batch size:  1024 |tokens per sec: 45117.43854887767 |TFLOPS: 19.344971642046524 | load balancing loss: 1.171875E-01 | lm loss: 4.731148E+00 | loss scale: 1.0 | grad norm: 1.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      489/   63312 | consumed samples:       500736 | elapsed time per iteration (ms): 23102.2 | learning rate: 7.724E-05 | global batch size:  1024 |tokens per sec: 45388.52471861502 |TFLOPS: 19.461205063862764 | load balancing loss: 1.171875E-01 | lm loss: 4.762589E+00 | loss scale: 1.0 | grad norm: 0.974 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      490/   63312 | consumed samples:       501760 | elapsed time per iteration (ms): 23256.3 | learning rate: 7.739E-05 | global batch size:  1024 |tokens per sec: 45087.76146771967 |TFLOPS: 19.332247021325905 | load balancing loss: 1.171875E-01 | lm loss: 4.744357E+00 | loss scale: 1.0 | grad norm: 0.934 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      491/   63312 | consumed samples:       502784 | elapsed time per iteration (ms): 23048.4 | learning rate: 7.755E-05 | global batch size:  1024 |tokens per sec: 45494.53177238604 |TFLOPS: 19.50665763198309 | load balancing loss: 1.171875E-01 | lm loss: 4.735219E+00 | loss scale: 1.0 | grad norm: 0.893 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      492/   63312 | consumed samples:       503808 | elapsed time per iteration (ms): 23159.4 | learning rate: 7.771E-05 | global batch size:  1024 |tokens per sec: 45276.5600693665 |TFLOPS: 19.41319806181902 | load balancing loss: 1.171875E-01 | lm loss: 4.718426E+00 | loss scale: 1.0 | grad norm: 0.945 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      493/   63312 | consumed samples:       504832 | elapsed time per iteration (ms): 23120.2 | learning rate: 7.787E-05 | global batch size:  1024 |tokens per sec: 45353.178845801645 |TFLOPS: 19.446049839425605 | load balancing loss: 1.162109E-01 | lm loss: 4.729150E+00 | loss scale: 1.0 | grad norm: 0.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      494/   63312 | consumed samples:       505856 | elapsed time per iteration (ms): 23051.1 | learning rate: 7.803E-05 | global batch size:  1024 |tokens per sec: 45489.18536157774 |TFLOPS: 19.504365255270226 | load balancing loss: 1.171875E-01 | lm loss: 4.723289E+00 | loss scale: 1.0 | grad norm: 0.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      495/   63312 | consumed samples:       506880 | elapsed time per iteration (ms): 23165.5 | learning rate: 7.818E-05 | global batch size:  1024 |tokens per sec: 45264.56095614538 |TFLOPS: 19.408053210682567 | load balancing loss: 1.171875E-01 | lm loss: 4.721342E+00 | loss scale: 1.0 | grad norm: 1.063 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      496/   63312 | consumed samples:       507904 | elapsed time per iteration (ms): 23098.6 | learning rate: 7.834E-05 | global batch size:  1024 |tokens per sec: 45395.57970124368 |TFLOPS: 19.46423002368487 | load balancing loss: 1.162109E-01 | lm loss: 4.717287E+00 | loss scale: 1.0 | grad norm: 0.940 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      497/   63312 | consumed samples:       508928 | elapsed time per iteration (ms): 22765.6 | learning rate: 7.850E-05 | global batch size:  1024 |tokens per sec: 46059.73464557435 |TFLOPS: 19.74899926097397 | load balancing loss: 1.157227E-01 | lm loss: 4.682218E+00 | loss scale: 1.0 | grad norm: 0.991 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      498/   63312 | consumed samples:       509952 | elapsed time per iteration (ms): 23009.3 | learning rate: 7.866E-05 | global batch size:  1024 |tokens per sec: 45571.93175763466 |TFLOPS: 19.539844367930147 | load balancing loss: 1.166992E-01 | lm loss: 4.680738E+00 | loss scale: 1.0 | grad norm: 1.083 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      499/   63312 | consumed samples:       510976 | elapsed time per iteration (ms): 22971.1 | learning rate: 7.882E-05 | global batch size:  1024 |tokens per sec: 45647.658656438376 |TFLOPS: 19.57231373580685 | load balancing loss: 1.166992E-01 | lm loss: 4.696514E+00 | loss scale: 1.0 | grad norm: 1.186 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      500/   63312 | consumed samples:       512000 | elapsed time per iteration (ms): 22964.8 | learning rate: 7.897E-05 | global batch size:  1024 |tokens per sec: 45660.04806218438 |TFLOPS: 19.577625932388194 | load balancing loss: 1.152344E-01 | lm loss: 4.733054E+00 | loss scale: 1.0 | grad norm: 1.060 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 500 | lm loss value: 4.381637E+00 | lm loss PPL: 7.996880E+01 | 
-----------------------------------------------------------------------------------------------
saving checkpoint at iteration     500 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     500 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (50138.93, 50138.99)
 iteration      501/   63312 | consumed samples:       513024 | elapsed time per iteration (ms): 167067.7 | learning rate: 7.913E-05 | global batch size:  1024 |tokens per sec: 6276.352336057752 |TFLOPS: 2.6911070721578003 | load balancing loss: 1.166992E-01 | lm loss: 4.691795E+00 | loss scale: 1.0 | grad norm: 1.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      502/   63312 | consumed samples:       514048 | elapsed time per iteration (ms): 23051.0 | learning rate: 7.929E-05 | global batch size:  1024 |tokens per sec: 45489.44601819461 |TFLOPS: 19.50447701682017 | load balancing loss: 1.162109E-01 | lm loss: 4.647385E+00 | loss scale: 1.0 | grad norm: 0.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      503/   63312 | consumed samples:       515072 | elapsed time per iteration (ms): 22653.7 | learning rate: 7.945E-05 | global batch size:  1024 |tokens per sec: 46287.18610483051 |TFLOPS: 19.846523459394177 | load balancing loss: 1.171875E-01 | lm loss: 4.713244E+00 | loss scale: 1.0 | grad norm: 1.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      504/   63312 | consumed samples:       516096 | elapsed time per iteration (ms): 22912.2 | learning rate: 7.961E-05 | global batch size:  1024 |tokens per sec: 45764.918931123444 |TFLOPS: 19.62259133935617 | load balancing loss: 1.157227E-01 | lm loss: 4.681993E+00 | loss scale: 1.0 | grad norm: 0.974 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      505/   63312 | consumed samples:       517120 | elapsed time per iteration (ms): 23022.7 | learning rate: 7.976E-05 | global batch size:  1024 |tokens per sec: 45545.281188590445 |TFLOPS: 19.528417422629467 | load balancing loss: 1.171875E-01 | lm loss: 4.652884E+00 | loss scale: 1.0 | grad norm: 1.079 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      506/   63312 | consumed samples:       518144 | elapsed time per iteration (ms): 23455.8 | learning rate: 7.992E-05 | global batch size:  1024 |tokens per sec: 44704.34422834851 |TFLOPS: 19.167849487661147 | load balancing loss: 1.176758E-01 | lm loss: 4.685410E+00 | loss scale: 1.0 | grad norm: 1.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      507/   63312 | consumed samples:       519168 | elapsed time per iteration (ms): 22960.4 | learning rate: 8.008E-05 | global batch size:  1024 |tokens per sec: 45668.94843305452 |TFLOPS: 19.581442137997914 | load balancing loss: 1.162109E-01 | lm loss: 4.699315E+00 | loss scale: 1.0 | grad norm: 1.045 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      508/   63312 | consumed samples:       520192 | elapsed time per iteration (ms): 22668.7 | learning rate: 8.024E-05 | global batch size:  1024 |tokens per sec: 46256.53697823417 |TFLOPS: 19.83338205545084 | load balancing loss: 1.162109E-01 | lm loss: 4.653930E+00 | loss scale: 1.0 | grad norm: 0.885 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      509/   63312 | consumed samples:       521216 | elapsed time per iteration (ms): 22919.2 | learning rate: 8.040E-05 | global batch size:  1024 |tokens per sec: 45750.9933244306 |TFLOPS: 19.616620467000917 | load balancing loss: 1.181641E-01 | lm loss: 4.633633E+00 | loss scale: 1.0 | grad norm: 1.176 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      510/   63312 | consumed samples:       522240 | elapsed time per iteration (ms): 23200.2 | learning rate: 8.055E-05 | global batch size:  1024 |tokens per sec: 45196.765667165186 |TFLOPS: 19.378984673439138 | load balancing loss: 1.171875E-01 | lm loss: 4.678016E+00 | loss scale: 1.0 | grad norm: 0.856 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      511/   63312 | consumed samples:       523264 | elapsed time per iteration (ms): 22889.4 | learning rate: 8.071E-05 | global batch size:  1024 |tokens per sec: 45810.62094894208 |TFLOPS: 19.642186960630813 | load balancing loss: 1.181641E-01 | lm loss: 4.664812E+00 | loss scale: 1.0 | grad norm: 1.098 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      512/   63312 | consumed samples:       524288 | elapsed time per iteration (ms): 22849.7 | learning rate: 8.087E-05 | global batch size:  1024 |tokens per sec: 45890.18229378492 |TFLOPS: 19.67630042117491 | load balancing loss: 1.166992E-01 | lm loss: 4.646952E+00 | loss scale: 1.0 | grad norm: 0.994 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      513/   63312 | consumed samples:       525312 | elapsed time per iteration (ms): 22657.2 | learning rate: 8.103E-05 | global batch size:  1024 |tokens per sec: 46279.95550882805 |TFLOPS: 19.843423201952238 | load balancing loss: 1.162109E-01 | lm loss: 4.640208E+00 | loss scale: 1.0 | grad norm: 0.942 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      514/   63312 | consumed samples:       526336 | elapsed time per iteration (ms): 22904.0 | learning rate: 8.119E-05 | global batch size:  1024 |tokens per sec: 45781.28476607322 |TFLOPS: 19.629608506624184 | load balancing loss: 1.162109E-01 | lm loss: 4.618189E+00 | loss scale: 1.0 | grad norm: 1.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      515/   63312 | consumed samples:       527360 | elapsed time per iteration (ms): 23366.5 | learning rate: 8.134E-05 | global batch size:  1024 |tokens per sec: 44875.10004173299 |TFLOPS: 19.241064334821825 | load balancing loss: 1.171875E-01 | lm loss: 4.634127E+00 | loss scale: 1.0 | grad norm: 1.241 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      516/   63312 | consumed samples:       528384 | elapsed time per iteration (ms): 22603.2 | learning rate: 8.150E-05 | global batch size:  1024 |tokens per sec: 46390.579093908964 |TFLOPS: 19.890855196878395 | load balancing loss: 1.176758E-01 | lm loss: 4.659620E+00 | loss scale: 1.0 | grad norm: 1.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      517/   63312 | consumed samples:       529408 | elapsed time per iteration (ms): 22637.9 | learning rate: 8.166E-05 | global batch size:  1024 |tokens per sec: 46319.39638127803 |TFLOPS: 19.860334236435186 | load balancing loss: 1.171875E-01 | lm loss: 4.623554E+00 | loss scale: 1.0 | grad norm: 1.177 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      518/   63312 | consumed samples:       530432 | elapsed time per iteration (ms): 22904.7 | learning rate: 8.182E-05 | global batch size:  1024 |tokens per sec: 45780.04384162079 |TFLOPS: 19.62907643634018 | load balancing loss: 1.157227E-01 | lm loss: 4.688922E+00 | loss scale: 1.0 | grad norm: 1.115 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      519/   63312 | consumed samples:       531456 | elapsed time per iteration (ms): 23436.3 | learning rate: 8.197E-05 | global batch size:  1024 |tokens per sec: 44741.44278196623 |TFLOPS: 19.183756207784853 | load balancing loss: 1.171875E-01 | lm loss: 4.634345E+00 | loss scale: 1.0 | grad norm: 1.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      520/   63312 | consumed samples:       532480 | elapsed time per iteration (ms): 23443.6 | learning rate: 8.213E-05 | global batch size:  1024 |tokens per sec: 44727.681281765035 |TFLOPS: 19.177855699251875 | load balancing loss: 1.171875E-01 | lm loss: 4.621112E+00 | loss scale: 1.0 | grad norm: 0.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     520 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     520 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (58867.62, 58867.66)
 iteration      521/   63312 | consumed samples:       533504 | elapsed time per iteration (ms): 82060.5 | learning rate: 8.229E-05 | global batch size:  1024 |tokens per sec: 12778.090156822813 |TFLOPS: 5.478852516316005 | load balancing loss: 1.181641E-01 | lm loss: 4.609691E+00 | loss scale: 1.0 | grad norm: 0.950 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      522/   63312 | consumed samples:       534528 | elapsed time per iteration (ms): 22970.5 | learning rate: 8.245E-05 | global batch size:  1024 |tokens per sec: 45648.776332038586 |TFLOPS: 19.572792960769227 | load balancing loss: 1.166992E-01 | lm loss: 4.604438E+00 | loss scale: 1.0 | grad norm: 1.159 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      523/   63312 | consumed samples:       535552 | elapsed time per iteration (ms): 22871.9 | learning rate: 8.261E-05 | global batch size:  1024 |tokens per sec: 45845.69013742773 |TFLOPS: 19.657223551327988 | load balancing loss: 1.171875E-01 | lm loss: 4.606336E+00 | loss scale: 1.0 | grad norm: 0.883 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      524/   63312 | consumed samples:       536576 | elapsed time per iteration (ms): 23056.8 | learning rate: 8.276E-05 | global batch size:  1024 |tokens per sec: 45477.88636968742 |TFLOPS: 19.49952059465269 | load balancing loss: 1.176758E-01 | lm loss: 4.608793E+00 | loss scale: 1.0 | grad norm: 1.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      525/   63312 | consumed samples:       537600 | elapsed time per iteration (ms): 23146.1 | learning rate: 8.292E-05 | global batch size:  1024 |tokens per sec: 45302.50127734089 |TFLOPS: 19.42432085488453 | load balancing loss: 1.171875E-01 | lm loss: 4.612973E+00 | loss scale: 1.0 | grad norm: 1.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      526/   63312 | consumed samples:       538624 | elapsed time per iteration (ms): 22844.0 | learning rate: 8.308E-05 | global batch size:  1024 |tokens per sec: 45901.56590632226 |TFLOPS: 19.681181364526346 | load balancing loss: 1.181641E-01 | lm loss: 4.597059E+00 | loss scale: 1.0 | grad norm: 1.092 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      527/   63312 | consumed samples:       539648 | elapsed time per iteration (ms): 23236.2 | learning rate: 8.324E-05 | global batch size:  1024 |tokens per sec: 45126.81481886131 |TFLOPS: 19.348991898576493 | load balancing loss: 1.171875E-01 | lm loss: 4.593481E+00 | loss scale: 1.0 | grad norm: 0.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      528/   63312 | consumed samples:       540672 | elapsed time per iteration (ms): 23142.9 | learning rate: 8.340E-05 | global batch size:  1024 |tokens per sec: 45308.6823374603 |TFLOPS: 19.426971103581568 | load balancing loss: 1.162109E-01 | lm loss: 4.548055E+00 | loss scale: 1.0 | grad norm: 0.914 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      529/   63312 | consumed samples:       541696 | elapsed time per iteration (ms): 23100.5 | learning rate: 8.355E-05 | global batch size:  1024 |tokens per sec: 45391.904596349275 |TFLOPS: 19.462654251605368 | load balancing loss: 1.166992E-01 | lm loss: 4.537391E+00 | loss scale: 1.0 | grad norm: 1.029 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      530/   63312 | consumed samples:       542720 | elapsed time per iteration (ms): 23219.8 | learning rate: 8.371E-05 | global batch size:  1024 |tokens per sec: 45158.62148290781 |TFLOPS: 19.362629619018914 | load balancing loss: 1.166992E-01 | lm loss: 4.577821E+00 | loss scale: 1.0 | grad norm: 0.987 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      531/   63312 | consumed samples:       543744 | elapsed time per iteration (ms): 22977.9 | learning rate: 8.387E-05 | global batch size:  1024 |tokens per sec: 45634.10924026154 |TFLOPS: 19.56650416238829 | load balancing loss: 1.166992E-01 | lm loss: 4.562728E+00 | loss scale: 1.0 | grad norm: 1.167 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      532/   63312 | consumed samples:       544768 | elapsed time per iteration (ms): 23023.0 | learning rate: 8.403E-05 | global batch size:  1024 |tokens per sec: 45544.63879982137 |TFLOPS: 19.52814198605947 | load balancing loss: 1.171875E-01 | lm loss: 4.533596E+00 | loss scale: 1.0 | grad norm: 1.175 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      533/   63312 | consumed samples:       545792 | elapsed time per iteration (ms): 22903.9 | learning rate: 8.419E-05 | global batch size:  1024 |tokens per sec: 45781.62503130408 |TFLOPS: 19.629754401902225 | load balancing loss: 1.166992E-01 | lm loss: 4.521178E+00 | loss scale: 1.0 | grad norm: 0.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      534/   63312 | consumed samples:       546816 | elapsed time per iteration (ms): 22884.1 | learning rate: 8.434E-05 | global batch size:  1024 |tokens per sec: 45821.16213619666 |TFLOPS: 19.646706697900303 | load balancing loss: 1.171875E-01 | lm loss: 4.539286E+00 | loss scale: 1.0 | grad norm: 2.163 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      535/   63312 | consumed samples:       547840 | elapsed time per iteration (ms): 23710.6 | learning rate: 8.450E-05 | global batch size:  1024 |tokens per sec: 44224.0133863607 |TFLOPS: 18.961898378380237 | load balancing loss: 1.181641E-01 | lm loss: 4.576271E+00 | loss scale: 1.0 | grad norm: 1.028 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      536/   63312 | consumed samples:       548864 | elapsed time per iteration (ms): 23443.2 | learning rate: 8.466E-05 | global batch size:  1024 |tokens per sec: 44728.28900425732 |TFLOPS: 19.178116271987328 | load balancing loss: 1.166992E-01 | lm loss: 4.595796E+00 | loss scale: 1.0 | grad norm: 1.025 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      537/   63312 | consumed samples:       549888 | elapsed time per iteration (ms): 22606.3 | learning rate: 8.482E-05 | global batch size:  1024 |tokens per sec: 46384.25490677581 |TFLOPS: 19.88814357971473 | load balancing loss: 1.166992E-01 | lm loss: 4.593043E+00 | loss scale: 1.0 | grad norm: 1.153 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      538/   63312 | consumed samples:       550912 | elapsed time per iteration (ms): 22597.9 | learning rate: 8.498E-05 | global batch size:  1024 |tokens per sec: 46401.57395580418 |TFLOPS: 19.895569455896908 | load balancing loss: 1.162109E-01 | lm loss: 4.561438E+00 | loss scale: 1.0 | grad norm: 0.810 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      539/   63312 | consumed samples:       551936 | elapsed time per iteration (ms): 22961.8 | learning rate: 8.513E-05 | global batch size:  1024 |tokens per sec: 45666.124612357315 |TFLOPS: 19.580231370430724 | load balancing loss: 1.166992E-01 | lm loss: 4.479572E+00 | loss scale: 1.0 | grad norm: 0.896 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      540/   63312 | consumed samples:       552960 | elapsed time per iteration (ms): 24007.2 | learning rate: 8.529E-05 | global batch size:  1024 |tokens per sec: 43677.64450745823 |TFLOPS: 18.72763218755882 | load balancing loss: 1.162109E-01 | lm loss: 4.515703E+00 | loss scale: 1.0 | grad norm: 0.961 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     540 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     540 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (53408.95, 53408.98)
 iteration      541/   63312 | consumed samples:       553984 | elapsed time per iteration (ms): 76255.0 | learning rate: 8.545E-05 | global batch size:  1024 |tokens per sec: 13750.912265842857 |TFLOPS: 5.895968751568611 | load balancing loss: 1.157227E-01 | lm loss: 4.571018E+00 | loss scale: 1.0 | grad norm: 0.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      542/   63312 | consumed samples:       555008 | elapsed time per iteration (ms): 22831.1 | learning rate: 8.561E-05 | global batch size:  1024 |tokens per sec: 45927.5852978812 |TFLOPS: 19.69233768031111 | load balancing loss: 1.157227E-01 | lm loss: 4.548439E+00 | loss scale: 1.0 | grad norm: 0.942 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      543/   63312 | consumed samples:       556032 | elapsed time per iteration (ms): 23198.3 | learning rate: 8.577E-05 | global batch size:  1024 |tokens per sec: 45200.51143975539 |TFLOPS: 19.380590745655685 | load balancing loss: 1.171875E-01 | lm loss: 4.532779E+00 | loss scale: 1.0 | grad norm: 0.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      544/   63312 | consumed samples:       557056 | elapsed time per iteration (ms): 22810.3 | learning rate: 8.592E-05 | global batch size:  1024 |tokens per sec: 45969.47079428291 |TFLOPS: 19.71029689444571 | load balancing loss: 1.157227E-01 | lm loss: 4.462699E+00 | loss scale: 1.0 | grad norm: 0.953 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      545/   63312 | consumed samples:       558080 | elapsed time per iteration (ms): 22859.3 | learning rate: 8.608E-05 | global batch size:  1024 |tokens per sec: 45870.82664742685 |TFLOPS: 19.668001314622025 | load balancing loss: 1.157227E-01 | lm loss: 4.492402E+00 | loss scale: 1.0 | grad norm: 0.883 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      546/   63312 | consumed samples:       559104 | elapsed time per iteration (ms): 22584.5 | learning rate: 8.624E-05 | global batch size:  1024 |tokens per sec: 46429.02951975673 |TFLOPS: 19.90734155828484 | load balancing loss: 1.162109E-01 | lm loss: 4.511569E+00 | loss scale: 1.0 | grad norm: 0.758 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      547/   63312 | consumed samples:       560128 | elapsed time per iteration (ms): 22924.4 | learning rate: 8.640E-05 | global batch size:  1024 |tokens per sec: 45740.55670145058 |TFLOPS: 19.61214556368014 | load balancing loss: 1.157227E-01 | lm loss: 4.502808E+00 | loss scale: 1.0 | grad norm: 0.893 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      548/   63312 | consumed samples:       561152 | elapsed time per iteration (ms): 22690.0 | learning rate: 8.656E-05 | global batch size:  1024 |tokens per sec: 46213.053300014755 |TFLOPS: 19.81473759004895 | load balancing loss: 1.157227E-01 | lm loss: 4.422387E+00 | loss scale: 1.0 | grad norm: 0.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      549/   63312 | consumed samples:       562176 | elapsed time per iteration (ms): 22856.5 | learning rate: 8.671E-05 | global batch size:  1024 |tokens per sec: 45876.42011057457 |TFLOPS: 19.670399619788576 | load balancing loss: 1.157227E-01 | lm loss: 4.459209E+00 | loss scale: 1.0 | grad norm: 0.781 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      550/   63312 | consumed samples:       563200 | elapsed time per iteration (ms): 23278.2 | learning rate: 8.687E-05 | global batch size:  1024 |tokens per sec: 45045.33410659998 |TFLOPS: 19.314055472246313 | load balancing loss: 1.152344E-01 | lm loss: 4.501600E+00 | loss scale: 1.0 | grad norm: 0.906 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      551/   63312 | consumed samples:       564224 | elapsed time per iteration (ms): 23004.6 | learning rate: 8.703E-05 | global batch size:  1024 |tokens per sec: 45581.119035771575 |TFLOPS: 19.54378359056207 | load balancing loss: 1.157227E-01 | lm loss: 4.503835E+00 | loss scale: 1.0 | grad norm: 1.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      552/   63312 | consumed samples:       565248 | elapsed time per iteration (ms): 22668.7 | learning rate: 8.719E-05 | global batch size:  1024 |tokens per sec: 46256.58611519022 |TFLOPS: 19.833403123868106 | load balancing loss: 1.162109E-01 | lm loss: 4.437563E+00 | loss scale: 1.0 | grad norm: 0.951 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      553/   63312 | consumed samples:       566272 | elapsed time per iteration (ms): 22635.1 | learning rate: 8.735E-05 | global batch size:  1024 |tokens per sec: 46325.13149940488 |TFLOPS: 19.862793278905155 | load balancing loss: 1.162109E-01 | lm loss: 4.463475E+00 | loss scale: 1.0 | grad norm: 0.804 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      554/   63312 | consumed samples:       567296 | elapsed time per iteration (ms): 23039.7 | learning rate: 8.750E-05 | global batch size:  1024 |tokens per sec: 45511.69941096708 |TFLOPS: 19.514018588016775 | load balancing loss: 1.147461E-01 | lm loss: 4.452485E+00 | loss scale: 1.0 | grad norm: 0.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      555/   63312 | consumed samples:       568320 | elapsed time per iteration (ms): 22978.9 | learning rate: 8.766E-05 | global batch size:  1024 |tokens per sec: 45632.194489453046 |TFLOPS: 19.56568317606275 | load balancing loss: 1.157227E-01 | lm loss: 4.413871E+00 | loss scale: 1.0 | grad norm: 0.899 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      556/   63312 | consumed samples:       569344 | elapsed time per iteration (ms): 22553.0 | learning rate: 8.782E-05 | global batch size:  1024 |tokens per sec: 46493.816203938455 |TFLOPS: 19.935120098215023 | load balancing loss: 1.152344E-01 | lm loss: 4.456166E+00 | loss scale: 1.0 | grad norm: 0.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      557/   63312 | consumed samples:       570368 | elapsed time per iteration (ms): 22661.1 | learning rate: 8.798E-05 | global batch size:  1024 |tokens per sec: 46272.08990064873 |TFLOPS: 19.84005067079643 | load balancing loss: 1.157227E-01 | lm loss: 4.461415E+00 | loss scale: 1.0 | grad norm: 0.885 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      558/   63312 | consumed samples:       571392 | elapsed time per iteration (ms): 22328.9 | learning rate: 8.813E-05 | global batch size:  1024 |tokens per sec: 46960.55266779566 |TFLOPS: 20.13524235577282 | load balancing loss: 1.152344E-01 | lm loss: 4.456502E+00 | loss scale: 1.0 | grad norm: 0.843 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      559/   63312 | consumed samples:       572416 | elapsed time per iteration (ms): 22763.5 | learning rate: 8.829E-05 | global batch size:  1024 |tokens per sec: 46063.959656358114 |TFLOPS: 19.750810815805718 | load balancing loss: 1.162109E-01 | lm loss: 4.431529E+00 | loss scale: 1.0 | grad norm: 0.828 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      560/   63312 | consumed samples:       573440 | elapsed time per iteration (ms): 22760.0 | learning rate: 8.845E-05 | global batch size:  1024 |tokens per sec: 46071.10022923744 |TFLOPS: 19.75387247409797 | load balancing loss: 1.152344E-01 | lm loss: 4.381211E+00 | loss scale: 1.0 | grad norm: 0.904 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     560 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     560 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (54654.88, 54654.89)
 iteration      561/   63312 | consumed samples:       574464 | elapsed time per iteration (ms): 77807.2 | learning rate: 8.861E-05 | global batch size:  1024 |tokens per sec: 13476.585298432134 |TFLOPS: 5.778345775267331 | load balancing loss: 1.162109E-01 | lm loss: 4.391005E+00 | loss scale: 1.0 | grad norm: 0.982 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      562/   63312 | consumed samples:       575488 | elapsed time per iteration (ms): 22387.0 | learning rate: 8.877E-05 | global batch size:  1024 |tokens per sec: 46838.61550930076 |TFLOPS: 20.082959448119723 | load balancing loss: 1.152344E-01 | lm loss: 4.451691E+00 | loss scale: 1.0 | grad norm: 0.944 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      563/   63312 | consumed samples:       576512 | elapsed time per iteration (ms): 22320.1 | learning rate: 8.892E-05 | global batch size:  1024 |tokens per sec: 46979.00435755032 |TFLOPS: 20.143153873503714 | load balancing loss: 1.157227E-01 | lm loss: 4.390495E+00 | loss scale: 1.0 | grad norm: 1.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      564/   63312 | consumed samples:       577536 | elapsed time per iteration (ms): 22525.9 | learning rate: 8.908E-05 | global batch size:  1024 |tokens per sec: 46549.89502838725 |TFLOPS: 19.95916497539718 | load balancing loss: 1.157227E-01 | lm loss: 4.389836E+00 | loss scale: 1.0 | grad norm: 0.883 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      565/   63312 | consumed samples:       578560 | elapsed time per iteration (ms): 22849.4 | learning rate: 8.924E-05 | global batch size:  1024 |tokens per sec: 45890.76168325951 |TFLOPS: 19.67654884558277 | load balancing loss: 1.157227E-01 | lm loss: 4.402883E+00 | loss scale: 1.0 | grad norm: 0.955 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      566/   63312 | consumed samples:       579584 | elapsed time per iteration (ms): 22859.3 | learning rate: 8.940E-05 | global batch size:  1024 |tokens per sec: 45870.81707894096 |TFLOPS: 19.667997211949217 | load balancing loss: 1.157227E-01 | lm loss: 4.397172E+00 | loss scale: 1.0 | grad norm: 1.258 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      567/   63312 | consumed samples:       580608 | elapsed time per iteration (ms): 22850.7 | learning rate: 8.956E-05 | global batch size:  1024 |tokens per sec: 45888.04873438698 |TFLOPS: 19.675385616448022 | load balancing loss: 1.162109E-01 | lm loss: 4.407812E+00 | loss scale: 1.0 | grad norm: 0.874 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      568/   63312 | consumed samples:       581632 | elapsed time per iteration (ms): 23239.7 | learning rate: 8.971E-05 | global batch size:  1024 |tokens per sec: 45120.107430927135 |TFLOPS: 19.346115976681276 | load balancing loss: 1.152344E-01 | lm loss: 4.415545E+00 | loss scale: 1.0 | grad norm: 0.883 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      569/   63312 | consumed samples:       582656 | elapsed time per iteration (ms): 23142.0 | learning rate: 8.987E-05 | global batch size:  1024 |tokens per sec: 45310.51728510988 |TFLOPS: 19.42775787276412 | load balancing loss: 1.157227E-01 | lm loss: 4.342065E+00 | loss scale: 1.0 | grad norm: 1.080 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      570/   63312 | consumed samples:       583680 | elapsed time per iteration (ms): 23030.7 | learning rate: 9.003E-05 | global batch size:  1024 |tokens per sec: 45529.53420998465 |TFLOPS: 19.521665601950488 | load balancing loss: 1.171875E-01 | lm loss: 4.413232E+00 | loss scale: 1.0 | grad norm: 1.035 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      571/   63312 | consumed samples:       584704 | elapsed time per iteration (ms): 22524.4 | learning rate: 9.019E-05 | global batch size:  1024 |tokens per sec: 46552.83167204391 |TFLOPS: 19.960424117983464 | load balancing loss: 1.152344E-01 | lm loss: 4.406602E+00 | loss scale: 1.0 | grad norm: 0.953 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      572/   63312 | consumed samples:       585728 | elapsed time per iteration (ms): 23148.2 | learning rate: 9.035E-05 | global batch size:  1024 |tokens per sec: 45298.3154118215 |TFLOPS: 19.42252608433961 | load balancing loss: 1.171875E-01 | lm loss: 4.398956E+00 | loss scale: 1.0 | grad norm: 1.162 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      573/   63312 | consumed samples:       586752 | elapsed time per iteration (ms): 22627.6 | learning rate: 9.050E-05 | global batch size:  1024 |tokens per sec: 46340.568483855575 |TFLOPS: 19.869412183613445 | load balancing loss: 1.162109E-01 | lm loss: 4.362841E+00 | loss scale: 1.0 | grad norm: 0.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      574/   63312 | consumed samples:       587776 | elapsed time per iteration (ms): 22752.2 | learning rate: 9.066E-05 | global batch size:  1024 |tokens per sec: 46086.77980859141 |TFLOPS: 19.760595396048277 | load balancing loss: 1.162109E-01 | lm loss: 4.353695E+00 | loss scale: 1.0 | grad norm: 0.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      575/   63312 | consumed samples:       588800 | elapsed time per iteration (ms): 22891.8 | learning rate: 9.082E-05 | global batch size:  1024 |tokens per sec: 45805.77294428875 |TFLOPS: 19.640108285166168 | load balancing loss: 1.157227E-01 | lm loss: 4.365274E+00 | loss scale: 1.0 | grad norm: 0.865 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      576/   63312 | consumed samples:       589824 | elapsed time per iteration (ms): 22561.8 | learning rate: 9.098E-05 | global batch size:  1024 |tokens per sec: 46475.63113465767 |TFLOPS: 19.92732289915275 | load balancing loss: 1.157227E-01 | lm loss: 4.329258E+00 | loss scale: 1.0 | grad norm: 0.836 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      577/   63312 | consumed samples:       590848 | elapsed time per iteration (ms): 22407.4 | learning rate: 9.114E-05 | global batch size:  1024 |tokens per sec: 46796.03513776887 |TFLOPS: 20.064702292875857 | load balancing loss: 1.142578E-01 | lm loss: 4.383095E+00 | loss scale: 1.0 | grad norm: 0.797 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      578/   63312 | consumed samples:       591872 | elapsed time per iteration (ms): 22608.5 | learning rate: 9.129E-05 | global batch size:  1024 |tokens per sec: 46379.65106740826 |TFLOPS: 19.886169594823873 | load balancing loss: 1.157227E-01 | lm loss: 4.388816E+00 | loss scale: 1.0 | grad norm: 0.949 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      579/   63312 | consumed samples:       592896 | elapsed time per iteration (ms): 23352.7 | learning rate: 9.145E-05 | global batch size:  1024 |tokens per sec: 44901.79745981232 |TFLOPS: 19.25251136754956 | load balancing loss: 1.162109E-01 | lm loss: 4.358399E+00 | loss scale: 1.0 | grad norm: 0.837 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      580/   63312 | consumed samples:       593920 | elapsed time per iteration (ms): 22770.2 | learning rate: 9.161E-05 | global batch size:  1024 |tokens per sec: 46050.45083578433 |TFLOPS: 19.745018648534646 | load balancing loss: 1.157227E-01 | lm loss: 4.333237E+00 | loss scale: 1.0 | grad norm: 0.734 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     580 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     580 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (69249.92, 69249.94)
 iteration      581/   63312 | consumed samples:       594944 | elapsed time per iteration (ms): 91822.3 | learning rate: 9.177E-05 | global batch size:  1024 |tokens per sec: 11419.621789771645 |TFLOPS: 4.8963830126726995 | load balancing loss: 1.162109E-01 | lm loss: 4.293857E+00 | loss scale: 1.0 | grad norm: 0.703 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      582/   63312 | consumed samples:       595968 | elapsed time per iteration (ms): 22377.8 | learning rate: 9.193E-05 | global batch size:  1024 |tokens per sec: 46857.94896748142 |TFLOPS: 20.091249041063747 | load balancing loss: 1.152344E-01 | lm loss: 4.298236E+00 | loss scale: 1.0 | grad norm: 0.687 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      583/   63312 | consumed samples:       596992 | elapsed time per iteration (ms): 22542.7 | learning rate: 9.208E-05 | global batch size:  1024 |tokens per sec: 46515.05019467633 |TFLOPS: 19.944224581135227 | load balancing loss: 1.157227E-01 | lm loss: 4.275306E+00 | loss scale: 1.0 | grad norm: 0.668 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      584/   63312 | consumed samples:       598016 | elapsed time per iteration (ms): 22494.9 | learning rate: 9.224E-05 | global batch size:  1024 |tokens per sec: 46613.84778378246 |TFLOPS: 19.986585952282 | load balancing loss: 1.152344E-01 | lm loss: 4.358368E+00 | loss scale: 1.0 | grad norm: 0.795 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      585/   63312 | consumed samples:       599040 | elapsed time per iteration (ms): 22452.7 | learning rate: 9.240E-05 | global batch size:  1024 |tokens per sec: 46701.643800878956 |TFLOPS: 20.02423018731933 | load balancing loss: 1.157227E-01 | lm loss: 4.284424E+00 | loss scale: 1.0 | grad norm: 0.967 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      586/   63312 | consumed samples:       600064 | elapsed time per iteration (ms): 22831.9 | learning rate: 9.256E-05 | global batch size:  1024 |tokens per sec: 45925.88994232707 |TFLOPS: 19.691610763930818 | load balancing loss: 1.157227E-01 | lm loss: 4.314859E+00 | loss scale: 1.0 | grad norm: 0.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      587/   63312 | consumed samples:       601088 | elapsed time per iteration (ms): 22498.9 | learning rate: 9.272E-05 | global batch size:  1024 |tokens per sec: 46605.674681101285 |TFLOPS: 19.98308157693003 | load balancing loss: 1.152344E-01 | lm loss: 4.301322E+00 | loss scale: 1.0 | grad norm: 0.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      588/   63312 | consumed samples:       602112 | elapsed time per iteration (ms): 22615.0 | learning rate: 9.287E-05 | global batch size:  1024 |tokens per sec: 46366.29327053505 |TFLOPS: 19.880442181876155 | load balancing loss: 1.157227E-01 | lm loss: 4.292130E+00 | loss scale: 1.0 | grad norm: 0.877 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      589/   63312 | consumed samples:       603136 | elapsed time per iteration (ms): 23130.7 | learning rate: 9.303E-05 | global batch size:  1024 |tokens per sec: 45332.658062231596 |TFLOPS: 19.437251157829238 | load balancing loss: 1.152344E-01 | lm loss: 4.266369E+00 | loss scale: 1.0 | grad norm: 0.937 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      590/   63312 | consumed samples:       604160 | elapsed time per iteration (ms): 22912.2 | learning rate: 9.319E-05 | global batch size:  1024 |tokens per sec: 45764.951313953214 |TFLOPS: 19.62260522411877 | load balancing loss: 1.152344E-01 | lm loss: 4.308799E+00 | loss scale: 1.0 | grad norm: 1.057 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      591/   63312 | consumed samples:       605184 | elapsed time per iteration (ms): 22815.2 | learning rate: 9.335E-05 | global batch size:  1024 |tokens per sec: 45959.62777927277 |TFLOPS: 19.706076511986783 | load balancing loss: 1.157227E-01 | lm loss: 4.276509E+00 | loss scale: 1.0 | grad norm: 0.903 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      592/   63312 | consumed samples:       606208 | elapsed time per iteration (ms): 22333.0 | learning rate: 9.351E-05 | global batch size:  1024 |tokens per sec: 46951.87860298479 |TFLOPS: 20.131523183249197 | load balancing loss: 1.157227E-01 | lm loss: 4.243061E+00 | loss scale: 1.0 | grad norm: 0.863 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      593/   63312 | consumed samples:       607232 | elapsed time per iteration (ms): 22672.2 | learning rate: 9.366E-05 | global batch size:  1024 |tokens per sec: 46249.50514556058 |TFLOPS: 19.8303670216183 | load balancing loss: 1.157227E-01 | lm loss: 4.277063E+00 | loss scale: 1.0 | grad norm: 0.970 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      594/   63312 | consumed samples:       608256 | elapsed time per iteration (ms): 22804.3 | learning rate: 9.382E-05 | global batch size:  1024 |tokens per sec: 45981.47498421978 |TFLOPS: 19.715443922321928 | load balancing loss: 1.166992E-01 | lm loss: 4.267133E+00 | loss scale: 1.0 | grad norm: 0.838 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      595/   63312 | consumed samples:       609280 | elapsed time per iteration (ms): 22463.2 | learning rate: 9.398E-05 | global batch size:  1024 |tokens per sec: 46679.66095708702 |TFLOPS: 20.01480461921431 | load balancing loss: 1.157227E-01 | lm loss: 4.260725E+00 | loss scale: 1.0 | grad norm: 0.694 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      596/   63312 | consumed samples:       610304 | elapsed time per iteration (ms): 22565.6 | learning rate: 9.414E-05 | global batch size:  1024 |tokens per sec: 46467.92079177626 |TFLOPS: 19.924016940986935 | load balancing loss: 1.152344E-01 | lm loss: 4.212018E+00 | loss scale: 1.0 | grad norm: 0.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      597/   63312 | consumed samples:       611328 | elapsed time per iteration (ms): 22863.4 | learning rate: 9.429E-05 | global batch size:  1024 |tokens per sec: 45862.684352943805 |TFLOPS: 19.664510148878982 | load balancing loss: 1.152344E-01 | lm loss: 4.239141E+00 | loss scale: 1.0 | grad norm: 0.786 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      598/   63312 | consumed samples:       612352 | elapsed time per iteration (ms): 22400.5 | learning rate: 9.445E-05 | global batch size:  1024 |tokens per sec: 46810.37908921802 |TFLOPS: 20.070852538611067 | load balancing loss: 1.152344E-01 | lm loss: 4.218850E+00 | loss scale: 1.0 | grad norm: 0.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      599/   63312 | consumed samples:       613376 | elapsed time per iteration (ms): 23093.8 | learning rate: 9.461E-05 | global batch size:  1024 |tokens per sec: 45405.10944656836 |TFLOPS: 19.468316085726318 | load balancing loss: 1.147461E-01 | lm loss: 4.226604E+00 | loss scale: 1.0 | grad norm: 0.747 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      600/   63312 | consumed samples:       614400 | elapsed time per iteration (ms): 23050.2 | learning rate: 9.477E-05 | global batch size:  1024 |tokens per sec: 45490.896622130815 |TFLOPS: 19.505098991225463 | load balancing loss: 1.157227E-01 | lm loss: 4.195920E+00 | loss scale: 1.0 | grad norm: 0.879 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     600 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
-----------------------------------------------------------------------------------------------
 validation loss at iteration 600 | lm loss value: 3.853112E+00 | lm loss PPL: 4.713956E+01 | 
-----------------------------------------------------------------------------------------------
  successfully saved checkpoint at iteration     600 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (47032.90, 47032.95)
 iteration      601/   63312 | consumed samples:       615424 | elapsed time per iteration (ms): 161595.4 | learning rate: 9.493E-05 | global batch size:  1024 |tokens per sec: 6488.89606353926 |TFLOPS: 2.7822392931585624 | load balancing loss: 1.157227E-01 | lm loss: 4.250951E+00 | loss scale: 1.0 | grad norm: 0.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      602/   63312 | consumed samples:       616448 | elapsed time per iteration (ms): 22526.9 | learning rate: 9.508E-05 | global batch size:  1024 |tokens per sec: 46547.63071523954 |TFLOPS: 19.95819410747914 | load balancing loss: 1.162109E-01 | lm loss: 4.212331E+00 | loss scale: 1.0 | grad norm: 0.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      603/   63312 | consumed samples:       617472 | elapsed time per iteration (ms): 22824.2 | learning rate: 9.524E-05 | global batch size:  1024 |tokens per sec: 45941.45594375872 |TFLOPS: 19.698284987157148 | load balancing loss: 1.166992E-01 | lm loss: 4.253646E+00 | loss scale: 1.0 | grad norm: 0.809 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      604/   63312 | consumed samples:       618496 | elapsed time per iteration (ms): 23047.9 | learning rate: 9.540E-05 | global batch size:  1024 |tokens per sec: 45495.48053631756 |TFLOPS: 19.507064432810896 | load balancing loss: 1.152344E-01 | lm loss: 4.164791E+00 | loss scale: 1.0 | grad norm: 0.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      605/   63312 | consumed samples:       619520 | elapsed time per iteration (ms): 22425.4 | learning rate: 9.556E-05 | global batch size:  1024 |tokens per sec: 46758.44611593667 |TFLOPS: 20.048585275048964 | load balancing loss: 1.142578E-01 | lm loss: 4.247620E+00 | loss scale: 1.0 | grad norm: 0.774 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      606/   63312 | consumed samples:       620544 | elapsed time per iteration (ms): 22507.8 | learning rate: 9.572E-05 | global batch size:  1024 |tokens per sec: 46587.17107007524 |TFLOPS: 19.97514779695299 | load balancing loss: 1.147461E-01 | lm loss: 4.220502E+00 | loss scale: 1.0 | grad norm: 0.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      607/   63312 | consumed samples:       621568 | elapsed time per iteration (ms): 23086.1 | learning rate: 9.587E-05 | global batch size:  1024 |tokens per sec: 45420.19115072221 |TFLOPS: 19.474782657157533 | load balancing loss: 1.152344E-01 | lm loss: 4.178805E+00 | loss scale: 1.0 | grad norm: 0.719 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      608/   63312 | consumed samples:       622592 | elapsed time per iteration (ms): 22948.1 | learning rate: 9.603E-05 | global batch size:  1024 |tokens per sec: 45693.378740005835 |TFLOPS: 19.59191710311961 | load balancing loss: 1.157227E-01 | lm loss: 4.180534E+00 | loss scale: 1.0 | grad norm: 0.871 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      609/   63312 | consumed samples:       623616 | elapsed time per iteration (ms): 23160.5 | learning rate: 9.619E-05 | global batch size:  1024 |tokens per sec: 45274.32006271055 |TFLOPS: 19.412237615777958 | load balancing loss: 1.157227E-01 | lm loss: 4.167082E+00 | loss scale: 1.0 | grad norm: 0.977 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      610/   63312 | consumed samples:       624640 | elapsed time per iteration (ms): 22670.7 | learning rate: 9.635E-05 | global batch size:  1024 |tokens per sec: 46252.54993515194 |TFLOPS: 19.831672533859145 | load balancing loss: 1.152344E-01 | lm loss: 4.202111E+00 | loss scale: 1.0 | grad norm: 0.869 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      611/   63312 | consumed samples:       625664 | elapsed time per iteration (ms): 22956.0 | learning rate: 9.651E-05 | global batch size:  1024 |tokens per sec: 45677.64638252342 |TFLOPS: 19.585171551529168 | load balancing loss: 1.152344E-01 | lm loss: 4.139646E+00 | loss scale: 1.0 | grad norm: 0.953 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      612/   63312 | consumed samples:       626688 | elapsed time per iteration (ms): 23014.1 | learning rate: 9.666E-05 | global batch size:  1024 |tokens per sec: 45562.427211259615 |TFLOPS: 19.535769110424194 | load balancing loss: 1.152344E-01 | lm loss: 4.171616E+00 | loss scale: 1.0 | grad norm: 1.071 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      613/   63312 | consumed samples:       627712 | elapsed time per iteration (ms): 22744.4 | learning rate: 9.682E-05 | global batch size:  1024 |tokens per sec: 46102.671588062585 |TFLOPS: 19.767409302890048 | load balancing loss: 1.147461E-01 | lm loss: 4.166123E+00 | loss scale: 1.0 | grad norm: 0.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      614/   63312 | consumed samples:       628736 | elapsed time per iteration (ms): 23050.4 | learning rate: 9.698E-05 | global batch size:  1024 |tokens per sec: 45490.66888577691 |TFLOPS: 19.505001344872966 | load balancing loss: 1.152344E-01 | lm loss: 4.188468E+00 | loss scale: 1.0 | grad norm: 0.828 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      615/   63312 | consumed samples:       629760 | elapsed time per iteration (ms): 22729.5 | learning rate: 9.714E-05 | global batch size:  1024 |tokens per sec: 46132.780771372214 |TFLOPS: 19.78031919573911 | load balancing loss: 1.142578E-01 | lm loss: 4.203245E+00 | loss scale: 1.0 | grad norm: 0.801 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      616/   63312 | consumed samples:       630784 | elapsed time per iteration (ms): 22586.6 | learning rate: 9.730E-05 | global batch size:  1024 |tokens per sec: 46424.701014354214 |TFLOPS: 19.905485628139925 | load balancing loss: 1.152344E-01 | lm loss: 4.152363E+00 | loss scale: 1.0 | grad norm: 0.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      617/   63312 | consumed samples:       631808 | elapsed time per iteration (ms): 22572.0 | learning rate: 9.745E-05 | global batch size:  1024 |tokens per sec: 46454.748616441066 |TFLOPS: 19.918369116852432 | load balancing loss: 1.147461E-01 | lm loss: 4.112917E+00 | loss scale: 1.0 | grad norm: 0.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      618/   63312 | consumed samples:       632832 | elapsed time per iteration (ms): 22937.0 | learning rate: 9.761E-05 | global batch size:  1024 |tokens per sec: 45715.505223637665 |TFLOPS: 19.601404259575364 | load balancing loss: 1.152344E-01 | lm loss: 4.113540E+00 | loss scale: 1.0 | grad norm: 0.782 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      619/   63312 | consumed samples:       633856 | elapsed time per iteration (ms): 22822.6 | learning rate: 9.777E-05 | global batch size:  1024 |tokens per sec: 45944.6950093129 |TFLOPS: 19.69967379896268 | load balancing loss: 1.152344E-01 | lm loss: 4.132449E+00 | loss scale: 1.0 | grad norm: 0.869 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      620/   63312 | consumed samples:       634880 | elapsed time per iteration (ms): 22744.6 | learning rate: 9.793E-05 | global batch size:  1024 |tokens per sec: 46102.169956336526 |TFLOPS: 19.767194218616044 | load balancing loss: 1.152344E-01 | lm loss: 4.109168E+00 | loss scale: 1.0 | grad norm: 0.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     620 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     620 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (51858.80, 51858.81)
 iteration      621/   63312 | consumed samples:       635904 | elapsed time per iteration (ms): 74539.8 | learning rate: 9.809E-05 | global batch size:  1024 |tokens per sec: 14067.330351594075 |TFLOPS: 6.031639106375137 | load balancing loss: 1.142578E-01 | lm loss: 4.076524E+00 | loss scale: 1.0 | grad norm: 0.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      622/   63312 | consumed samples:       636928 | elapsed time per iteration (ms): 22360.0 | learning rate: 9.824E-05 | global batch size:  1024 |tokens per sec: 46895.128663859105 |TFLOPS: 20.10719055271023 | load balancing loss: 1.157227E-01 | lm loss: 4.104235E+00 | loss scale: 1.0 | grad norm: 0.940 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      623/   63312 | consumed samples:       637952 | elapsed time per iteration (ms): 23112.2 | learning rate: 9.840E-05 | global batch size:  1024 |tokens per sec: 45368.978618449575 |TFLOPS: 19.452824296568018 | load balancing loss: 1.157227E-01 | lm loss: 4.114968E+00 | loss scale: 1.0 | grad norm: 0.930 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      624/   63312 | consumed samples:       638976 | elapsed time per iteration (ms): 22425.4 | learning rate: 9.856E-05 | global batch size:  1024 |tokens per sec: 46758.441641866506 |TFLOPS: 20.048583356705123 | load balancing loss: 1.157227E-01 | lm loss: 4.157552E+00 | loss scale: 1.0 | grad norm: 0.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      625/   63312 | consumed samples:       640000 | elapsed time per iteration (ms): 22618.2 | learning rate: 9.872E-05 | global batch size:  1024 |tokens per sec: 46359.77485519628 |TFLOPS: 19.87764728562461 | load balancing loss: 1.162109E-01 | lm loss: 4.145808E+00 | loss scale: 1.0 | grad norm: 1.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      626/   63312 | consumed samples:       641024 | elapsed time per iteration (ms): 22734.2 | learning rate: 9.888E-05 | global batch size:  1024 |tokens per sec: 46123.32625225761 |TFLOPS: 19.776265388385593 | load balancing loss: 1.152344E-01 | lm loss: 4.070743E+00 | loss scale: 1.0 | grad norm: 1.024 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      627/   63312 | consumed samples:       642048 | elapsed time per iteration (ms): 22521.0 | learning rate: 9.903E-05 | global batch size:  1024 |tokens per sec: 46559.83331696664 |TFLOPS: 19.963426208235795 | load balancing loss: 1.152344E-01 | lm loss: 4.099315E+00 | loss scale: 1.0 | grad norm: 0.856 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      628/   63312 | consumed samples:       643072 | elapsed time per iteration (ms): 23149.1 | learning rate: 9.919E-05 | global batch size:  1024 |tokens per sec: 45296.68392235102 |TFLOPS: 19.42182655177402 | load balancing loss: 1.147461E-01 | lm loss: 4.109966E+00 | loss scale: 1.0 | grad norm: 0.873 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      629/   63312 | consumed samples:       644096 | elapsed time per iteration (ms): 23159.2 | learning rate: 9.935E-05 | global batch size:  1024 |tokens per sec: 45276.89427163266 |TFLOPS: 19.413341357484057 | load balancing loss: 1.157227E-01 | lm loss: 4.063898E+00 | loss scale: 1.0 | grad norm: 0.885 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      630/   63312 | consumed samples:       645120 | elapsed time per iteration (ms): 22687.4 | learning rate: 9.951E-05 | global batch size:  1024 |tokens per sec: 46218.34489174518 |TFLOPS: 19.817006462025258 | load balancing loss: 1.147461E-01 | lm loss: 4.094857E+00 | loss scale: 1.0 | grad norm: 0.745 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      631/   63312 | consumed samples:       646144 | elapsed time per iteration (ms): 22590.3 | learning rate: 9.967E-05 | global batch size:  1024 |tokens per sec: 46417.1540356914 |TFLOPS: 19.902249715532495 | load balancing loss: 1.152344E-01 | lm loss: 4.040190E+00 | loss scale: 1.0 | grad norm: 0.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      632/   63312 | consumed samples:       647168 | elapsed time per iteration (ms): 22496.4 | learning rate: 9.982E-05 | global batch size:  1024 |tokens per sec: 46610.93505246096 |TFLOPS: 19.985337062570448 | load balancing loss: 1.147461E-01 | lm loss: 4.049982E+00 | loss scale: 1.0 | grad norm: 0.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      633/   63312 | consumed samples:       648192 | elapsed time per iteration (ms): 22251.7 | learning rate: 9.998E-05 | global batch size:  1024 |tokens per sec: 47123.44579432248 |TFLOPS: 20.205085924350676 | load balancing loss: 1.142578E-01 | lm loss: 4.023292E+00 | loss scale: 1.0 | grad norm: 0.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      634/   63312 | consumed samples:       649216 | elapsed time per iteration (ms): 22690.5 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46212.20062015109 |TFLOPS: 19.814371987117735 | load balancing loss: 1.152344E-01 | lm loss: 4.007483E+00 | loss scale: 1.0 | grad norm: 0.769 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      635/   63312 | consumed samples:       650240 | elapsed time per iteration (ms): 22544.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46510.648579736015 |TFLOPS: 19.94233730386643 | load balancing loss: 1.157227E-01 | lm loss: 4.053510E+00 | loss scale: 1.0 | grad norm: 0.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      636/   63312 | consumed samples:       651264 | elapsed time per iteration (ms): 22431.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46746.37073597599 |TFLOPS: 20.043407723077493 | load balancing loss: 1.152344E-01 | lm loss: 4.005818E+00 | loss scale: 1.0 | grad norm: 0.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      637/   63312 | consumed samples:       652288 | elapsed time per iteration (ms): 22867.0 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45855.395477219026 |TFLOPS: 19.66138490288246 | load balancing loss: 1.152344E-01 | lm loss: 4.028332E+00 | loss scale: 1.0 | grad norm: 0.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      638/   63312 | consumed samples:       653312 | elapsed time per iteration (ms): 22642.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46309.8798536042 |TFLOPS: 19.85625384171638 | load balancing loss: 1.152344E-01 | lm loss: 3.985250E+00 | loss scale: 1.0 | grad norm: 0.744 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      639/   63312 | consumed samples:       654336 | elapsed time per iteration (ms): 22863.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45861.610218350885 |TFLOPS: 19.66404959296295 | load balancing loss: 1.152344E-01 | lm loss: 4.014921E+00 | loss scale: 1.0 | grad norm: 0.698 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      640/   63312 | consumed samples:       655360 | elapsed time per iteration (ms): 22408.5 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46793.59845131212 |TFLOPS: 20.063657516578214 | load balancing loss: 1.142578E-01 | lm loss: 3.965795E+00 | loss scale: 1.0 | grad norm: 0.726 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     640 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     640 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (57802.48, 57802.50)
 iteration      641/   63312 | consumed samples:       656384 | elapsed time per iteration (ms): 80108.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 13089.39690957012 |TFLOPS: 5.612331288550605 | load balancing loss: 1.142578E-01 | lm loss: 4.013176E+00 | loss scale: 1.0 | grad norm: 0.831 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      642/   63312 | consumed samples:       657408 | elapsed time per iteration (ms): 22317.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46984.1340178444 |TFLOPS: 20.145353314254653 | load balancing loss: 1.142578E-01 | lm loss: 3.986003E+00 | loss scale: 1.0 | grad norm: 0.872 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      643/   63312 | consumed samples:       658432 | elapsed time per iteration (ms): 22492.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46619.64715919008 |TFLOPS: 19.989072546299976 | load balancing loss: 1.147461E-01 | lm loss: 3.997971E+00 | loss scale: 1.0 | grad norm: 0.796 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      644/   63312 | consumed samples:       659456 | elapsed time per iteration (ms): 22373.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46866.15186804183 |TFLOPS: 20.09476619283095 | load balancing loss: 1.142578E-01 | lm loss: 4.013331E+00 | loss scale: 1.0 | grad norm: 0.907 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      645/   63312 | consumed samples:       660480 | elapsed time per iteration (ms): 22740.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46111.14733484948 |TFLOPS: 19.77104344273733 | load balancing loss: 1.142578E-01 | lm loss: 3.948124E+00 | loss scale: 1.0 | grad norm: 0.862 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      646/   63312 | consumed samples:       661504 | elapsed time per iteration (ms): 23015.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45560.01063329154 |TFLOPS: 19.53473295602876 | load balancing loss: 1.147461E-01 | lm loss: 3.993980E+00 | loss scale: 1.0 | grad norm: 0.756 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      647/   63312 | consumed samples:       662528 | elapsed time per iteration (ms): 22363.0 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46888.802143906505 |TFLOPS: 20.104477935305244 | load balancing loss: 1.152344E-01 | lm loss: 3.959633E+00 | loss scale: 1.0 | grad norm: 0.867 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      648/   63312 | consumed samples:       663552 | elapsed time per iteration (ms): 23072.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45447.78549640925 |TFLOPS: 19.486614264889834 | load balancing loss: 1.152344E-01 | lm loss: 3.965224E+00 | loss scale: 1.0 | grad norm: 0.944 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      649/   63312 | consumed samples:       664576 | elapsed time per iteration (ms): 22674.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46244.571103057315 |TFLOPS: 19.82825145576669 | load balancing loss: 1.152344E-01 | lm loss: 4.019177E+00 | loss scale: 1.0 | grad norm: 1.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      650/   63312 | consumed samples:       665600 | elapsed time per iteration (ms): 22705.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46181.324328231334 |TFLOPS: 19.801133181662234 | load balancing loss: 1.152344E-01 | lm loss: 3.907553E+00 | loss scale: 1.0 | grad norm: 0.891 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      651/   63312 | consumed samples:       666624 | elapsed time per iteration (ms): 22813.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45963.48041642013 |TFLOPS: 19.70772840444256 | load balancing loss: 1.147461E-01 | lm loss: 3.955724E+00 | loss scale: 1.0 | grad norm: 0.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      652/   63312 | consumed samples:       667648 | elapsed time per iteration (ms): 22801.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45986.834845623365 |TFLOPS: 19.71774206623684 | load balancing loss: 1.147461E-01 | lm loss: 3.991221E+00 | loss scale: 1.0 | grad norm: 0.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      653/   63312 | consumed samples:       668672 | elapsed time per iteration (ms): 23069.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45453.65535302906 |TFLOPS: 19.489131079086416 | load balancing loss: 1.147461E-01 | lm loss: 3.958867E+00 | loss scale: 1.0 | grad norm: 0.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      654/   63312 | consumed samples:       669696 | elapsed time per iteration (ms): 22766.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46058.299630429516 |TFLOPS: 19.748383970563527 | load balancing loss: 1.157227E-01 | lm loss: 3.920694E+00 | loss scale: 1.0 | grad norm: 0.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      655/   63312 | consumed samples:       670720 | elapsed time per iteration (ms): 22458.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46690.21929875323 |TFLOPS: 20.01933171176853 | load balancing loss: 1.162109E-01 | lm loss: 3.931829E+00 | loss scale: 1.0 | grad norm: 0.741 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      656/   63312 | consumed samples:       671744 | elapsed time per iteration (ms): 22490.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46622.770041321986 |TFLOPS: 19.9904115422232 | load balancing loss: 1.147461E-01 | lm loss: 3.918832E+00 | loss scale: 1.0 | grad norm: 0.652 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      657/   63312 | consumed samples:       672768 | elapsed time per iteration (ms): 22411.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46787.61137476293 |TFLOPS: 20.061090442076914 | load balancing loss: 1.152344E-01 | lm loss: 3.941625E+00 | loss scale: 1.0 | grad norm: 0.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      658/   63312 | consumed samples:       673792 | elapsed time per iteration (ms): 22362.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46889.11158071513 |TFLOPS: 20.10461061230289 | load balancing loss: 1.142578E-01 | lm loss: 3.944755E+00 | loss scale: 1.0 | grad norm: 0.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      659/   63312 | consumed samples:       674816 | elapsed time per iteration (ms): 22601.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46394.58018195133 |TFLOPS: 19.89257074051754 | load balancing loss: 1.142578E-01 | lm loss: 3.911469E+00 | loss scale: 1.0 | grad norm: 0.733 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      660/   63312 | consumed samples:       675840 | elapsed time per iteration (ms): 22622.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46351.804420977394 |TFLOPS: 19.874229808283292 | load balancing loss: 1.142578E-01 | lm loss: 3.863171E+00 | loss scale: 1.0 | grad norm: 0.707 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     660 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     660 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (53341.25, 53341.29)
 iteration      661/   63312 | consumed samples:       676864 | elapsed time per iteration (ms): 75751.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 13842.356591984522 |TFLOPS: 5.935177269448458 | load balancing loss: 1.147461E-01 | lm loss: 3.841240E+00 | loss scale: 1.0 | grad norm: 0.786 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      662/   63312 | consumed samples:       677888 | elapsed time per iteration (ms): 22725.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46141.3300403446 |TFLOPS: 19.783984859640924 | load balancing loss: 1.152344E-01 | lm loss: 3.860975E+00 | loss scale: 1.0 | grad norm: 0.766 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      663/   63312 | consumed samples:       678912 | elapsed time per iteration (ms): 22989.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45610.76630756917 |TFLOPS: 19.556495429944686 | load balancing loss: 1.147461E-01 | lm loss: 3.871604E+00 | loss scale: 1.0 | grad norm: 0.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      664/   63312 | consumed samples:       679936 | elapsed time per iteration (ms): 22640.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46314.5332552629 |TFLOPS: 19.858249077395126 | load balancing loss: 1.152344E-01 | lm loss: 3.874452E+00 | loss scale: 1.0 | grad norm: 1.197 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      665/   63312 | consumed samples:       680960 | elapsed time per iteration (ms): 22426.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46755.831913520014 |TFLOPS: 20.047464385361 | load balancing loss: 1.147461E-01 | lm loss: 3.843041E+00 | loss scale: 1.0 | grad norm: 0.742 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      666/   63312 | consumed samples:       681984 | elapsed time per iteration (ms): 22165.5 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47306.71368644693 |TFLOPS: 20.283665566503995 | load balancing loss: 1.142578E-01 | lm loss: 3.934955E+00 | loss scale: 1.0 | grad norm: 0.740 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      667/   63312 | consumed samples:       683008 | elapsed time per iteration (ms): 22696.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46200.016466108405 |TFLOPS: 19.809147796161447 | load balancing loss: 1.147461E-01 | lm loss: 3.861813E+00 | loss scale: 1.0 | grad norm: 0.867 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      668/   63312 | consumed samples:       684032 | elapsed time per iteration (ms): 22735.0 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46121.728626876386 |TFLOPS: 19.775580375701175 | load balancing loss: 1.152344E-01 | lm loss: 3.822473E+00 | loss scale: 1.0 | grad norm: 0.797 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      669/   63312 | consumed samples:       685056 | elapsed time per iteration (ms): 22749.5 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46092.23719044983 |TFLOPS: 19.7629353537384 | load balancing loss: 1.157227E-01 | lm loss: 3.807482E+00 | loss scale: 1.0 | grad norm: 0.738 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      670/   63312 | consumed samples:       686080 | elapsed time per iteration (ms): 22927.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45734.760423403735 |TFLOPS: 19.609660297715315 | load balancing loss: 1.157227E-01 | lm loss: 3.857504E+00 | loss scale: 1.0 | grad norm: 0.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      671/   63312 | consumed samples:       687104 | elapsed time per iteration (ms): 22918.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45752.90235121034 |TFLOPS: 19.617438999038765 | load balancing loss: 1.142578E-01 | lm loss: 3.872684E+00 | loss scale: 1.0 | grad norm: 0.886 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      672/   63312 | consumed samples:       688128 | elapsed time per iteration (ms): 22302.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47015.492896042706 |TFLOPS: 20.158799037881344 | load balancing loss: 1.147461E-01 | lm loss: 3.864494E+00 | loss scale: 1.0 | grad norm: 0.884 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      673/   63312 | consumed samples:       689152 | elapsed time per iteration (ms): 22633.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46328.763092769754 |TFLOPS: 19.86435039457746 | load balancing loss: 1.147461E-01 | lm loss: 3.850204E+00 | loss scale: 1.0 | grad norm: 0.925 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      674/   63312 | consumed samples:       690176 | elapsed time per iteration (ms): 22928.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45732.730694037804 |TFLOPS: 19.60879001211649 | load balancing loss: 1.147461E-01 | lm loss: 3.836127E+00 | loss scale: 1.0 | grad norm: 0.890 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      675/   63312 | consumed samples:       691200 | elapsed time per iteration (ms): 22371.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46871.96524264186 |TFLOPS: 20.097258789272576 | load balancing loss: 1.147461E-01 | lm loss: 3.785986E+00 | loss scale: 1.0 | grad norm: 0.846 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      676/   63312 | consumed samples:       692224 | elapsed time per iteration (ms): 22371.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46871.82936933256 |TFLOPS: 20.097200530971577 | load balancing loss: 1.142578E-01 | lm loss: 3.873355E+00 | loss scale: 1.0 | grad norm: 0.983 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      677/   63312 | consumed samples:       693248 | elapsed time per iteration (ms): 22184.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47265.62554753338 |TFLOPS: 20.266048234765496 | load balancing loss: 1.142578E-01 | lm loss: 3.861028E+00 | loss scale: 1.0 | grad norm: 0.855 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      678/   63312 | consumed samples:       694272 | elapsed time per iteration (ms): 22949.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45689.70699810351 |TFLOPS: 19.590342772987803 | load balancing loss: 1.147461E-01 | lm loss: 3.783067E+00 | loss scale: 1.0 | grad norm: 0.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      679/   63312 | consumed samples:       695296 | elapsed time per iteration (ms): 22744.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46102.99006704916 |TFLOPS: 19.76754585689582 | load balancing loss: 1.142578E-01 | lm loss: 3.867109E+00 | loss scale: 1.0 | grad norm: 0.804 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      680/   63312 | consumed samples:       696320 | elapsed time per iteration (ms): 22456.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46694.40014785767 |TFLOPS: 20.021124331428826 | load balancing loss: 1.147461E-01 | lm loss: 3.780665E+00 | loss scale: 1.0 | grad norm: 0.683 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     680 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     680 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (54417.79, 54417.83)
 iteration      681/   63312 | consumed samples:       697344 | elapsed time per iteration (ms): 77195.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 13583.354990455324 |TFLOPS: 5.824125339241936 | load balancing loss: 1.157227E-01 | lm loss: 3.779303E+00 | loss scale: 1.0 | grad norm: 0.670 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      682/   63312 | consumed samples:       698368 | elapsed time per iteration (ms): 22472.0 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46661.42727864819 |TFLOPS: 20.006986578038344 | load balancing loss: 1.142578E-01 | lm loss: 3.777691E+00 | loss scale: 1.0 | grad norm: 0.744 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      683/   63312 | consumed samples:       699392 | elapsed time per iteration (ms): 22325.0 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46968.71529089763 |TFLOPS: 20.138742237803168 | load balancing loss: 1.142578E-01 | lm loss: 3.733360E+00 | loss scale: 1.0 | grad norm: 0.607 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      684/   63312 | consumed samples:       700416 | elapsed time per iteration (ms): 22408.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46793.45606187225 |TFLOPS: 20.0635964643606 | load balancing loss: 1.147461E-01 | lm loss: 3.772974E+00 | loss scale: 1.0 | grad norm: 0.615 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      685/   63312 | consumed samples:       701440 | elapsed time per iteration (ms): 22603.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46390.18274188691 |TFLOPS: 19.890685253307275 | load balancing loss: 1.142578E-01 | lm loss: 3.742596E+00 | loss scale: 1.0 | grad norm: 0.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      686/   63312 | consumed samples:       702464 | elapsed time per iteration (ms): 22319.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46981.04886041503 |TFLOPS: 20.144030493524973 | load balancing loss: 1.147461E-01 | lm loss: 3.771640E+00 | loss scale: 1.0 | grad norm: 0.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      687/   63312 | consumed samples:       703488 | elapsed time per iteration (ms): 22262.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47100.88307121669 |TFLOPS: 20.19541172180977 | load balancing loss: 1.147461E-01 | lm loss: 3.777657E+00 | loss scale: 1.0 | grad norm: 0.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      688/   63312 | consumed samples:       704512 | elapsed time per iteration (ms): 22813.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45962.206059140444 |TFLOPS: 19.707181999189288 | load balancing loss: 1.142578E-01 | lm loss: 3.807980E+00 | loss scale: 1.0 | grad norm: 0.590 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      689/   63312 | consumed samples:       705536 | elapsed time per iteration (ms): 22609.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46376.87903042768 |TFLOPS: 19.884981030523573 | load balancing loss: 1.147461E-01 | lm loss: 3.728894E+00 | loss scale: 1.0 | grad norm: 0.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      690/   63312 | consumed samples:       706560 | elapsed time per iteration (ms): 22530.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46539.799939854456 |TFLOPS: 19.95483651155544 | load balancing loss: 1.157227E-01 | lm loss: 3.754661E+00 | loss scale: 1.0 | grad norm: 0.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      691/   63312 | consumed samples:       707584 | elapsed time per iteration (ms): 22685.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46221.76642257075 |TFLOPS: 19.818473509333835 | load balancing loss: 1.147461E-01 | lm loss: 3.757484E+00 | loss scale: 1.0 | grad norm: 0.536 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      692/   63312 | consumed samples:       708608 | elapsed time per iteration (ms): 22946.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45695.95429571819 |TFLOPS: 19.593021422287993 | load balancing loss: 1.142578E-01 | lm loss: 3.728971E+00 | loss scale: 1.0 | grad norm: 0.511 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      693/   63312 | consumed samples:       709632 | elapsed time per iteration (ms): 22674.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46243.83735993591 |TFLOPS: 19.827936849256822 | load balancing loss: 1.142578E-01 | lm loss: 3.707631E+00 | loss scale: 1.0 | grad norm: 0.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      694/   63312 | consumed samples:       710656 | elapsed time per iteration (ms): 22445.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46715.987497115464 |TFLOPS: 20.0303803236272 | load balancing loss: 1.142578E-01 | lm loss: 3.696016E+00 | loss scale: 1.0 | grad norm: 0.524 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      695/   63312 | consumed samples:       711680 | elapsed time per iteration (ms): 22731.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46129.33901713221 |TFLOPS: 19.778843477251687 | load balancing loss: 1.142578E-01 | lm loss: 3.697716E+00 | loss scale: 1.0 | grad norm: 0.596 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      696/   63312 | consumed samples:       712704 | elapsed time per iteration (ms): 22701.0 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46190.77984129654 |TFLOPS: 19.80518741519123 | load balancing loss: 1.142578E-01 | lm loss: 3.704922E+00 | loss scale: 1.0 | grad norm: 0.498 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      697/   63312 | consumed samples:       713728 | elapsed time per iteration (ms): 22535.5 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46529.90958252935 |TFLOPS: 19.950595830166208 | load balancing loss: 1.147461E-01 | lm loss: 3.682813E+00 | loss scale: 1.0 | grad norm: 0.537 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      698/   63312 | consumed samples:       714752 | elapsed time per iteration (ms): 22684.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46224.222620732886 |TFLOPS: 19.81952665165135 | load balancing loss: 1.142578E-01 | lm loss: 3.735437E+00 | loss scale: 1.0 | grad norm: 0.533 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      699/   63312 | consumed samples:       715776 | elapsed time per iteration (ms): 22918.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45753.03752643594 |TFLOPS: 19.617496958022915 | load balancing loss: 1.142578E-01 | lm loss: 3.686590E+00 | loss scale: 1.0 | grad norm: 0.559 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      700/   63312 | consumed samples:       716800 | elapsed time per iteration (ms): 22216.0 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47199.050490322974 |TFLOPS: 20.237502895419432 | load balancing loss: 1.142578E-01 | lm loss: 3.726401E+00 | loss scale: 1.0 | grad norm: 0.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     700 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
-----------------------------------------------------------------------------------------------
 validation loss at iteration 700 | lm loss value: 3.256510E+00 | lm loss PPL: 2.595877E+01 | 
-----------------------------------------------------------------------------------------------
  successfully saved checkpoint at iteration     700 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (55373.12, 55373.17)
 iteration      701/   63312 | consumed samples:       717824 | elapsed time per iteration (ms): 169175.5 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 6198.153435024206 |TFLOPS: 2.6575777856648557 | load balancing loss: 1.142578E-01 | lm loss: 3.745817E+00 | loss scale: 1.0 | grad norm: 0.504 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      702/   63312 | consumed samples:       718848 | elapsed time per iteration (ms): 22786.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46018.11551347706 |TFLOPS: 19.73115425567039 | load balancing loss: 1.142578E-01 | lm loss: 3.679742E+00 | loss scale: 1.0 | grad norm: 0.583 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      703/   63312 | consumed samples:       719872 | elapsed time per iteration (ms): 22860.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45867.61377424708 |TFLOPS: 19.666623733302004 | load balancing loss: 1.147461E-01 | lm loss: 3.679006E+00 | loss scale: 1.0 | grad norm: 0.560 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      704/   63312 | consumed samples:       720896 | elapsed time per iteration (ms): 22530.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46539.96787637557 |TFLOPS: 19.954908517576683 | load balancing loss: 1.142578E-01 | lm loss: 3.754755E+00 | loss scale: 1.0 | grad norm: 0.666 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      705/   63312 | consumed samples:       721920 | elapsed time per iteration (ms): 22439.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46729.94130358561 |TFLOPS: 20.036363287180663 | load balancing loss: 1.142578E-01 | lm loss: 3.691696E+00 | loss scale: 1.0 | grad norm: 0.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      706/   63312 | consumed samples:       722944 | elapsed time per iteration (ms): 22521.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46559.24381033265 |TFLOPS: 19.963173445900786 | load balancing loss: 1.142578E-01 | lm loss: 3.640567E+00 | loss scale: 1.0 | grad norm: 0.699 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      707/   63312 | consumed samples:       723968 | elapsed time per iteration (ms): 22200.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47232.313544670964 |TFLOPS: 20.25176507128282 | load balancing loss: 1.142578E-01 | lm loss: 3.651130E+00 | loss scale: 1.0 | grad norm: 0.736 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      708/   63312 | consumed samples:       724992 | elapsed time per iteration (ms): 22472.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46660.85054302502 |TFLOPS: 20.00673929152045 | load balancing loss: 1.142578E-01 | lm loss: 3.647967E+00 | loss scale: 1.0 | grad norm: 0.706 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      709/   63312 | consumed samples:       726016 | elapsed time per iteration (ms): 22227.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47174.05271662875 |TFLOPS: 20.226784617990955 | load balancing loss: 1.142578E-01 | lm loss: 3.669367E+00 | loss scale: 1.0 | grad norm: 0.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      710/   63312 | consumed samples:       727040 | elapsed time per iteration (ms): 22674.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46245.61365155237 |TFLOPS: 19.82869846853428 | load balancing loss: 1.142578E-01 | lm loss: 3.660411E+00 | loss scale: 1.0 | grad norm: 0.851 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      711/   63312 | consumed samples:       728064 | elapsed time per iteration (ms): 22142.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47354.90100980405 |TFLOPS: 20.304326810444955 | load balancing loss: 1.142578E-01 | lm loss: 3.684180E+00 | loss scale: 1.0 | grad norm: 0.663 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      712/   63312 | consumed samples:       729088 | elapsed time per iteration (ms): 22520.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46560.11131691352 |TFLOPS: 19.96354540607286 | load balancing loss: 1.142578E-01 | lm loss: 3.690575E+00 | loss scale: 1.0 | grad norm: 0.874 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      713/   63312 | consumed samples:       730112 | elapsed time per iteration (ms): 22900.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45787.87222753488 |TFLOPS: 19.632433007732125 | load balancing loss: 1.152344E-01 | lm loss: 3.677920E+00 | loss scale: 1.0 | grad norm: 0.806 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      714/   63312 | consumed samples:       731136 | elapsed time per iteration (ms): 22628.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46339.023638921215 |TFLOPS: 19.868749801563226 | load balancing loss: 1.147461E-01 | lm loss: 3.654649E+00 | loss scale: 1.0 | grad norm: 0.722 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      715/   63312 | consumed samples:       732160 | elapsed time per iteration (ms): 22465.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46674.053191918036 |TFLOPS: 20.012400181780396 | load balancing loss: 1.132812E-01 | lm loss: 3.680868E+00 | loss scale: 1.0 | grad norm: 0.660 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      716/   63312 | consumed samples:       733184 | elapsed time per iteration (ms): 22407.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46796.29754231168 |TFLOPS: 20.06481480388275 | load balancing loss: 1.142578E-01 | lm loss: 3.682914E+00 | loss scale: 1.0 | grad norm: 0.624 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      717/   63312 | consumed samples:       734208 | elapsed time per iteration (ms): 22783.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46023.60047173796 |TFLOPS: 19.733506037274747 | load balancing loss: 1.132812E-01 | lm loss: 3.685493E+00 | loss scale: 1.0 | grad norm: 0.625 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      718/   63312 | consumed samples:       735232 | elapsed time per iteration (ms): 22430.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46747.481249430675 |TFLOPS: 20.043883877131893 | load balancing loss: 1.142578E-01 | lm loss: 3.682272E+00 | loss scale: 1.0 | grad norm: 0.739 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      719/   63312 | consumed samples:       736256 | elapsed time per iteration (ms): 22322.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46973.8376872839 |TFLOPS: 20.14093856401358 | load balancing loss: 1.142578E-01 | lm loss: 3.667938E+00 | loss scale: 1.0 | grad norm: 0.751 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      720/   63312 | consumed samples:       737280 | elapsed time per iteration (ms): 22277.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47067.985144610575 |TFLOPS: 20.181306101505374 | load balancing loss: 1.142578E-01 | lm loss: 3.622786E+00 | loss scale: 1.0 | grad norm: 0.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     720 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     720 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (58843.39, 58843.43)
 iteration      721/   63312 | consumed samples:       738304 | elapsed time per iteration (ms): 81042.5 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 12938.59455359888 |TFLOPS: 5.547671870958513 | load balancing loss: 1.142578E-01 | lm loss: 3.654707E+00 | loss scale: 1.0 | grad norm: 0.634 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      722/   63312 | consumed samples:       739328 | elapsed time per iteration (ms): 22211.0 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47209.81471904446 |TFLOPS: 20.242118266018096 | load balancing loss: 1.142578E-01 | lm loss: 3.627215E+00 | loss scale: 1.0 | grad norm: 0.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      723/   63312 | consumed samples:       740352 | elapsed time per iteration (ms): 22349.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46916.40506820253 |TFLOPS: 20.116313221281555 | load balancing loss: 1.142578E-01 | lm loss: 3.607660E+00 | loss scale: 1.0 | grad norm: 0.500 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      724/   63312 | consumed samples:       741376 | elapsed time per iteration (ms): 22732.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46127.24992489718 |TFLOPS: 19.777947738678215 | load balancing loss: 1.142578E-01 | lm loss: 3.664694E+00 | loss scale: 1.0 | grad norm: 0.542 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      725/   63312 | consumed samples:       742400 | elapsed time per iteration (ms): 22596.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46403.90094946252 |TFLOPS: 19.89656719929236 | load balancing loss: 1.142578E-01 | lm loss: 3.629084E+00 | loss scale: 1.0 | grad norm: 0.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      726/   63312 | consumed samples:       743424 | elapsed time per iteration (ms): 22640.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46313.9894485885 |TFLOPS: 19.85801590979889 | load balancing loss: 1.142578E-01 | lm loss: 3.594601E+00 | loss scale: 1.0 | grad norm: 0.507 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      727/   63312 | consumed samples:       744448 | elapsed time per iteration (ms): 22692.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46208.2221871234 |TFLOPS: 19.812666157252924 | load balancing loss: 1.142578E-01 | lm loss: 3.602790E+00 | loss scale: 1.0 | grad norm: 0.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      728/   63312 | consumed samples:       745472 | elapsed time per iteration (ms): 22874.0 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 45841.390394733455 |TFLOPS: 19.655379953748778 | load balancing loss: 1.137695E-01 | lm loss: 3.562656E+00 | loss scale: 1.0 | grad norm: 0.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      729/   63312 | consumed samples:       746496 | elapsed time per iteration (ms): 22175.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47284.93597973362 |TFLOPS: 20.27432795487648 | load balancing loss: 1.147461E-01 | lm loss: 3.558191E+00 | loss scale: 1.0 | grad norm: 0.541 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      730/   63312 | consumed samples:       747520 | elapsed time per iteration (ms): 22453.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46700.72638191489 |TFLOPS: 20.02383682625065 | load balancing loss: 1.142578E-01 | lm loss: 3.610870E+00 | loss scale: 1.0 | grad norm: 0.628 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      731/   63312 | consumed samples:       748544 | elapsed time per iteration (ms): 22332.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46952.21944897936 |TFLOPS: 20.131669327540877 | load balancing loss: 1.147461E-01 | lm loss: 3.615422E+00 | loss scale: 1.0 | grad norm: 0.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      732/   63312 | consumed samples:       749568 | elapsed time per iteration (ms): 22389.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46833.76992498934 |TFLOPS: 20.080881810423335 | load balancing loss: 1.147461E-01 | lm loss: 3.628194E+00 | loss scale: 1.0 | grad norm: 0.713 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      733/   63312 | consumed samples:       750592 | elapsed time per iteration (ms): 22226.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47176.71187227694 |TFLOPS: 20.227924782243633 | load balancing loss: 1.147461E-01 | lm loss: 3.589794E+00 | loss scale: 1.0 | grad norm: 0.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      734/   63312 | consumed samples:       751616 | elapsed time per iteration (ms): 22234.5 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47159.85016133688 |TFLOPS: 20.2206949985843 | load balancing loss: 1.142578E-01 | lm loss: 3.584816E+00 | loss scale: 1.0 | grad norm: 0.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      735/   63312 | consumed samples:       752640 | elapsed time per iteration (ms): 22555.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46488.293784813395 |TFLOPS: 19.932752254542912 | load balancing loss: 1.142578E-01 | lm loss: 3.601682E+00 | loss scale: 1.0 | grad norm: 0.613 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      736/   63312 | consumed samples:       753664 | elapsed time per iteration (ms): 22311.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46997.82662573973 |TFLOPS: 20.151224283883256 | load balancing loss: 1.142578E-01 | lm loss: 3.602971E+00 | loss scale: 1.0 | grad norm: 0.525 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      737/   63312 | consumed samples:       754688 | elapsed time per iteration (ms): 22528.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46545.19273574171 |TFLOPS: 19.957148776765205 | load balancing loss: 1.142578E-01 | lm loss: 3.564564E+00 | loss scale: 1.0 | grad norm: 0.476 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      738/   63312 | consumed samples:       755712 | elapsed time per iteration (ms): 22737.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46117.22172393034 |TFLOPS: 19.77364795416997 | load balancing loss: 1.142578E-01 | lm loss: 3.585307E+00 | loss scale: 1.0 | grad norm: 0.509 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      739/   63312 | consumed samples:       756736 | elapsed time per iteration (ms): 22739.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46111.67671925537 |TFLOPS: 19.771270426507886 | load balancing loss: 1.147461E-01 | lm loss: 3.573700E+00 | loss scale: 1.0 | grad norm: 0.489 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      740/   63312 | consumed samples:       757760 | elapsed time per iteration (ms): 22214.0 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47203.40199953986 |TFLOPS: 20.23936868889327 | load balancing loss: 1.142578E-01 | lm loss: 3.599577E+00 | loss scale: 1.0 | grad norm: 0.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     740 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     740 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (80673.91, 80673.94)
 iteration      741/   63312 | consumed samples:       758784 | elapsed time per iteration (ms): 102829.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 10197.236893131301 |TFLOPS: 4.372261920657344 | load balancing loss: 1.132812E-01 | lm loss: 3.543102E+00 | loss scale: 1.0 | grad norm: 0.461 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      742/   63312 | consumed samples:       759808 | elapsed time per iteration (ms): 22614.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46368.05698382058 |TFLOPS: 19.881198407951718 | load balancing loss: 1.142578E-01 | lm loss: 3.546778E+00 | loss scale: 1.0 | grad norm: 0.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      743/   63312 | consumed samples:       760832 | elapsed time per iteration (ms): 22227.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47174.13620601549 |TFLOPS: 20.226820415675192 | load balancing loss: 1.142578E-01 | lm loss: 3.555403E+00 | loss scale: 1.0 | grad norm: 0.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      744/   63312 | consumed samples:       761856 | elapsed time per iteration (ms): 22219.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47191.33879096789 |TFLOPS: 20.234196355639376 | load balancing loss: 1.142578E-01 | lm loss: 3.510468E+00 | loss scale: 1.0 | grad norm: 0.466 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      745/   63312 | consumed samples:       762880 | elapsed time per iteration (ms): 22468.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46668.60573596727 |TFLOPS: 20.01006448001456 | load balancing loss: 1.142578E-01 | lm loss: 3.558887E+00 | loss scale: 1.0 | grad norm: 0.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      746/   63312 | consumed samples:       763904 | elapsed time per iteration (ms): 22244.5 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47138.684875511404 |TFLOPS: 20.211619974219914 | load balancing loss: 1.142578E-01 | lm loss: 3.567649E+00 | loss scale: 1.0 | grad norm: 0.490 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      747/   63312 | consumed samples:       764928 | elapsed time per iteration (ms): 22473.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46658.90113364548 |TFLOPS: 20.005903444664355 | load balancing loss: 1.142578E-01 | lm loss: 3.526279E+00 | loss scale: 1.0 | grad norm: 0.541 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      748/   63312 | consumed samples:       765952 | elapsed time per iteration (ms): 22452.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46701.39733397306 |TFLOPS: 20.024124509881545 | load balancing loss: 1.142578E-01 | lm loss: 3.589498E+00 | loss scale: 1.0 | grad norm: 0.533 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      749/   63312 | consumed samples:       766976 | elapsed time per iteration (ms): 22119.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47404.43073150396 |TFLOPS: 20.325563633556825 | load balancing loss: 1.142578E-01 | lm loss: 3.519374E+00 | loss scale: 1.0 | grad norm: 0.534 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      750/   63312 | consumed samples:       768000 | elapsed time per iteration (ms): 22430.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46747.1861011095 |TFLOPS: 20.04375732659873 | load balancing loss: 1.142578E-01 | lm loss: 3.524231E+00 | loss scale: 1.0 | grad norm: 0.532 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      751/   63312 | consumed samples:       769024 | elapsed time per iteration (ms): 22009.0 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47643.130845351145 |TFLOPS: 20.427910909507002 | load balancing loss: 1.142578E-01 | lm loss: 3.560360E+00 | loss scale: 1.0 | grad norm: 0.506 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      752/   63312 | consumed samples:       770048 | elapsed time per iteration (ms): 22433.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46742.25707878118 |TFLOPS: 20.04164391324391 | load balancing loss: 1.142578E-01 | lm loss: 3.517491E+00 | loss scale: 1.0 | grad norm: 0.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      753/   63312 | consumed samples:       771072 | elapsed time per iteration (ms): 22441.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46724.69772137703 |TFLOPS: 20.03411500021247 | load balancing loss: 1.142578E-01 | lm loss: 3.545254E+00 | loss scale: 1.0 | grad norm: 0.596 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      754/   63312 | consumed samples:       772096 | elapsed time per iteration (ms): 22163.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47311.48001141229 |TFLOPS: 20.285709220227645 | load balancing loss: 1.142578E-01 | lm loss: 3.501232E+00 | loss scale: 1.0 | grad norm: 0.619 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      755/   63312 | consumed samples:       773120 | elapsed time per iteration (ms): 22136.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47368.793261777486 |TFLOPS: 20.310283381321184 | load balancing loss: 1.142578E-01 | lm loss: 3.554173E+00 | loss scale: 1.0 | grad norm: 0.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      756/   63312 | consumed samples:       774144 | elapsed time per iteration (ms): 22394.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46822.49055587684 |TFLOPS: 20.076045563450112 | load balancing loss: 1.132812E-01 | lm loss: 3.566673E+00 | loss scale: 1.0 | grad norm: 0.552 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      757/   63312 | consumed samples:       775168 | elapsed time per iteration (ms): 22423.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46762.49003122081 |TFLOPS: 20.050319181693723 | load balancing loss: 1.142578E-01 | lm loss: 3.476691E+00 | loss scale: 1.0 | grad norm: 0.545 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      758/   63312 | consumed samples:       776192 | elapsed time per iteration (ms): 22227.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47175.65627051913 |TFLOPS: 20.227472172637974 | load balancing loss: 1.142578E-01 | lm loss: 3.525506E+00 | loss scale: 1.0 | grad norm: 0.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      759/   63312 | consumed samples:       777216 | elapsed time per iteration (ms): 22217.8 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47195.35714954561 |TFLOPS: 20.235919304353523 | load balancing loss: 1.147461E-01 | lm loss: 3.537135E+00 | loss scale: 1.0 | grad norm: 0.648 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      760/   63312 | consumed samples:       778240 | elapsed time per iteration (ms): 22416.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46776.726358342734 |TFLOPS: 20.056423281423633 | load balancing loss: 1.142578E-01 | lm loss: 3.474885E+00 | loss scale: 1.0 | grad norm: 0.633 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     760 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
  successfully saved checkpoint at iteration     760 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
(min, max) time across ranks (ms):
    save-checkpoint ................................: (64375.43, 64375.47)
 iteration      761/   63312 | consumed samples:       779264 | elapsed time per iteration (ms): 86759.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 12086.018363881389 |TFLOPS: 5.1821133919482 | load balancing loss: 1.142578E-01 | lm loss: 3.502877E+00 | loss scale: 1.0 | grad norm: 0.580 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      762/   63312 | consumed samples:       780288 | elapsed time per iteration (ms): 22547.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46505.337547714254 |TFLOPS: 19.940060096490406 | load balancing loss: 1.137695E-01 | lm loss: 3.529051E+00 | loss scale: 1.0 | grad norm: 0.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      763/   63312 | consumed samples:       781312 | elapsed time per iteration (ms): 22257.7 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47110.770401243695 |TFLOPS: 20.199651105186568 | load balancing loss: 1.142578E-01 | lm loss: 3.484496E+00 | loss scale: 1.0 | grad norm: 0.713 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      764/   63312 | consumed samples:       782336 | elapsed time per iteration (ms): 22228.4 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47172.743236915434 |TFLOPS: 20.226223153317097 | load balancing loss: 1.142578E-01 | lm loss: 3.518900E+00 | loss scale: 1.0 | grad norm: 0.547 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      765/   63312 | consumed samples:       783360 | elapsed time per iteration (ms): 21837.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 48017.94134723992 |TFLOPS: 20.588618138538287 | load balancing loss: 1.142578E-01 | lm loss: 3.517190E+00 | loss scale: 1.0 | grad norm: 0.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      766/   63312 | consumed samples:       784384 | elapsed time per iteration (ms): 22024.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47610.463997105384 |TFLOPS: 20.4139043685048 | load balancing loss: 1.132812E-01 | lm loss: 3.549499E+00 | loss scale: 1.0 | grad norm: 0.624 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      767/   63312 | consumed samples:       785408 | elapsed time per iteration (ms): 22657.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46280.26670183284 |TFLOPS: 19.843556631952843 | load balancing loss: 1.142578E-01 | lm loss: 3.541896E+00 | loss scale: 1.0 | grad norm: 0.605 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      768/   63312 | consumed samples:       786432 | elapsed time per iteration (ms): 22218.5 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47193.90063705083 |TFLOPS: 20.235294796539826 | load balancing loss: 1.142578E-01 | lm loss: 3.517220E+00 | loss scale: 1.0 | grad norm: 0.552 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      769/   63312 | consumed samples:       787456 | elapsed time per iteration (ms): 21934.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47805.5029128548 |TFLOPS: 20.497531063982652 | load balancing loss: 1.147461E-01 | lm loss: 3.456814E+00 | loss scale: 1.0 | grad norm: 0.516 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      770/   63312 | consumed samples:       788480 | elapsed time per iteration (ms): 22422.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46764.92745690208 |TFLOPS: 20.051364274948135 | load balancing loss: 1.132812E-01 | lm loss: 3.507416E+00 | loss scale: 1.0 | grad norm: 0.491 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      771/   63312 | consumed samples:       789504 | elapsed time per iteration (ms): 22287.5 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47047.64455488911 |TFLOPS: 20.172584681495724 | load balancing loss: 1.147461E-01 | lm loss: 3.465286E+00 | loss scale: 1.0 | grad norm: 0.456 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      772/   63312 | consumed samples:       790528 | elapsed time per iteration (ms): 22266.9 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47091.177374956525 |TFLOPS: 20.1912502174124 | load balancing loss: 1.142578E-01 | lm loss: 3.496668E+00 | loss scale: 1.0 | grad norm: 0.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      773/   63312 | consumed samples:       791552 | elapsed time per iteration (ms): 22229.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47170.8115328819 |TFLOPS: 20.22539489796947 | load balancing loss: 1.142578E-01 | lm loss: 3.472596E+00 | loss scale: 1.0 | grad norm: 0.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      774/   63312 | consumed samples:       792576 | elapsed time per iteration (ms): 22007.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47647.0066171762 |TFLOPS: 20.429572721401875 | load balancing loss: 1.142578E-01 | lm loss: 3.483293E+00 | loss scale: 1.0 | grad norm: 0.446 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      775/   63312 | consumed samples:       793600 | elapsed time per iteration (ms): 21857.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47973.9950874625 |TFLOPS: 20.56977533237483 | load balancing loss: 1.142578E-01 | lm loss: 3.472282E+00 | loss scale: 1.0 | grad norm: 0.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      776/   63312 | consumed samples:       794624 | elapsed time per iteration (ms): 22488.3 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46627.66252459151 |TFLOPS: 19.992509288752085 | load balancing loss: 1.142578E-01 | lm loss: 3.470248E+00 | loss scale: 1.0 | grad norm: 0.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      777/   63312 | consumed samples:       795648 | elapsed time per iteration (ms): 22336.5 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46944.54860437511 |TFLOPS: 20.128380305022915 | load balancing loss: 1.132812E-01 | lm loss: 3.478394E+00 | loss scale: 1.0 | grad norm: 0.507 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      778/   63312 | consumed samples:       796672 | elapsed time per iteration (ms): 22456.1 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 46694.55829521436 |TFLOPS: 20.021192140157087 | load balancing loss: 1.132812E-01 | lm loss: 3.500153E+00 | loss scale: 1.0 | grad norm: 0.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      779/   63312 | consumed samples:       797696 | elapsed time per iteration (ms): 22139.6 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47362.113375343666 |TFLOPS: 20.30741925122447 | load balancing loss: 1.142578E-01 | lm loss: 3.462660E+00 | loss scale: 1.0 | grad norm: 0.498 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      780/   63312 | consumed samples:       798720 | elapsed time per iteration (ms): 22022.2 | learning rate: 1.000E-04 | global batch size:  1024 |tokens per sec: 47614.57157969111 |TFLOPS: 20.41566557373262 | load balancing loss: 1.142578E-01 | lm loss: 3.452358E+00 | loss scale: 1.0 | grad norm: 0.516 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration     780 to /groups/gaf51275/llama/checkpoints/MoE/megablocks/moe/mixtral-7bx8_8expert_0cap_fac_2top_k_1gb/
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/280: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/46: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13679734912 vs 13679734808
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/330: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/4: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/76: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/320: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/338: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/154: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/47: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/329: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13797175552 vs 13797175448
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/309: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10784041408 vs 10784041304
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/327: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/327: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/179: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12639305856 vs 12639305752
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 8155961280 vs 8155961176
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12639305856 vs 12639305752
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/320: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/327: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12639305856 vs 12639305752
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/320: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12093001792 vs 12093001688
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/316: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/297: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/183: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9059842624 vs 9059842520
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/320: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/324: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/171: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/337: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/314: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13713319424 vs 13713319320
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/324: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/175: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 7870746432 vs 7870746328
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/295: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/58: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    torch.save(state_dict, model_checkpoint_name)
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13948301056 vs 13948300952
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9407900032 vs 9407899928
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/333: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/334: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
Traceback (most recent call last):
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/327: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/317: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12639305856 vs 12639305752
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11565292288 vs 11565292184
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/172: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/339: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 7613633152 vs 7613633048
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/313: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/318: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/319: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/319: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11467789760 vs 11467789656
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/319: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/315: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11165668288 vs 11165668184
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/313: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10695905984 vs 10695905880
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11800173440 vs 11800173336
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/319: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/314: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/320: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12093001792 vs 12093001688
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11321052352 vs 11321052248
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11858054976 vs 11858054872
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/319: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/325: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/314: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10930787136 vs 10930787032
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/329: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12932125824 vs 12932125720
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13662858496 vs 13662858392
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/295: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9407900032 vs 9407899928
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/48: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/314: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13914616192 vs 13914616088
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
Traceback (most recent call last):
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11321052352 vs 11321052248
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/307: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/320: file write failed

During handling of the above exception, another exception occurred:

    save_checkpoint_and_time(iteration, model, optimizer,
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    pretrain(train_valid_test_datasets_provider, model_provider,
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12093001728 vs 12093001624
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/324: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/314: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/163: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/37: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 14301087744 vs 14301087640
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/311: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/325: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12630004416 vs 12630004312
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/304: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
Traceback (most recent call last):
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/323: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12337184384 vs 12337184280
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13167015296 vs 13167015192
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/319: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/340: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11858054976 vs 11858054872
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/71: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 14334316160 vs 14334316056
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/338: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13948200576 vs 13948200472
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/184: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/82: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 14485499008 vs 14485498904
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/325: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/309: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/303: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9621892416 vs 9621892312
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/167: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 7266495168 vs 7266495064
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/313: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/313: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/338: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/315: file write failed

During handling of the above exception, another exception occurred:

    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    with _open_zipfile_writer(f) as opened_zipfile:
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11263170816 vs 11263170712
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13948200576 vs 13948200472
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/35: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/315: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/183: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 8930908288 vs 8930908184
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/184: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/325: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9165789440 vs 9165789336
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/312: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11028289664 vs 11028289560
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/323: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12337184384 vs 12337184280
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/167: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 14476945664 vs 14476945560
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 7266495168 vs 7266495064
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/326: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/323: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12864885568 vs 12864885464
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/324: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/315: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    torch.save(state_dict, model_checkpoint_name)
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12004858048 vs 12004857944
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11555933504 vs 11555933400
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/334: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/304: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9856773568 vs 9856773464
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11467789760 vs 11467789656
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/58: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
Traceback (most recent call last):
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/319: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13948301056 vs 13948300952
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/170: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 7316837184 vs 7316837080
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/305: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10481919936 vs 10481919832
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/309: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 7378752000 vs 7378751896
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/309: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/320: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/305: file write failed

During handling of the above exception, another exception occurred:

    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/314: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/315: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/304: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/315: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/317: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11565292288 vs 11565292184
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/329: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12932125888 vs 12932125784
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/314: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/310: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/314: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/308: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10726159872 vs 10726159768
    torch.save(state_dict, model_checkpoint_name)
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9165789440 vs 9165789336
    self.file_like.write_end_of_file()
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9662353024 vs 9662352920
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 5910212736 vs 5910212632
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10491278720 vs 10491278616
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9662631808 vs 9662631704
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 6774502784 vs 6774502680
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/310: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/310: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11018930816 vs 11018930712
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/329: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/314: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/303: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    save_checkpoint_and_time(iteration, model, optimizer,
    self.file_like.write_end_of_file()
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11321052352 vs 11321052248
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
    with _open_zipfile_writer(f) as opened_zipfile:
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/309: file write failed

During handling of the above exception, another exception occurred:

  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    self.file_like.write_end_of_file()
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10189157248 vs 10189157144
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10961041024 vs 10961040920
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/303: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9621892416 vs 9621892312
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider,
    pretrain(train_valid_test_datasets_provider, model_provider,
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11555933440 vs 11555933336
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11321052288 vs 11321052184
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10481919872 vs 10481919768
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11018930816 vs 11018930712
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11555933440 vs 11555933336
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11018930816 vs 11018930712
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10247038720 vs 10247038616
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/324: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/313: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    pretrain(train_valid_test_datasets_provider, model_provider,
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/305: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    iteration = train(forward_step_func,
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    iteration = train(forward_step_func,
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/308: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11858054976 vs 11858054872
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12932125888 vs 12932125784
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12395123264 vs 12395123160
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10784041408 vs 10784041304
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12395123264 vs 12395123160
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11858054976 vs 11858054872
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12630004416 vs 12630004312
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12093001792 vs 12093001688
    self.file_like.write_end_of_file()
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10481919872 vs 10481919768
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10158895040 vs 10158894936
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11498051968 vs 11498051864
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13615874240 vs 13615874136
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13078871616 vs 13078871512
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12843990464 vs 12843990360
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 14167570112 vs 14167570008
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13078871616 vs 13078871512
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10695905984 vs 10695905880
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13968196032 vs 13968195928
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 13380993088 vs 13380992984
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/320: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/313: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10695905984 vs 10695905880
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/304: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9856773568 vs 9856773464
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
    pretrain(train_valid_test_datasets_provider, model_provider,
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10784041408 vs 10784041304
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12630004416 vs 12630004312
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12093001792 vs 12093001688
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12093001792 vs 12093001688
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12932125888 vs 12932125784
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12093001792 vs 12093001688
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11555933504 vs 11555933400
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/319: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11858054976 vs 11858054872
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11321052288 vs 11321052184
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11321052288 vs 11321052184
    self.file_like.write_end_of_file()
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11253811968 vs 11253811864
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12093001728 vs 12093001624
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12395123200 vs 12395123096
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11555933440 vs 11555933336
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11790814592 vs 11790814488
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12630004352 vs 12630004248
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/184: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9165789440 vs 9165789336
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/308: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10726159872 vs 10726159768
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10695905984 vs 10695905880
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10930787136 vs 10930787032
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10695905984 vs 10695905880
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 10393776192 vs 10393776088
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 12004858048 vs 12004857944
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11467789760 vs 11467789656
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9856773568 vs 9856773464
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11769976896 vs 11769976792
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/303: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 9621892416 vs 9621892312
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/313: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/pretrain_gpt.py", line 160, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 149, in pretrain
    iteration = train(forward_step_func,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 799, in train
    save_checkpoint_and_time(iteration, model, optimizer,
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/training.py", line 721, in save_checkpoint_and_time
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/third_party/Megatron-LM/megatron/checkpointing.py", line 282, in save_checkpoint
    torch.save(state_dict, model_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/bb/1/llm/gaf51275/llama/mistral/megablocks/.env/lib/python3.10/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 11263170816 vs 11263170712
wandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploaded--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
wandb: | 0.022 MB of 0.022 MB uploaded--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[46053,1],65]
  Exit code:    1
--------------------------------------------------------------------------
